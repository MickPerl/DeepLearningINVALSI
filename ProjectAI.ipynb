{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "ProjectAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1QPsU3xbgoY25jiMgoWamYhxKLT_iMHYA#scrollTo=XDspGgUS55Xo)"
      ],
      "metadata": {
        "id": "XDspGgUS55Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installazione delle librerie necessarie per l'analisi dei dati"
      ],
      "metadata": {
        "id": "HkcKgoFzNz6q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install scikit-learn==0.24.2 cluster-over-sampling"
      ],
      "outputs": [],
      "metadata": {
        "id": "4edKDd_3aPHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import delle librerire fondamentali per l'analisi dei dati"
      ],
      "metadata": {
        "id": "NUg8I6hAdzjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "import re\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "outputs": [],
      "metadata": {
        "id": "dxT1q959d-rL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ottenimento dataset\n",
        "La variabile `DATASET_PATH` sarà settata con il valore corretto.\n",
        "\n",
        "**Settare a** `True` **la variabile** `from_gdrive` **per accedere al dataset tramite Google Drive. Settarla a** `False`\n",
        "**per accedere al dataset in locale.**"
      ],
      "metadata": {
        "id": "kcnbPK6Ibq7D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from_gdrive = True\r\n",
        "if from_gdrive:\r\n",
        "    from google.colab import drive\r\n",
        "    drive.mount(\"/content/gdrive\")\r\n",
        "    %cd gdrive/MyDrive/ProjectAI/\r\n",
        "    DATASET_PATH = \"Data/invalsi_mat_2014.csv\"\r\n",
        "else:\r\n",
        "    DATASET_PATH = \"../invalsi_mat_2014.csv\""
      ],
      "outputs": [],
      "metadata": {
        "id": "ICIk29N7Rf88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "in_jupyter_notebook = False\r\n",
        "if in_jupyter_notebook:\r\n",
        "    from IPython.core.display import display, HTML\r\n",
        "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ],
      "outputs": [],
      "metadata": {
        "id": "FGOx3t8pZpnm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from_azure = False\r\n",
        "if from_azure:\r\n",
        "    from azureml.core import Workspace\r\n",
        "    from os import environ\r\n",
        "    #%load_ext dotenv\r\n",
        "    #%dotenv\r\n",
        "    # Eseguire export $(cat .env | xargs)\r\n",
        "    subscription_id = environ.get('subscription_id')\r\n",
        "    resource_group = environ.get('resource_group')\r\n",
        "    workspace_name = environ.get('workspace_name')\r\n",
        "    workspace = Workspace(subscription_id, resource_group, workspace_name)"
      ],
      "outputs": [],
      "metadata": {
        "id": "y19qe5o3xt9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility per tipi e conversioni"
      ],
      "metadata": {
        "id": "ImDtO788e8ZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from typing import List, Dict, Tuple\r\n",
        "string_list = List[str]\r\n",
        "one_hot_list = Tuple[int]\r\n",
        "one_hot_encoding_list = Dict[str, one_hot_list]\r\n",
        "one_hot_encoding_int = Dict[str, int]"
      ],
      "outputs": [],
      "metadata": {
        "id": "XRNMMf13oGTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mappe e funzioni per conversioni valori del dataset in valori numerici o booleani"
      ],
      "metadata": {
        "id": "o38E8qc6eCgd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def from_categorical_to_one_hot_int(categorical_data: string_list) -> one_hot_encoding_int:\r\n",
        "    dictionary_to_return = {}\r\n",
        "    for index, key in enumerate(categorical_data):\r\n",
        "        dictionary_to_return[key] = index\r\n",
        "    \r\n",
        "    return dictionary_to_return\r\n",
        "\r\n",
        "def from_categorical_to_one_hot_list(categorical_data: string_list) -> one_hot_encoding_list:\r\n",
        "    dictionary_to_return = {}\r\n",
        "    indexes = range(len(categorical_data))\r\n",
        "    for index, key in enumerate(categorical_data):\r\n",
        "        dictionary_to_return[key] = [1 if index == i else 0 for i in indexes]\r\n",
        "    \r\n",
        "    return dictionary_to_return\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_MESI = [\"Gennaio\", \"Febbraio\", \"Marzo\", \"Aprile\", \"Maggio\", \"Giugno\", \"Luglio\", \"Agosto\", \"Settembre\", \"Ottobre\", \"Novembre\", \"Dicembre\", \"Non disponibile\"]\r\n",
        "MESI = from_categorical_to_one_hot_int(list_MESI)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_ANNI = [\"2000\", \"2001\", \"1999\", \"1998\", \"<=1997\", \">=2002\", \"Non disponibile\"]\r\n",
        "ANNI = from_categorical_to_one_hot_int(list_ANNI)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_ETA = ['Mancante di sistema', 'Non disponibile', '6 anni', '5 anni', '10 anni o più', '2 anni', '1 anno o prima', '9 anni', '8 anni', '4 anni', '7 anni', '3 anni']\r\n",
        "ETA = from_categorical_to_one_hot_int(list_ETA)\r\n",
        "\r\n",
        "def convert_question_result(result: str) -> bool:\r\n",
        "    return result == \"Corretta\" # alternativamente result == \"Errata\"\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_REGOLARITA = ['Regolare', 'Posticipatario', 'Anticipatario', 'Dato mancante']\r\n",
        "REGOLARITA = from_categorical_to_one_hot_int(list_REGOLARITA)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_AREA_GEOGRAFICA_5_ISTAT = ['Sud', 'Nord est', 'Centro', 'Nord ovest', 'Isole']\r\n",
        "AREA_GEOGRAFICA_5_ISTAT = from_categorical_to_one_hot_int(list_AREA_GEOGRAFICA_5_ISTAT)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_AREA_GEOGRAFICA_5 = ['Sud', 'Nord est', 'Centro', 'Nord ovest', 'Sud e isole']\r\n",
        "AREA_GEOGRAFICA_5 = from_categorical_to_one_hot_int(list_AREA_GEOGRAFICA_5)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_AREA_GEOGRAFICA_4 = ['Mezzogiorno', 'Nord est', 'Centro', 'Nord ovest']\r\n",
        "AREA_GEOGRAFICA_4 = from_categorical_to_one_hot_int(list_AREA_GEOGRAFICA_4)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_AREA_GEOGRAFICA_3 = ['Mezzogiorno', 'Nord', 'Centro']\r\n",
        "AREA_GEOGRAFICA_3 = from_categorical_to_one_hot_int(list_AREA_GEOGRAFICA_3)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_REGIONI = ['Campania', 'Emilia-Romagna', 'Lazio', 'Piemonte', 'Puglia', 'Lombardia', 'Veneto', 'Sicilia', 'Prov. Aut. Trento', 'Friuli-Venezia Giulia', 'Abruzzo', 'Liguria', 'Toscana', 'Sardegna', 'Calabria', 'Molise', 'Marche', 'Umbria', 'Basilicata', 'Prov. Aut. Bolzano (l. it.)']\r\n",
        "REGIONI = from_categorical_to_one_hot_int(list_REGIONI)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_PROVINCE = ['', 'RE', 'FR', 'TO', 'BA', 'CO', 'LE', 'RO', 'CT', 'RM', 'TA', 'BS', 'SA', 'TN', 'UD', 'FG', 'LT', 'AG', 'CH', 'PC', 'TS', 'SR', 'SP', 'PD', 'SI', 'PA', 'TP', 'BO', 'CA', 'CN', 'RC', 'TE', 'MI', 'LC', 'LU', 'FI', 'AQ', 'TV', 'RG', 'VA', 'GO', 'MO', 'GE', 'AL', 'CB', 'PR', 'OR', 'VE', 'MC', 'NO', 'PT', 'MN', 'VR', 'PI', 'AP', 'LO', 'VI', 'SV', 'PU', 'BG', 'AR', 'VT', 'LI', 'SS', 'BR', 'RA', 'TR', 'SO', 'IM', 'PZ', 'GR', 'AN', 'PN', 'ME', 'CR', 'FE', 'BI', 'PV', 'PG', 'VB', 'BL', 'PE', 'CS', 'CZ', 'AV', 'RN', 'CL', 'AT', 'MS', 'KR', 'RI', 'EN', 'CE', 'MT', 'VV', 'VC', 'NU', 'FC', 'PO', 'BZ', 'BN', 'IS', \r\n",
        "            'NA', # presente in cod_provincia_ISTAT ma non in sigla_provincia_istat\r\n",
        "            'PS', # presente in cod_provincia_ISTAT ma non in sigla_provincia_istat\r\n",
        "            'FO', # presente in cod_provincia_ISTAT ma non in sigla_provincia_istat\r\n",
        "            'LB', # presente in cod_provincia_ISTAT ma non in sigla_provincia_istat\r\n",
        "]\r\n",
        "PROVINCE = from_categorical_to_one_hot_int(list_PROVINCE)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_CITTADINANZA = ['Italiano', 'Straniero II generazione', 'Straniero I generazione', 'Dato mancante']\r\n",
        "CITTADINANZA = from_categorical_to_one_hot_int(list_CITTADINANZA)\r\n",
        "\r\n",
        "list_VOTI_NUMERICI = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\r\n",
        "def voto_orale_decode(voto_orale: str) -> float:\r\n",
        "    if voto_orale in list_VOTI_NUMERICI:\r\n",
        "        return float(voto_orale)\r\n",
        "    elif voto_orale == 'Non disponibile':\r\n",
        "        return np.nan\r\n",
        "    elif voto_orale == 'Non classificato': \r\n",
        "        return 0.0\r\n",
        "\r\n",
        "list_VOTI_NAN = ['Non disponibile', 'Senza voto scritto']\r\n",
        "def voto_scritto_decode(voto_scritto: str) -> float:\r\n",
        "    if voto_scritto in list_VOTI_NUMERICI:\r\n",
        "        return float(voto_scritto)\r\n",
        "    elif voto_scritto in list_VOTI_NAN:\r\n",
        "        return np.nan\r\n",
        "    elif voto_scritto == 'Non classificato': \r\n",
        "        return 0.0\r\n",
        "\r\n",
        "def sesso_to_num(sesso: str) -> int:\r\n",
        "    return 0 if sesso == \"Maschio\" else 1\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_PROFESSIONI = ['1. Disoccupato/a', '2. Casalingo/a', '3. Dirigente, docente universitario, funzionario o ufficiale militare', '4. Imprenditore/proprietario agricolo', '5. Professionista dipendente, sottuff. militare o libero profession. (medico, av', '6. Lavoratore in proprio (commerciante, coltivatore diretto, artigiano, meccanic', '7. Insegnante, impiegato, militare graduato', '8. Operaio, addetto ai servizi/socio di cooperativa', '9. Pensionato/a', '10. Non disponibile']\r\n",
        "PROFESSIONI = from_categorical_to_one_hot_int(list_PROFESSIONI)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_TITOLI = ['1. Licenza elementare', '2. Licenza media', '3. Qualifica professionale triennale', '4. Diploma di maturità', '5. Altro titolo di studio superiore al diploma (I.S.E.F., Accademia di Belle Art', '6. Laurea o titolo superiore (ad esempio Dottorato di Ricerca)', '7. Non disponibile']\r\n",
        "TITOLI = from_categorical_to_one_hot_int(list_TITOLI)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_LUOGHI_GENITORI = ['Italia (o Repubblica di San Marino)', 'Unione Europea', 'Paese europeo Non UE', 'Altro', 'Non disponibile']\r\n",
        "LUOGHI_GENITORI = from_categorical_to_one_hot_int(list_LUOGHI_GENITORI)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_FREQUENZA_SCUOLA = ['No', 'Sì', 'Non disponibile']\r\n",
        "FREQUENZA_SCUOLA = from_categorical_to_one_hot_int(list_FREQUENZA_SCUOLA)\r\n",
        "\r\n",
        "#Features categorica -> One Hot Encoding\r\n",
        "list_LUOGO_DI_NASCITA = ['Italia (o Repubblica di San Marino)', 'Unione Europea', 'Paese europeo Non UE', 'Altro', 'Non disponibile']\r\n",
        "LUOGO_DI_NASCITA = from_categorical_to_one_hot_int(list_LUOGO_DI_NASCITA)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ix0xEhejePKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definiamo le conversioni che si dovranno eseguire al momento dell'import."
      ],
      "metadata": {
        "id": "PB7HCRC7eemV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "columns_converters = {\r\n",
        "    \"CODICE_SCUOLA\": str, #identificativo della scuola (non considerato)\r\n",
        "    \"CODICE_PLESSO\": str, #identificativo del plesso (non considerato)\r\n",
        "    \"CODICE_CLASSE\": str, #identificato della classe (non considerato)\r\n",
        "    \"macrotipologia\": str, #categoria di scuola (non considerato)\r\n",
        "    \"campione\": int, #campione di riferimento (non considerato)\r\n",
        "    \"livello\": int, # (non considerato)\r\n",
        "    \"prog\": int,\r\n",
        "    \"CODICE_STUDENTE\": str, #codice dello studente (non considerato)\r\n",
        "    \"sesso\": sesso_to_num, #sesso dello studente\r\n",
        "    \"mese\": lambda month: MESI[month], #mese di nascita\r\n",
        "    \"anno\": lambda year: ANNI[year], #anno di nascita\r\n",
        "    \"luogo\": lambda luogo: LUOGO_DI_NASCITA[luogo],\r\n",
        "    \"eta\": lambda eta: ETA[eta], # cosa vuol dire eta?\r\n",
        "    \"codice_orario\": lambda _: np.nan, # unico dato: Mancante di sistema\r\n",
        "    \"freq_asilo_nido\": lambda frequenza: FREQUENZA_SCUOLA[frequenza],\r\n",
        "    \"freq_scuola_materna\": lambda frequenza: FREQUENZA_SCUOLA[frequenza],\r\n",
        "    \"luogo_padre\": lambda luogo: LUOGHI_GENITORI[luogo],\r\n",
        "    \"titolo_padre\": lambda titolo: TITOLI[titolo],\r\n",
        "    \"prof_padre\": lambda professione: PROFESSIONI[professione],\r\n",
        "    \"luogo_madre\": lambda luogo: LUOGHI_GENITORI[luogo],\r\n",
        "    \"titolo_madre\": lambda titolo: TITOLI[titolo],\r\n",
        "    \"prof_madre\": lambda professione: PROFESSIONI[professione],\r\n",
        "    \"voto_scritto_ita\": lambda voto: voto_scritto_decode(voto),\r\n",
        "    \"voto_orale_ita\": lambda voto: voto_orale_decode(voto),\r\n",
        "    \"voto_scritto_mat\": lambda voto: voto_scritto_decode(voto),\r\n",
        "    \"voto_orale_mat\": lambda voto: voto_orale_decode(voto),\r\n",
        "    \"D1\": lambda result: convert_question_result(result),\r\n",
        "    \"D2\": lambda result: convert_question_result(result),\r\n",
        "    \"D3_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D3_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D4_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D4_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D4_c\": lambda result: convert_question_result(result),\r\n",
        "    \"D4_d\": lambda result: convert_question_result(result),\r\n",
        "    \"D5_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D5_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D6\": lambda result: convert_question_result(result),\r\n",
        "    \"D7_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D7_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D8\": lambda result: convert_question_result(result),\r\n",
        "    \"D9\": lambda result: convert_question_result(result),\r\n",
        "    \"D10_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D10_b1\": lambda result: convert_question_result(result),\r\n",
        "    \"D10_b2\": lambda result: convert_question_result(result),\r\n",
        "    \"D10_b3\": lambda result: convert_question_result(result),\r\n",
        "    \"D11_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D11_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D12_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D12_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D13_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D13_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D13_c\": lambda result: convert_question_result(result),\r\n",
        "    \"D14\": lambda result: convert_question_result(result),\r\n",
        "    \"D15\": lambda result: convert_question_result(result),\r\n",
        "    \"D16_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D16_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D16_c\": lambda result: convert_question_result(result),\r\n",
        "    \"D16_d\": lambda result: convert_question_result(result),\r\n",
        "    \"D17_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D17_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D18\": lambda result: convert_question_result(result),\r\n",
        "    \"D19_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D19_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D20\": lambda result: convert_question_result(result),\r\n",
        "    \"D21\": lambda result: convert_question_result(result),\r\n",
        "    \"D22\": lambda result: convert_question_result(result),\r\n",
        "    \"D23_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D23_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D23_c\": lambda result: convert_question_result(result),\r\n",
        "    \"D23_d\": lambda result: convert_question_result(result),\r\n",
        "    \"D24_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D24_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D25\": lambda result: convert_question_result(result),\r\n",
        "    \"D26_a\": lambda result: convert_question_result(result),\r\n",
        "    \"D26_b\": lambda result: convert_question_result(result),\r\n",
        "    \"D26_c\": lambda result: convert_question_result(result),\r\n",
        "    \"D26_d\": lambda result: convert_question_result(result),\r\n",
        "    \"regolarità\": lambda regular: REGOLARITA[regular],\r\n",
        "    \"cittadinanza\": lambda cittadinanza: CITTADINANZA[cittadinanza],\r\n",
        "    \"cod_provincia_ISTAT\": lambda province_istat: PROVINCE[province_istat.upper()],\r\n",
        "    \"sigla_provincia_istat\": lambda province: PROVINCE[province],\r\n",
        "    \"Nome_reg\": lambda regione: REGIONI[\" \".join(regione.split())],\r\n",
        "    \"Cod_reg\": lambda codice: REGIONI[codice],\r\n",
        "    \"Areageo_3\": lambda area: AREA_GEOGRAFICA_3[area],\r\n",
        "    \"Areageo_4\": lambda area: AREA_GEOGRAFICA_4[area],\r\n",
        "    \"Areageo_5\": lambda area: AREA_GEOGRAFICA_5[area],\r\n",
        "    \"Areageo_5_Istat\": lambda area: AREA_GEOGRAFICA_5_ISTAT[area],\r\n",
        "    \"Pon\": lambda pon: True if pon == \"Area_Pon\" else False, # lo studente appartiene all'aera Pon oppure no\r\n",
        "    \"pu_ma_gr\": int,\r\n",
        "    \"pu_ma_no\": float,\r\n",
        "    \"Fattore_correzione_new\": float,\r\n",
        "    \"Cheating\": float,\r\n",
        "    \"PesoClasse\": lambda val: float(val) if val != \"\" else np.nan, # (non considerato)\r\n",
        "    \"PesoScuola\": lambda val: float(val) if val != \"\" else np.nan, # (non considerato)\r\n",
        "    \"PesoTotale_Matematica\": lambda val: float(val) if val != \"\" else np.nan, # (non considerato)\r\n",
        "    \"WLE_MAT\": float,\r\n",
        "    \"WLE_MAT_200\": float,\r\n",
        "    \"WLE_MAT_200_CORR\": float,\r\n",
        "    \"pu_ma_no_corr\": float,\r\n",
        "    \"n_stud_prev\": lambda val: int(float(val)),\r\n",
        "    \"n_classi_prev\": lambda val: int(float(val)),\r\n",
        "    \"LIVELLI\": int,\r\n",
        "    \"DROPOUT\": eval\r\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "mTqfUwl0erHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lettura del dataset"
      ],
      "metadata": {
        "id": "Ef9Xh_mJfGPS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if not from_azure:\r\n",
        "    dataset = pd.read_csv(DATASET_PATH, sep=';', converters=columns_converters)\r\n",
        "else:\r\n",
        "    from azureml.core import Workspace, Dataset\r\n",
        "    dataset = Dataset.get_by_name(workspace, name='invalsi_mat_2014')\r\n",
        "    dataset = dataset.to_pandas_dataframe()"
      ],
      "outputs": [],
      "metadata": {
        "id": "POdTrO5WfJpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verifica per la presenza di variabili categoriche\n",
        "Oltre alle variabili categoriche fin qui trovate si vuole verificare quali discriminano in maniera non significativa le osservazioni."
      ],
      "metadata": {
        "id": "fqN-4pz_-lGw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# codice che restituisce le colonne che hanno un nr di valori distinti < 4\r\n",
        "[cname for cname in dataset.columns if dataset[cname].nunique() < 4 and dataset[cname].dtype != \"bool\"]"
      ],
      "outputs": [],
      "metadata": {
        "id": "vplCl4Nq2xLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# codice per ottenere le variabili qualitative/categoriche\r\n",
        "s = (dataset.dtypes == 'object')\r\n",
        "object_cols = list(s[s].index)\r\n",
        "\r\n",
        "print(\"Categorical variables:\")\r\n",
        "print(object_cols)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zxGS6Cq14Xjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eliminazione colonne superflue\n",
        "Vengono rimosse dal dataset originale tutte quegli attributi che si suppone non abbiano alcuna rilevanza nell'esito della cella DROPOUT.\n",
        "Inoltre, per la compuntazione del clustering ai fini dell'oversampling è necessario rimuovere degli ulteriori attributi poiché ricchi di valori nulli e quindi problematici.\n"
      ],
      "metadata": {
        "id": "kAMeln9J19au"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "columns_to_drop = [\"Unnamed: 0\", # Indice record\r\n",
        "                   \"CODICE_SCUOLA\",\r\n",
        "                   \"CODICE_PLESSO\",\r\n",
        "                   \"CODICE_CLASSE\",\r\n",
        "                   \"macrotipologia\",\r\n",
        "                   \"campione\",\r\n",
        "                   \"livello\",\r\n",
        "                   \"CODICE_STUDENTE\",\r\n",
        "                   \"codice_orario\",\r\n",
        "                   \"PesoClasse\", # potrebbe essere utile?\r\n",
        "                   \"PesoScuola\", # potrebbe essere utile?\r\n",
        "                   \"PesoTotale_Matematica\", # potrebbe essere utile?\r\n",
        "                   ]\r\n",
        "columns_maybe_to_drop = [\"voto_scritto_ita\", #Ricco di NaN\r\n",
        "                         \"voto_scritto_mat\",#Ricco di NaN\r\n",
        "                         \"voto_orale_ita\", # Pochi NaN ma va tolto per cluster-based oversampling\r\n",
        "                         \"voto_orale_mat\" # Pochi NaN ma va tolto per cluster-based oversampling\r\n",
        "                        ]\r\n",
        "for col in columns_maybe_to_drop:\r\n",
        "   print(\"Percentage of NaN in column: \", col, end=\" \")\r\n",
        "   print(round(dataset[col].isnull().mean() * 100, 2), \"%\")\r\n",
        "   \r\n",
        "dataset = dataset.drop(columns_to_drop + columns_maybe_to_drop, axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "v4T5s4Yb1_qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrici correlazione tra domande\n",
        "Con le seguenti matrici di correlazione si cerca di vedere se sono presenti domande fortemente correlate in modo da poterne rimuovere e semplificare una riduzione di dimensionalità."
      ],
      "metadata": {
        "id": "j3YeO8Ycfx7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "questions_columns = [col for col in columns_converters.keys() if re.search(\"^D\\d\", col)]\r\n",
        "questions_dataset = dataset[questions_columns]"
      ],
      "outputs": [],
      "metadata": {
        "id": "6EqG0d5Mf2CO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard correlation coefficient (pearson method)"
      ],
      "metadata": {
        "id": "RNEbExY6gQcu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "questions_correlate_matrix_pearson = questions_dataset.corr(method='pearson').round(2)\r\n",
        "questions_correlate_matrix_pearson.style.background_gradient(cmap='YlOrRd')"
      ],
      "outputs": [],
      "metadata": {
        "id": "pe416QwMgJ6U",
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kendall Tau correlation coefficient"
      ],
      "metadata": {
        "id": "ABgGSd1tgYej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "questions_correlate_matrix_kendall = questions_dataset.corr(method='kendall').round(2)\r\n",
        "questions_correlate_matrix_kendall.style.background_gradient(cmap='YlOrRd')"
      ],
      "outputs": [],
      "metadata": {
        "id": "VN09a_kCgNe0",
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spearman rank correlation"
      ],
      "metadata": {
        "id": "e7wAdEsKgkvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "questions_correlate_matrix_spearman = questions_dataset.corr(method='spearman').round(2)\r\n",
        "questions_correlate_matrix_spearman.style.background_gradient(cmap='YlOrRd')"
      ],
      "outputs": [],
      "metadata": {
        "id": "gKUli2aEgfZ5",
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nota sulle matrici di correlazione ottenute\n",
        "Le **matrici di correlazione sulle domande** non evidenziano significative dipendenze lineari tra di esse: i valori più elevati appaiono in corrispondenza di domande consecutive, il più delle volte parti della stessa domanda (e.g. D7, D3 e D4).  \n",
        "Conseguentemente, **non è possibile realizzare alcuna riduzione di dimensionalità**."
      ],
      "metadata": {
        "id": "JBU9svIjbD9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rappresentazione delle domande per generalizzazione con coorti diverse\n",
        "Addestrando la rete con le domande non si ottiene nulla di replicabili in coorti successive.\n",
        "E' possibile però trasformare le domande e i risultati dati dagli studenti in altre feature. Ogni domanda dispone infatti di un ambito, uno scopo e un processo.  \n",
        "Per questo motivo le domande vengono trasformate in due modalità:\n",
        "\n",
        "- da domanda a coppia ambito - processo;\n",
        "- da domanda a vettore di termini chiave  estratti tramite Natural Language Processing dallo scopo."
      ],
      "metadata": {
        "id": "rsdodsx-2kGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalizzazione Domanda => (Ambito, Processo)"
      ],
      "metadata": {
        "id": "v2LLlA3bC0w8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# mapping domanda - ambito, processo\r\n",
        "questions_ambiti_processi = {\r\n",
        "    'D1' : ('Numeri', 'Conoscere e utilizzare algoritmi e procedure'),\r\n",
        "    'D2' : ('Dati e previsioni', 'Conoscere e padroneggiare i contenuti specifici della matematica'),\r\n",
        "    'D3_a' : ('Dati e previsioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D3_b': ('Numeri', 'Conoscere e utilizzare algoritmi e procedure'),\r\n",
        "    'D4_a' : ('Spazio figure', 'Riconoscere le forme nello spazio e utilizzarle per la risoluzione di problemi geometrici o di modellizzazione'),\r\n",
        "    'D4_b' : ('Spazio figure', 'Riconoscere le forme nello spazio e utilizzarle per la risoluzione di problemi geometrici o di modellizzazione'),\r\n",
        "    'D4_c' : ('Spazio figure', 'Riconoscere le forme nello spazio e utilizzarle per la risoluzione di problemi geometrici o di modellizzazione'),\r\n",
        "    'D4_d' : ('Spazio figure', 'Riconoscere le forme nello spazio e utilizzarle per la risoluzione di problemi geometrici o di modellizzazione'),\r\n",
        "    'D5_a' : ('Relazioni e funzioni', 'Conoscere e utilizzare algoritmi e procedure'),\r\n",
        "    'D5_b' : ('Relazioni e funzioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D6' : ('Numeri', 'Acquisire progressivamente forme tipiche del pensiero matematico'),\r\n",
        "    'D7_a' : ('Numeri', 'Conoscere e padroneggiare i contenuti specifici della matematica'),\r\n",
        "    'D7_b' : ('Numeri', 'Conoscere e padroneggiare i contenuti specifici della matematica'),\r\n",
        "    'D8' : ('Spazio figure', 'Riconoscere le forme nello spazio e utilizzarle per la risoluzione di problemi geometrici o di modellizzazione'),\r\n",
        "    'D9' : ('Relazioni e funzioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D10_a'  : ('Relazioni e funzioni', 'Rappresentare relazioni e dati e, in situazioni significative, utilizzare le rappresentazioni per ricavare informazioni, formulare giudizi e prendere decisioni'),\r\n",
        "    'D10_b1' : ('Relazioni e funzioni', 'Rappresentare relazioni e dati e, in situazioni significative, utilizzare le rappresentazioni per ricavare informazioni, formulare giudizi e prendere decisioni'),\r\n",
        "    'D10_b2' : ('Relazioni e funzioni', 'Rappresentare relazioni e dati e, in situazioni significative, utilizzare le rappresentazioni per ricavare informazioni, formulare giudizi e prendere decisioni'),\r\n",
        "    'D10_b3' : ('Relazioni e funzioni', 'Rappresentare relazioni e dati e, in situazioni significative, utilizzare le rappresentazioni per ricavare informazioni, formulare giudizi e prendere decisioni'),\r\n",
        "    'D11_a' : ('Spazio figure', 'Riconoscere in contesti diversi il carattere misurabile di oggetti e fenomeni, utilizzare strumenti di misura, misurare grandezze, stimare misure di grandezze'),\r\n",
        "    'D11_b' : ('Spazio figure', 'Conoscere e utilizzare algoritmi e procedure'),\r\n",
        "    'D12_a' : ('Numeri', 'Risolvere problemi utilizzando strategie in ambiti diversi – numerico, geometrico, algebrico –'),\r\n",
        "    'D12_b' : ('Numeri', 'Risolvere problemi utilizzando strategie in ambiti diversi – numerico, geometrico, algebrico –'),\r\n",
        "    'D13_a' : ('Dati e previsioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D13_b' : ('Dati e previsioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D13_c' : ('Dati e previsioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D14' : ('Relazioni e funzioni', 'Acquisire progressivamente forme tipiche del pensiero matematico'),\r\n",
        "    'D15' : ('Dati e previsioni', 'Conoscere e utilizzare algoritmi e procedure'),\r\n",
        "    'D16_a' : ('Dati e previsioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D16_b' : ('Dati e previsioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D16_c' : ('Dati e previsioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D16_d' : ('Dati e previsioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D17_a' : ('Numeri', 'Riconoscere in contesti diversi il carattere misurabile di oggetti e fenomeni, utilizzare strumenti di misura, misurare grandezze, stimare misure di grandezze'),\r\n",
        "    'D17_b' : ('Numeri', 'Riconoscere in contesti diversi il carattere misurabile di oggetti e fenomeni, utilizzare strumenti di misura, misurare grandezze, stimare misure di grandezze'),\r\n",
        "    'D18' : ('Spazio figure', 'Conoscere e utilizzare algoritmi e procedure'),\r\n",
        "    'D19_a' : ('Spazio figure', 'Risolvere problemi utilizzando strategie in ambiti diversi – numerico, geometrico, algebrico –'),\r\n",
        "    'D19_b' : ('Spazio figure', 'Risolvere problemi utilizzando strategie in ambiti diversi – numerico, geometrico, algebrico –'),\r\n",
        "    'D20' : ('Dati e previsioni', 'Conoscere e utilizzare algoritmi e procedure'),\r\n",
        "    'D21' : ('Numeri', 'Riconoscere in contesti diversi il carattere misurabile di oggetti e fenomeni, utilizzare strumenti di misura, misurare grandezze, stimare misure di grandezze'),\r\n",
        "    'D22' : ('Spazio figure', 'Acquisire progressivamente forme tipiche del pensiero matematico'),\r\n",
        "    'D23_a' : ('Relazioni e funzioni', 'Acquisire progressivamente forme tipiche del pensiero matematico'),\r\n",
        "    'D23_b' : ('Relazioni e funzioni', 'Acquisire progressivamente forme tipiche del pensiero matematico'),\r\n",
        "    'D23_c' : ('Relazioni e funzioni', 'Acquisire progressivamente forme tipiche del pensiero matematico'),\r\n",
        "    'D23_d' : ('Relazioni e funzioni', 'Acquisire progressivamente forme tipiche del pensiero matematico'),\r\n",
        "    'D24_a' : ('Relazioni e funzioni', 'Conoscere e padroneggiare i contenuti specifici della matematica'),\r\n",
        "    'D24_b' : ('Relazioni e funzioni', 'Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell\\'informazione in ambito scientifico, tecnologico, economico e sociale'),\r\n",
        "    'D25' : ('Spazio figure', 'Conoscere e utilizzare algoritmi e procedure'),\r\n",
        "    'D26_a': ('Numeri', 'Conoscere e padroneggiare i contenuti specifici della matematica'),\r\n",
        "    'D26_b': ('Numeri', 'Conoscere e padroneggiare i contenuti specifici della matematica'),\r\n",
        "    'D26_c': ('Numeri', 'Conoscere e padroneggiare i contenuti specifici della matematica'),\r\n",
        "    'D26_d': ('Numeri', 'Conoscere e padroneggiare i contenuti specifici della matematica')\r\n",
        "}\r\n",
        "\r\n",
        "list_ambiti_processi = [AP for dom in questions_ambiti_processi.values() for AP in dom]\r\n",
        "ambiti_processi = set(list_ambiti_processi)\r\n",
        "conteggio_ambiti_processi = {AP: list_ambiti_processi.count(AP) for AP in ambiti_processi}\r\n",
        "\r\n",
        "for AP in ambiti_processi:\r\n",
        "    dataset[AP] = 0.0\r\n",
        "\r\n",
        "for i, row in dataset.iterrows():\r\n",
        "    for question, APs in questions_ambiti_processi.items():\r\n",
        "        if row[question] == True:\r\n",
        "            for AP in APs:\r\n",
        "                dataset.at[i, AP] += 1/conteggio_ambiti_processi[AP]\r\n",
        "\r\n",
        "dataset = dataset.drop(questions_columns, axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yUB0DaHnzePw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "type(questions_ambiti_processi)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-cq9WOJSI1xM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importazione dataset salvato"
      ],
      "metadata": {
        "id": "2GkD_CRbIE0y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if not from_azure:\r\n",
        "    if from_gdrive:\r\n",
        "        AP_DATASET_PATH = \"Data/dataset_with_AP/dataset_with_AP.csv\"\r\n",
        "    else:\r\n",
        "        AP_DATASET_PATH = \"../dataset_with_AP/dataset_with_AP.csv\" \r\n",
        "\r\n",
        "    dataset = pd.read_csv(AP_DATASET_PATH, sep=',')\r\n",
        "\r\n",
        "    dataset['DROPOUT'] = dataset['DROPOUT'].astype('int64')\r\n",
        "    dataset['Pon'] = dataset['Pon'].astype('int64')\r\n",
        "    dataset['sesso'] = dataset['sesso'].astype('int64')\r\n",
        "else:\r\n",
        "    from azureml.core import Workspace, Dataset\r\n",
        "    dataset = Dataset.get_by_name(workspace, name='dataset_with_AP')\r\n",
        "    dataset = dataset.to_pandas_dataframe()\r\n",
        "\r\n",
        "dataset['DROPOUT'] = dataset['DROPOUT'].astype('int64') # Memorizzati come boolean e qui convertiti\r\n",
        "dataset['Pon'] = dataset['Pon'].astype('int64') # Memorizzati come boolean e qui convertiti\r\n",
        "dataset['sesso'] = dataset['sesso'].astype('int64') # Memorizzati come boolean e qui convertiti"
      ],
      "outputs": [],
      "metadata": {
        "id": "sriI349OIvW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalizzazione Domanda => termini chiave da Scopo"
      ],
      "metadata": {
        "id": "6gKlTKS_CqfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split dataset in training, validation e test\n",
        "Avendo un dataset di una sola coorte di studenti relativi alle prove di un anno, non si dispone di un set per fare testing.\n",
        "Per questo motivo si è scelto di separare il dataset in questo modo:\n",
        "\n",
        "1. dataset diviso in train_dataset (80%) e test_dataset (20%)\n",
        "2. train_dataset diviso in train_dataset (80%) e validation_dataset (20%)"
      ],
      "metadata": {
        "id": "x4rVw_SqFdPe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)\r\n",
        "train_dataset, validation_dataset = train_test_split(train_dataset, test_size=0.2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "YK77ZxFOFjpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Valutazione e gestione dello sbilanciamento del dataset"
      ],
      "metadata": {
        "id": "Ce0xEolbKOeQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "nr_nodrop_original, nr_drop_original = np.bincount(dataset['DROPOUT'])\r\n",
        "nr_nodrop, nr_drop = np.bincount(train_dataset['DROPOUT'])\r\n",
        "total_original = nr_nodrop_original + nr_drop_original\r\n",
        "total = nr_nodrop + nr_drop\r\n",
        "print('Dataset originale')\r\n",
        "print(f'Nr di istanze: {total_original:,}\\nIstanze con dropout a True: {nr_nodrop_original:,} ({(100 * nr_nodrop_original / total_original):.2f}% del totale)\\nProporzione dropout True v/s False: {round(nr_drop_original/nr_nodrop_original, 2)} : 1')\r\n",
        "print('Training dataset')\r\n",
        "print(f'Nr di istanze: {total:,}\\nIstanze con dropout a True: {nr_drop:,} ({(100 * nr_drop / total):.2f}% del totale)\\nProporzione dropout True v/s False: {round(nr_drop/nr_nodrop, 2)} : 1')"
      ],
      "outputs": [],
      "metadata": {
        "id": "SM8tXExZKaF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sbilanciamento dataset originale"
      ],
      "metadata": {
        "id": "AEoSFlmxE-q2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data = pd.Series([nr_nodrop_original, nr_drop_original])\r\n",
        "data.plot.pie(autopct=\"%.1f%%\");"
      ],
      "outputs": [],
      "metadata": {
        "id": "Cnn9VuBDFG85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sbilanciamento dataset di training"
      ],
      "metadata": {
        "id": "TG-fa363FCSK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data = pd.Series([nr_nodrop, nr_drop])\r\n",
        "data.plot.pie(autopct=\"%.1f%%\");"
      ],
      "outputs": [],
      "metadata": {
        "id": "tR5LJ8RUl9LL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le due classi (i.e. target del classificatore) appaiono leggermente sbilanciate: in dettaglio, la classe dei soggetti che manifestano dropout ha una cardinalità inferiore della classe in cui non si è avuto dropout."
      ],
      "metadata": {
        "id": "ZJN0Q8UzdGlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random under-sampling\n",
        "Tecnica consistente nell'eliminazione in maniera randomica di istanze della classe sovra-rappresentata (studenti che non hanno avuto un dropout) fintanto che la sua cardinalità coincida con quella della classe sotto-rappresentata."
      ],
      "metadata": {
        "id": "6m6aNeE6kZZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# class_nodrop contiene i record della classe sovrarappresentata, ovvero SENZA DROPOUT (== False funziona per casting implicito)\r\n",
        "class_nodrop = train_dataset[train_dataset['DROPOUT'] == False] \r\n",
        "# class_drop contiene i record della classe sottorappresentata, ovvero CON DROPOUT (== True funziona per casting implicito) \r\n",
        "class_drop = train_dataset[train_dataset['DROPOUT'] == True]  \r\n",
        "\r\n",
        "# Sotto campionamento di class_drop in modo che abbia stessa cardinalità di class_nodrop\r\n",
        "class_nodrop = class_nodrop.sample(len(class_drop))\r\n",
        "\r\n",
        "print(f'Classe No-drop: {len(class_nodrop):,}')\r\n",
        "print(f'Classe Drop: {len(class_drop):,}')"
      ],
      "outputs": [],
      "metadata": {
        "id": "Vi9UK1Jkkb_X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Il dataset di training va aggiornato unendo i due class_drop e class_nodrop\r\n",
        "print(f'Dimensioni train_dataset originale: {train_dataset.shape}')\r\n",
        "train_dataset = class_drop.append(class_nodrop)\r\n",
        "train_dataset = train_dataset.sample(frac=1)\r\n",
        "print(f'Dimensioni train_dataset undersampled: {train_dataset.shape}')"
      ],
      "outputs": [],
      "metadata": {
        "id": "o7ZLgOb1KI0h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Verifica che effettivamente l'undersampling abbia funzionato\r\n",
        "nr_nodrop, nr_drop = np.bincount(train_dataset['DROPOUT'])\r\n",
        "data = pd.Series([nr_nodrop, nr_drop])\r\n",
        "data.plot.pie(autopct=\"%.1f%%\");"
      ],
      "outputs": [],
      "metadata": {
        "id": "r6H6sPzn5DsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic minority over-sampling (SMOTE)\n",
        "Abbiamo scelto di tentare la variante della tecnica dell'over-sampling come alternativa al random under-sampling, in quanto permette di evitare la riduzione di cardinalità del training set che inevitabilmente segue al random under-sampling.\n",
        "\n",
        "\n",
        "VAI MICHELE SCELGO TE\n"
      ],
      "metadata": {
        "id": "9p59HqO4MAfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# La seguente cella non riesce ad eseguire in Google Colab (finisce la memoria e riavvia il runtime).\r\n",
        "# I risultati di questa cella sono stati calcolati localmente e salvati su un dataset CSV che è possibile importare grazie alle cella seguente.\r\n",
        "# Imports\r\n",
        "from imblearn.over_sampling import SMOTENC\r\n",
        "\r\n",
        "# Create KMeans-Random instance\r\n",
        "categorical_features_indexes = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]\r\n",
        "smotenc = SMOTENC(categorical_features_indexes, random_state=19, n_jobs=-1)\r\n",
        "\r\n",
        "# Fit and resample imbalanced dataa\r\n",
        "X_SMOTE = train_dataset.loc[:, train_dataset.columns != 'DROPOUT']\r\n",
        "y_SMOTE = train_dataset['DROPOUT']\r\n",
        "X_SMOTE_res, y_SMOTE_res = smotenc.fit_resample(X_SMOTE, y_SMOTE)\r\n",
        "\r\n",
        "no_drop, drop = np.bincount(y_SMOTE['DROPOUT'])\r\n",
        "res_no_drop, res_drop = np.bincount(y_SMOTE_res['DROPOUT'])\r\n",
        "\r\n",
        "print(f'Classe No-drop originale: {len(no_drop):,} - Classe No-drop ricampionata: {len(res_no_drop):,}')\r\n",
        "print(f'Classe Drop originale: {len(drop):,} - Classe Drop ricampionata: {len(res_drop):,}')"
      ],
      "outputs": [],
      "metadata": {
        "id": "v3uTh1BpOxjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if from_gdrive:\r\n",
        "    RESAMPLED_DATASET_PATH = \"Data/resampled_training_dataset.csv\"\r\n",
        "else:\r\n",
        "    RESAMPLED_DATASET_PATH = \"../resampled_training_dataset.csv\" \r\n",
        "\r\n",
        "resampled_train_dataset = pd.read_csv(RESAMPLED_DATASET_PATH, sep=',')"
      ],
      "outputs": [],
      "metadata": {
        "id": "xpizgu1keGmC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "nr_nodrop_sample, nr_drop_sample = np.bincount(resampled_train_dataset['DROPOUT'])\r\n",
        "total_sample = nr_nodrop_sample + nr_drop_sample\r\n",
        "print(f'Nr di istanze: {total_sample:,}\\nIstanze con dropout a True: {nr_drop_sample:,} ({(100 * nr_drop_sample / total_sample):.2f}% del totale)\\nProporzione dropout True v/s False: {round(nr_drop_sample/nr_nodrop_sample, 2)} : 1')"
      ],
      "outputs": [],
      "metadata": {
        "id": "vOAilk3Kgy--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data = pd.Series([nr_nodrop_sample, nr_drop_sample])\r\n",
        "data.plot.pie(autopct=\"%.1f%%\");"
      ],
      "outputs": [],
      "metadata": {
        "id": "YAVimNfAhgFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning"
      ],
      "metadata": {
        "id": "Xa9b5de8aDtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversione del dataframe di Pandas nel tensore di TensorFlow."
      ],
      "metadata": {
        "id": "9oLyHxpuhU7g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def dataframe_to_dataset(dataframe):\r\n",
        "    dataframe = dataframe.copy()\r\n",
        "    dataframe = dataframe.drop(\"Unnamed: 0\", axis=1)\r\n",
        "    labels = dataframe.pop(\"DROPOUT\")\r\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels.values))\r\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\r\n",
        "    return ds"
      ],
      "outputs": [],
      "metadata": {
        "id": "egzMbPotaHo-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#train_dataset = train_dataset.sample(100)\r\n",
        "train_ds = dataframe_to_dataset(train_dataset)\r\n",
        "val_ds = dataframe_to_dataset(validation_dataset)\r\n",
        "test_ds = dataframe_to_dataset(test_dataset)\r\n",
        "\r\n",
        "for x, y in train_ds.take(1):\r\n",
        "    print(\"Input:\", x)\r\n",
        "    print(\"Target:\", y)"
      ],
      "outputs": [],
      "metadata": {
        "id": "NUC2fyiP6zEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divisione del dataset di training in blocchi"
      ],
      "metadata": {
        "id": "PsOInBIxRLBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_ds = train_ds.batch(32, drop_remainder=True)\r\n",
        "val_ds = val_ds.batch(32, drop_remainder=True)\r\n",
        "test_ds = test_ds.batch(32, drop_remainder=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "xRLJH5n8iNKh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import IntegerLookup\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\r\n",
        "\r\n",
        "\r\n",
        "def encode_numerical_feature(feature, name, dataset): # layer, stringa, dataset\r\n",
        "    # Create a Normalization layer for our feature\r\n",
        "    normalizer = Normalization()\r\n",
        "\r\n",
        "    # Prepare a Dataset that only yields our feature\r\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\r\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\r\n",
        "\r\n",
        "    # Learn the statistics of the data\r\n",
        "    normalizer.adapt(feature_ds)\r\n",
        "\r\n",
        "    # Normalize the input feature\r\n",
        "    encoded_feature = normalizer(feature)\r\n",
        "    return encoded_feature\r\n",
        "\r\n",
        "\r\n",
        "def encode_categorical_feature(feature, name, dataset, is_string):\r\n",
        "    lookup_class = StringLookup if is_string else IntegerLookup\r\n",
        "    # Create a lookup layer which will turn strings into integer indices\r\n",
        "    lookup = lookup_class(output_mode=\"binary\")\r\n",
        "\r\n",
        "    # Prepare a Dataset that only yields our feature\r\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\r\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\r\n",
        "\r\n",
        "    # Learn the set of possible string values and assign them a fixed integer index\r\n",
        "    lookup.adapt(feature_ds)\r\n",
        "\r\n",
        "    # Turn the string input into integer indices\r\n",
        "    encoded_feature = lookup(feature)\r\n",
        "    return encoded_feature\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "rexYr65kilq-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Categorical features encoded as integers\r\n",
        "prog = keras.Input(shape=(1,), name=\"prog\", dtype=\"int64\")\r\n",
        "mese = keras.Input(shape=(1,), name=\"mese\", dtype=\"int64\")\r\n",
        "anno = keras.Input(shape=(1,), name=\"anno\", dtype=\"int64\")\r\n",
        "luogo = keras.Input(shape=(1,), name=\"luogo\", dtype=\"int64\")\r\n",
        "eta = keras.Input(shape=(1,), name=\"eta\", dtype=\"int64\")\r\n",
        "freq_asilo_nido = keras.Input(shape=(1,), name=\"freq_asilo_nido\", dtype=\"int64\")\r\n",
        "freq_scuola_materna = keras.Input(shape=(1,), name=\"freq_scuola_materna\", dtype=\"int64\")\r\n",
        "luogo_padre = keras.Input(shape=(1,), name=\"luogo_padre\", dtype=\"int64\")\r\n",
        "titolo_padre = keras.Input(shape=(1,), name=\"titolo_padre\", dtype=\"int64\")\r\n",
        "prof_padre = keras.Input(shape=(1,), name=\"prof_padre\", dtype=\"int64\")\r\n",
        "luogo_madre = keras.Input(shape=(1,), name=\"luogo_madre\", dtype=\"int64\")\r\n",
        "titolo_madre = keras.Input(shape=(1,), name=\"titolo_madre\", dtype=\"int64\")\r\n",
        "prof_madre = keras.Input(shape=(1,), name=\"prof_madre\", dtype=\"int64\")\r\n",
        "regolarita = keras.Input(shape=(1,), name=\"regolarità\", dtype=\"int64\")\r\n",
        "cittadinanza = keras.Input(shape=(1,), name=\"cittadinanza\", dtype=\"int64\")\r\n",
        "cod_provincia_ISTAT = keras.Input(shape=(1,), name=\"cod_provincia_ISTAT\", dtype=\"int64\")\r\n",
        "sigla_provincia_istat = keras.Input(shape=(1,), name=\"sigla_provincia_istat\", dtype=\"int64\")\r\n",
        "Nome_reg = keras.Input(shape=(1,), name=\"Nome_reg\", dtype=\"int64\")\r\n",
        "Cod_reg = keras.Input(shape=(1,), name=\"Cod_reg\", dtype=\"int64\")\r\n",
        "Areageo_3 = keras.Input(shape=(1,), name=\"Areageo_3\", dtype=\"int64\")\r\n",
        "Areageo_4 = keras.Input(shape=(1,), name=\"Areageo_4\", dtype=\"int64\")\r\n",
        "Areageo_5 = keras.Input(shape=(1,), name=\"Areageo_5\", dtype=\"int64\")\r\n",
        "Areageo_5_Istat = keras.Input(shape=(1,), name=\"Areageo_5_Istat\", dtype=\"int64\")\r\n",
        "LIVELLI = keras.Input(shape=(1,), name=\"LIVELLI\", dtype=\"int64\")\r\n",
        "\r\n",
        "sesso = keras.Input(shape=(1,), name=\"sesso\", dtype=\"int64\")\r\n",
        "Pon = keras.Input(shape=(1,), name=\"Pon\", dtype=\"int64\")\r\n",
        "\r\n",
        "# Numerical features\r\n",
        "pu_ma_gr = keras.Input(shape=(1,), name=\"pu_ma_gr\", dtype=\"float32\")\r\n",
        "pu_ma_no = keras.Input(shape=(1,), name=\"pu_ma_no\", dtype=\"float32\")\r\n",
        "Fattore_correzione_new = keras.Input(shape=(1,), name=\"Fattore_correzione_new\", dtype=\"float32\")\r\n",
        "Cheating = keras.Input(shape=(1,), name=\"Cheating\", dtype=\"float32\")\r\n",
        "WLE_MAT = keras.Input(shape=(1,), name=\"WLE_MAT\", dtype=\"float32\")\r\n",
        "WLE_MAT_200 = keras.Input(shape=(1,), name=\"WLE_MAT_200\", dtype=\"float32\")\r\n",
        "WLE_MAT_200_CORR = keras.Input(shape=(1,), name=\"WLE_MAT_200_CORR\", dtype=\"float32\")\r\n",
        "pu_ma_no_corr = keras.Input(shape=(1,), name=\"pu_ma_no_corr\", dtype=\"float32\")\r\n",
        "n_stud_prev = keras.Input(shape=(1,), name=\"n_stud_prev\", dtype=\"float32\")\r\n",
        "Numeri = keras.Input(shape=(1,), name=\"Numeri\", dtype=\"float32\")\r\n",
        "n_classi_prev = keras.Input(shape=(1,), name=\"n_classi_prev\", dtype=\"float32\")\r\n",
        "Dati = keras.Input(shape=(1,), name=\"Dati e previsioni\", dtype=\"float32\")\r\n",
        "Riconoscere_forme = keras.Input(shape=(1,), name=\"Riconoscere le forme nello spazio e utilizzarle per la risoluzione di problemi geometrici o di modellizzazione\", dtype=\"float32\")\r\n",
        "Conoscere_padr = keras.Input(shape=(1,), name=\"Conoscere e padroneggiare i contenuti specifici della matematica\", dtype=\"float32\")\r\n",
        "Relazioni = keras.Input(shape=(1,), name=\"Relazioni e funzioni\", dtype=\"float32\")\r\n",
        "Spazio = keras.Input(shape=(1,), name=\"Spazio figure\", dtype=\"float32\")\r\n",
        "Acquisire = keras.Input(shape=(1,), name=\"Acquisire progressivamente forme tipiche del pensiero matematico\", dtype=\"float32\")\r\n",
        "Conoscere_util = keras.Input(shape=(1,), name=\"Conoscere e utilizzare algoritmi e procedure\", dtype=\"float32\")\r\n",
        "Rappresentare = keras.Input(shape=(1,), name=\"Rappresentare relazioni e dati e, in situazioni significative, utilizzare le rappresentazioni per ricavare informazioni, formulare giudizi e prendere decisioni\", dtype=\"float32\")\r\n",
        "Riconoscere_contesti = keras.Input(shape=(1,), name=\"Riconoscere in contesti diversi il carattere misurabile di oggetti e fenomeni, utilizzare strumenti di misura, misurare grandezze, stimare misure di grandezze\", dtype=\"float32\")\r\n",
        "Risolvere = keras.Input(shape=(1,), name=\"Risolvere problemi utilizzando strategie in ambiti diversi – numerico, geometrico, algebrico –\", dtype=\"float32\")\r\n",
        "Utilizzare = keras.Input(shape=(1,), name=\"Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell'informazione in ambito scientifico, tecnologico, economico e sociale\", dtype=\"float32\")\r\n",
        "\r\n",
        "all_inputs = [\r\n",
        "    prog, \r\n",
        "    sesso, \r\n",
        "    mese, \r\n",
        "    anno, \r\n",
        "    luogo, \r\n",
        "    eta, \r\n",
        "    freq_asilo_nido, \r\n",
        "    freq_scuola_materna, \r\n",
        "    luogo_padre, \r\n",
        "    titolo_padre, \r\n",
        "    prof_padre, \r\n",
        "    luogo_madre, \r\n",
        "    titolo_madre, \r\n",
        "    prof_madre, \r\n",
        "    regolarita,\r\n",
        "    cittadinanza, \r\n",
        "    cod_provincia_ISTAT, \r\n",
        "    sigla_provincia_istat, \r\n",
        "    Nome_reg, \r\n",
        "    Cod_reg, \r\n",
        "    Areageo_3, \r\n",
        "    Areageo_4, \r\n",
        "    Areageo_5, \r\n",
        "    Areageo_5_Istat, \r\n",
        "    Pon, \r\n",
        "    pu_ma_gr, \r\n",
        "    pu_ma_no, \r\n",
        "    Fattore_correzione_new, \r\n",
        "    Cheating, \r\n",
        "    WLE_MAT, \r\n",
        "    WLE_MAT_200, \r\n",
        "    WLE_MAT_200_CORR, \r\n",
        "    pu_ma_no_corr, \r\n",
        "    n_stud_prev, \r\n",
        "    n_classi_prev, \r\n",
        "    LIVELLI, \r\n",
        "    Numeri, \r\n",
        "    Dati, \r\n",
        "    Riconoscere_forme, \r\n",
        "    Conoscere_padr, \r\n",
        "    Relazioni, \r\n",
        "    Spazio, \r\n",
        "    Acquisire, \r\n",
        "    Conoscere_util, \r\n",
        "    Rappresentare, \r\n",
        "    Riconoscere_contesti, \r\n",
        "    Risolvere, \r\n",
        "    Utilizzare     \r\n",
        "]"
      ],
      "outputs": [],
      "metadata": {
        "id": "tWFBtGPFoCyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Integer categorical features\r\n",
        "sesso_encoded = encode_categorical_feature(sesso, \"sesso\", train_ds, False)\r\n",
        "Pon_encoded = encode_categorical_feature(Pon, \"Pon\", train_ds, False)\r\n",
        "\r\n",
        "n_stud_prev_encoded = encode_numerical_feature(n_stud_prev, \"n_stud_prev\", train_ds)\r\n",
        "n_classi_prev_encoded = encode_numerical_feature(n_classi_prev, \"n_classi_prev\", train_ds)\r\n",
        "pu_ma_gr_encoded = encode_numerical_feature(pu_ma_gr, \"pu_ma_gr\", train_ds)\r\n",
        "pu_ma_no_encoded = encode_numerical_feature(pu_ma_no, \"pu_ma_no\", train_ds)\r\n",
        "Fattore_correzione_new_encoded = encode_numerical_feature(Fattore_correzione_new, \"Fattore_correzione_new\", train_ds)\r\n",
        "Cheating_encoded = encode_numerical_feature(Cheating, \"Cheating\", train_ds)\r\n",
        "WLE_MAT_encoded = encode_numerical_feature(WLE_MAT, \"WLE_MAT\", train_ds)\r\n",
        "WLE_MAT_200_encoded = encode_numerical_feature(WLE_MAT_200, \"WLE_MAT_200\", train_ds)\r\n",
        "WLE_MAT_200_CORR_encoded = encode_numerical_feature(WLE_MAT_200_CORR, \"WLE_MAT_200_CORR\", train_ds)\r\n",
        "pu_ma_no_corr_encoded = encode_numerical_feature(pu_ma_no_corr, \"pu_ma_no_corr\", train_ds)\r\n",
        "Numeri_encoded = encode_numerical_feature(Numeri, \"Numeri\", train_ds)\r\n",
        "Dati_encoded = encode_numerical_feature(Dati, \"Dati e previsioni\", train_ds)\r\n",
        "Riconoscere_forme_encoded = encode_numerical_feature(Riconoscere_forme, \"Riconoscere le forme nello spazio e utilizzarle per la risoluzione di problemi geometrici o di modellizzazione\", train_ds)\r\n",
        "Conoscere_padr_encoded = encode_numerical_feature(Conoscere_padr, \"Conoscere e padroneggiare i contenuti specifici della matematica\", train_ds)\r\n",
        "Relazioni_encoded = encode_numerical_feature(Relazioni, \"Relazioni e funzioni\", train_ds)\r\n",
        "Spazio_encoded = encode_numerical_feature(Spazio, \"Spazio figure\", train_ds)\r\n",
        "Acquisire_encoded = encode_numerical_feature(Acquisire, \"Acquisire progressivamente forme tipiche del pensiero matematico\", train_ds)\r\n",
        "Conoscere_util_encoded = encode_numerical_feature(Conoscere_util, \"Conoscere e utilizzare algoritmi e procedure\", train_ds)\r\n",
        "Rappresentare_encoded = encode_numerical_feature(Rappresentare, \"Rappresentare relazioni e dati e, in situazioni significative, utilizzare le rappresentazioni per ricavare informazioni, formulare giudizi e prendere decisioni\", train_ds)\r\n",
        "Riconoscere_contesti_encoded = encode_numerical_feature(Riconoscere_contesti, \"Riconoscere in contesti diversi il carattere misurabile di oggetti e fenomeni, utilizzare strumenti di misura, misurare grandezze, stimare misure di grandezze\", train_ds)\r\n",
        "Risolvere_encoded = encode_numerical_feature(Risolvere, \"Risolvere problemi utilizzando strategie in ambiti diversi – numerico, geometrico, algebrico –\", train_ds)\r\n",
        "Utilizzare_encoded = encode_numerical_feature(Utilizzare, \"Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell'informazione in ambito scientifico, tecnologico, economico e sociale\", train_ds)\r\n",
        "\r\n",
        "prog_encoded = encode_categorical_feature(prog, \"prog\", train_ds, False)\r\n",
        "mese_encoded = encode_categorical_feature(mese, \"mese\", train_ds, False)\r\n",
        "anno_encoded = encode_categorical_feature(anno, \"anno\", train_ds, False)\r\n",
        "luogo_encoded = encode_categorical_feature(luogo, \"luogo\", train_ds, False)\r\n",
        "eta_encoded = encode_categorical_feature(eta, \"eta\", train_ds, False)\r\n",
        "freq_asilo_nido_encoded = encode_categorical_feature(freq_asilo_nido, \"freq_asilo_nido\", train_ds, False)\r\n",
        "freq_scuola_materna_encoded = encode_categorical_feature(freq_scuola_materna, \"freq_scuola_materna\", train_ds, False)\r\n",
        "luogo_padre_encoded = encode_categorical_feature(luogo_padre, \"luogo_padre\", train_ds, False)\r\n",
        "titolo_padre_encoded = encode_categorical_feature(titolo_padre, \"titolo_padre\", train_ds, False)\r\n",
        "prof_padre_encoded = encode_categorical_feature(prof_padre, \"prof_padre\", train_ds, False)\r\n",
        "luogo_madre_encoded = encode_categorical_feature(luogo_madre, \"luogo_madre\", train_ds, False)\r\n",
        "titolo_madre_encoded = encode_categorical_feature(titolo_madre, \"titolo_madre\", train_ds, False)\r\n",
        "prof_madre_encoded = encode_categorical_feature(prof_madre, \"prof_madre\", train_ds, False)\r\n",
        "regolarita_encoded = encode_categorical_feature(regolarita, \"regolarità\", train_ds, False)\r\n",
        "cittadinanza_encoded = encode_categorical_feature(cittadinanza, \"cittadinanza\", train_ds, False)\r\n",
        "cod_provincia_ISTAT_encoded = encode_categorical_feature(cod_provincia_ISTAT, \"cod_provincia_ISTAT\", train_ds, False)\r\n",
        "sigla_provincia_istat_encoded = encode_categorical_feature(sigla_provincia_istat, \"sigla_provincia_istat\", train_ds, False)\r\n",
        "Nome_reg_encoded = encode_categorical_feature(Nome_reg, \"Nome_reg\", train_ds, False)\r\n",
        "Cod_reg_encoded = encode_categorical_feature(Cod_reg, \"Cod_reg\", train_ds, False)\r\n",
        "Areageo_3_encoded = encode_categorical_feature(Areageo_3, \"Areageo_3\", train_ds, False)\r\n",
        "Areageo_4_encoded = encode_categorical_feature(Areageo_4, \"Areageo_4\", train_ds, False)\r\n",
        "Areageo_5_encoded = encode_categorical_feature(Areageo_5, \"Areageo_5\", train_ds, False)\r\n",
        "Areageo_5_Istat_encoded = encode_categorical_feature(Areageo_5_Istat, \"Areageo_5_Istat\", train_ds, False)\r\n",
        "LIVELLI_encoded = encode_categorical_feature(LIVELLI, \"LIVELLI\", train_ds, False)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "srzHs3M0rGy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "all_features = layers.concatenate(\n",
        "    [\n",
        "        prog_encoded, \n",
        "        sesso_encoded, \n",
        "        mese_encoded, \n",
        "        anno_encoded, \n",
        "        luogo_encoded, \n",
        "        eta_encoded, \n",
        "        freq_asilo_nido_encoded, \n",
        "        freq_scuola_materna_encoded, \n",
        "        luogo_padre_encoded, \n",
        "        titolo_padre_encoded, \n",
        "        prof_padre_encoded, \n",
        "        luogo_madre_encoded, \n",
        "        titolo_madre_encoded, \n",
        "        prof_madre_encoded, \n",
        "        regolarita_encoded,\n",
        "        cittadinanza_encoded, \n",
        "        cod_provincia_ISTAT_encoded, \n",
        "        sigla_provincia_istat_encoded, \n",
        "        Nome_reg_encoded, \n",
        "        Cod_reg_encoded, \n",
        "        Areageo_3_encoded, \n",
        "        Areageo_4_encoded, \n",
        "        Areageo_5_encoded, \n",
        "        Areageo_5_Istat_encoded, \n",
        "        Pon_encoded, \n",
        "        pu_ma_gr_encoded, \n",
        "        pu_ma_no_encoded, \n",
        "        Fattore_correzione_new_encoded, \n",
        "        Cheating_encoded, \n",
        "        WLE_MAT_encoded, \n",
        "        WLE_MAT_200_encoded, \n",
        "        WLE_MAT_200_CORR_encoded, \n",
        "        pu_ma_no_corr_encoded, \n",
        "        n_stud_prev_encoded, \n",
        "        n_classi_prev_encoded, \n",
        "        LIVELLI_encoded, \n",
        "        Numeri_encoded, \n",
        "        Dati_encoded, \n",
        "        Riconoscere_forme_encoded, \n",
        "        Conoscere_padr_encoded, \n",
        "        Relazioni_encoded, \n",
        "        Spazio_encoded, \n",
        "        Acquisire_encoded, \n",
        "        Conoscere_util_encoded, \n",
        "        Rappresentare_encoded, \n",
        "        Riconoscere_contesti_encoded, \n",
        "        Risolvere_encoded, \n",
        "        Utilizzare_encoded\n",
        "    ]\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "3lrPyVrGuEDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diverse architetture"
      ],
      "metadata": {
        "id": "cVkCufpss3ps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "\n",
        "initializer = keras.initializers.glorot_uniform(seed=19)"
      ],
      "outputs": [],
      "metadata": {
        "id": "IhQVOp6s4laL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compile_model(model: keras.Model):\n",
        "  model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "              loss=losses.BinaryCrossentropy(),\n",
        "              metrics=[metrics.Accuracy(),\n",
        "                       metrics.Precision(),\n",
        "                       metrics.Recall(),  # metrica più interessante\n",
        "                       metrics.FalseNegatives(),\n",
        "                       metrics.FalsePositives(),\n",
        "                       metrics.TrueNegatives(),\n",
        "                       metrics.TruePositives()])\n",
        "  \n",
        "def plot_model(model: keras.Model):\n",
        "  keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "NAZ_RMIC4qcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architettura 48-(32)-1 (relu - sigmoid) (Adam)"
      ],
      "metadata": {
        "id": "0W3p94ZbtHxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def model1(input_layer, input_data) -> keras.Model:\n",
        "  x = layers.Dense(32, activation=\"relu\", kernel_initializer=initializer)(input_layer)\n",
        "  x = layers.Dropout(0.5, kernel_initializer=initializer)(x)\n",
        "  output = layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)(x)\n",
        "  return keras.Model(input_data, output)"
      ],
      "outputs": [],
      "metadata": {
        "id": "85iF60GkvWuO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Compilazione del modello\n",
        "compile_model(model)\n",
        "# Addestramento del modello\n",
        "model.fit(train_ds, epochs=18, validation_data=val_ds, verbose=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dOP1zTVkve81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "score = model.evaluate(test_ds.batch(32, drop_remainder=True))\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "hIq1nb3xFnbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architettura 48-(32-32)-1 (tanh - tanh - sigmoid) (sgd)\n",
        "\n",
        "https://visualstudiomagazine.com/articles/2018/08/30/neural-binary-classification-keras.aspx\n",
        "\n",
        "The Glorot initialization algorithm is a relatively advanced technique that often works better than a random uniform algorithm."
      ],
      "metadata": {
        "id": "x4WVUAwiuRBX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def model2(input_layer, input_data) -> keras.Model:\n",
        "  x = layers.Dense(32, activation=\"tanh\", kernel_initializer=initializer)(input_layer)\n",
        "  x = layers.Dense(32, activation=\"tanh\", kernel_initializer=initializer)(x)\n",
        "  x = layers.Dropout(0.5, kernel_initializer=initializer)(x)\n",
        "  output = layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer)(x)\n",
        "  return keras.Model(input_data, output)"
      ],
      "outputs": [],
      "metadata": {
        "id": "50deTgNWupLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The demo uses a batch size of 32, which is called mini-batch training. Alternatives are a batch size of one, called online training, and a batch size equal to the size of the training set, called batch training. "
      ],
      "metadata": {
        "id": "eQ3b8PbE0sZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Compilazione del modello\n",
        "compile_model(model)\n",
        "# Addestramento del modello\n",
        "h = model1.fit(train_ds, batch_size=32, epochs=50, validation_data=val_ds, verbose=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "lCn56xr0zrNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The demo captures the return object from fit(), which is a log of training history information, but doesn't use it. This code would save the model using the default hierarchical data format, which you can think of as sort of like a binary XML. It is also possible to save check-point models during training using the custom callback mechanism."
      ],
      "metadata": {
        "id": "UiiVsLt81CpZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "mp = \".\\\\Models\\\\banknote_model.h5\"\n",
        "model.save(mp)"
      ],
      "outputs": [],
      "metadata": {
        "id": "xPxOD5CF1B7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "score = model1.evaluate(test_ds.batch(32, drop_remainder=True))\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "mcmbKtNj1UFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predizioni"
      ],
      "metadata": {
        "id": "pC-Y0_c9ywJD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sample = {\n",
        "    \"age\": 60,\n",
        "    \"sex\": 1,\n",
        "    \"cp\": 1,\n",
        "    \"trestbps\": 145,\n",
        "    \"chol\": 233,\n",
        "    \"fbs\": 1,\n",
        "    \"restecg\": 2,\n",
        "    \"thalach\": 150,\n",
        "    \"exang\": 0,\n",
        "    \"oldpeak\": 2.3,\n",
        "    \"slope\": 3,\n",
        "    \"ca\": 0,\n",
        "    \"thal\": \"fixed\",\n",
        "}\n",
        "\n",
        "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
        "predictions = model.predict(input_dict)\n",
        "\n",
        "print(\n",
        "    \"This particular patient had a %.1f percent probability \"\n",
        "    \"of having a heart disease, as evaluated by our model.\" % (100 * predictions[0][0],)\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "-3RkehPGwJdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESEGUI QUEST'ULTIMA CELLA PER SMONTARE GDRIVE"
      ],
      "metadata": {
        "id": "mVXPbWHEil7M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "outputs": [],
      "metadata": {
        "id": "tpuu1v81gsYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Risultati ottenuti\n",
        "\n",
        "Brevi conclusioni traibili da questi risultati: non cambia quasi nulla con diverse configurazioni del solver. Bisogna per forza cambiare il modello.\n",
        "L'accuratezza 50% viene raggiunta dopo circa 10 epoche (è sostanzialmente stabile da quel momento in poi), tranne softmax che lo è dopo circa 20.\n",
        "\n",
        "## Architettura 48-(32)-1 (relu - sigmoid) (Adam)\n",
        "\n",
        "### Configurazione di default + epoche 30\n",
        "\n",
        "- Accuratezza: 50%\n",
        "- Accuratezza validazione: 34%\n",
        "- Epoche: 30\n",
        "\n",
        "### Configurazione con SGD\n",
        "\n",
        "- Accuratezza: 50%\n",
        "- Accuratezza validazione: 34%\n",
        "- Epoche: 50\n",
        "\n",
        "### Configurazione di default + dropout\n",
        "\n",
        "- Accuratezza: 50%\n",
        "- Accuratezza validazione: 34%\n",
        "- Epoche: 50\n",
        "\n",
        "### Configurazione di default + learning rate 0.01\n",
        "\n",
        "- Accuratezza: 50%\n",
        "- Accuratezza validazione: 34%\n",
        "- Epoche: 50\n",
        "\n",
        "### Configurazione di default + batch size 128\n",
        "\n",
        "- Accuratezza: 50%\n",
        "- Accuratezza validazione: 34%\n",
        "- Epoche: 50\n",
        "\n",
        "### Configurazione di default + activation softmax\n",
        "\n",
        "- Accuratezza: 50%\n",
        "- Accuratezza validazione: 34%\n",
        "- Epoche: 50\n",
        "\n",
        "## Architettura 48-(32-32)-1 (tanh - tanh - sigmoid) (sgd)\n",
        "\n",
        "### Configurazione\n",
        "\n",
        "- Accuratezza: %\n",
        "- Accuratezza validazione: %\n",
        "- Epoche: 50"
      ],
      "metadata": {
        "id": "njCGBlv3djEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Esecuzione su Cluster HPC\n",
        "Per via della numerosità e complessità computazionale dei test nella fase di progettazione dell'architettura della rete, abbiamo pensato di usufruire del servizio di High Performance Computing su cluster dipartimentale con GPU offerto dal dipartimento DISI.\n",
        "\n",
        "Dopo aver fatto richiesta, i nostri account istituzionali sono stati abilitati all'accesso ai sistemi dipartimentali e al cluster stesso: la macchina di nostro interesse è `slurm.cs.unibo.it` su cui si trova lo schedulatore del cluster. \n",
        "\n",
        "In dettaglio, il cluster utilizza uno schedulatore [SLURM](https://slurm.schedmd.com/overview.html) per la distribuzione \n",
        "dei job. Pertanto, per effettuare il submit di un job, abbiamo predisposto nella nostra area di lavoro un file di script \n",
        "SLURM contenente le direttive per la configurazione del job di interesse.\\\n",
        "Sulla scia delle [raccomandazioni](https://disi.unibo.it/it/dipartimento/servizi-tecnici-e-amministrativi/servizi-informatici/utilizzo-cluster-hpc/unibo.tiles.multi.links_attachments/e7809f6f52644346a912b99dd2280788/@@objects-download/11906a7e3d2345278c0fc5ee68ce7975/file/IstruzioniUsoClusterGPU.pdf) contenute nelle istruzioni consegnateci dai tecnici del DISI, abbiamo proceduto col creare sulla macchina slurm un virtual environment \n",
        "Python, in cui, mediante il comando `pip3 install` abbiamo installato le dipendenze necessarie (specificate nel file di testo `requirements.txt`).\n",
        "\n",
        "Abbiamo creato differenti file di script SLURM per le diverse architetture neurali progettate. A titolo di esempio, riportiamo il contenuto dello script di default.\n"
      ],
      "metadata": {
        "id": "xoJagE6ypp6r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#!/bin/bash\n",
        "#SBATCH --job-name=default\n",
        "#SBATCH --mail-type=ALL\n",
        "#SBATCH --mail-user=tommaso.azzalin@studio.unibo.it\n",
        "#SBATCH --time=20:00:00\n",
        "#SBATCH --nodes=1\n",
        "#SBATCH --ntasks-per-node=1\n",
        "#SBATCH --output=default\n",
        "#SBATCH --gres=gpu:1\n",
        "\n",
        "cd  ../\n",
        "\n",
        ". venv/bin/activate # per attivare il virtual environment python\n",
        "\n",
        "pip3 install -r requirements.txt\n",
        "\n",
        "python3 src/base.py"
      ],
      "outputs": [],
      "metadata": {
        "id": "g9-O3abQtneJ"
      }
    }
  ]
}