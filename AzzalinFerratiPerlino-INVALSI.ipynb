{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modello di deep learning per predizione dropout scolastico su dati INVALSI\r\n",
    "Progetto del corso di **Intelligenza Artificiale**, A.A. 2020/2021\r\n",
    "\r\n",
    "**LM Informatica**, **Alma Mater Studiorum - Università di Bologna**\r\n",
    "\r\n",
    "Realizzato da:\r\n",
    "- Marco Ferrati, matr. 983546\r\n",
    "- Michele Perlino, matr. 983733\r\n",
    "- Tommaso Azzalin, matr. 985911"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 - Setup dell'ambiente"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 - Installazione delle librerie necessarie"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip3 install --no-cache-dir -r requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 - Import delle librerie fondamentali per l'analisi dei dati"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\r\n",
    "from beautifultable import BeautifulTable\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import IntegerLookup\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping\r\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\r\n",
    "from imblearn.over_sampling import SMOTENC\r\n",
    "from livelossplot import PlotLossesKeras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Com'è ben noto, il machine learning è un ambito in cui la sperimentazione occupa un ruolo molto importante: una volta consolidati i fondamenti teorici, abbiamo dovuto sperimentare innumerevoli architetture neurali differenti, tracciando di volta in volta le performance raggiunte.  \r\n",
    "Al fine di rendere il codice del notebook quanto più flessibile, abbiamo creato il file Python `config.py` contenente la definizione degli iperparametri della rete; in dettaglio, in questo file, per ogni iperparametro, si verifica l'esistenza di una variabile d'ambiente col medesimo nome: in caso positivo, si assegna il valore di quest'ultima, mentre, in caso negativo, si assegna un valore di default.\r\n",
    "\r\n",
    "Ad esempio:\r\n",
    "\r\n",
    "`LEARNING_RATE = float(getenv(key=\"LEARNING_RATE\", default=\"0.001\"))`\r\n",
    "\r\n",
    "Per impostare variabili d'ambiente in maniera batch, abbiamo creato dei file con estensione `.env`:\r\n",
    "- se si opera da PowerShell, è sufficiente dare il comando `Set-PsEnv` (installabile mediante comando `Install-Module -Name Set-PsEnv`) per definire tutte le variabili d'ambiente presenti nel file `.env` che si trova nella directory corrente;\r\n",
    "- se si opera da una shell Unix, si può impostare una variabile d'ambiente alla volta con `export nome_variabile=valore`: il contenuto di una variabile può essere controllato con `echo $nome_variabile`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import src.config as cfg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Configuration\")\r\n",
    "if cfg.check_config() > 0:\r\n",
    "    print(\"The configuration is incorrect. Please fix it before continuing otherwise you could encounter issues while working with this notebook.\")\r\n",
    "\r\n",
    "cfg.print_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All'interno della directory `src`, abbiamo creato altri due file Python (`mapping_domande_ambiti_processi.py` e `column_converters.py`) contenente del codice che si è voluto separare da quello del notebook, per questioni di ordine e leggibilità. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from src.mapping_domande_ambiti_processi import MAPPING_DOMANDE_AMBITI_PROCESSI\r\n",
    "from src.column_converters import COLUMN_CONVERTERS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nella cartella `src`, di cui sopra, sono presenti altri due file Python `invalsi.py` e `save_plots.py`. Il primo contiene del codice equivalente a quello presente in questo notebook, ma che può essere eseguito da linea di comando come un qualsiasi script Python; la sua creazione si è resa necessaria per l'esecuzione degli script attraverso i job avviati sul Cluster HPC del DISI, descritto nella sezione [Esecuzione su Cluster HPC](#cluster). Il secondo contiene del codice per memorizzare dei grafici sotto forma di file immagine contenenti gli andamenti delle metriche che permettono di valutare il funzionamento di un'architettura neurale."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 - Import del dataset originale"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "\"\"\"\r\n",
    "original_dataset = pd.read_csv(cfg.ORIGINAL_DATASET, sep=';', converters=COLUMN_CONVERTERS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 - Exploratory Data Analysis del dataset originale\r\n",
    "Il codice che segue realizza il processo EDA, acronimo che sta per *Exploratory Data Analysis*, al fine di scandagliare i punti di attenzione del dataset a disposizione."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "original_dataset.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Come restituito dalla funzione `info()`, il dataset presenta 342226 righe e 104 colonne."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "original_dataset.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il dataset contiene svariate informazioni relative a studenti che hanno ultimato il ciclo di studi delle superiori: l'obiettivo del progetto è la progettazione e implementazione di un classificatore capace di predire, sulla base dei risultati conseguiti al test INVALSI di terza media, quali studenti registreranno un *dropout*.  \r\n",
    "Il concetto di *dropout* può essere declinato su due livelli:\r\n",
    "- **implicito**: si dice che lo studente ha registrato un dropout implicito nel caso in cui non abbia acquisito le minime conoscenze e competenze, per cui presenta lacune formative;\r\n",
    "- **esplicito**: si dice che lo studente ha registrato un dropout esplicito nel caso in cui non abbia conseguito il diploma."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "L'obiettivo del progetto è quello di realizzare un predittore, mediante tecniche di deep learning, che in base ai dati di uno studente e i risultati della sua prova INVALSI di terza media, possa predire un eventuale dropout implicito futuro."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 - Ricerca di colonne con alte percentuali di valori nulli"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(\"Columns with high null values percentages:\")\r\n",
    "\r\n",
    "table = BeautifulTable()\r\n",
    "table.columns.header = [\"\", \"Type\",\"Ratio null values\"]\r\n",
    "\r\n",
    "for col in original_dataset.columns :\r\n",
    "    ratio_null_values = original_dataset[col].isnull().mean().round(3)\r\n",
    "    if ratio_null_values > 0:\r\n",
    "        table.rows.append([col, str(original_dataset[col].dtypes), ratio_null_values])\r\n",
    "table.rows.append(['LIVELLI', str(original_dataset['LIVELLI'].dtypes), original_dataset['LIVELLI'].isnull().mean().round(3)])\r\n",
    "table.rows.append(['DROPOUT', str(original_dataset['DROPOUT'].dtypes), original_dataset['DROPOUT'].isnull().mean().round(3)])\r\n",
    "        \r\n",
    "table.columns.alignment = BeautifulTable.ALIGN_LEFT\r\n",
    "table.set_style(BeautifulTable.STYLE_SEPARATED)\r\n",
    "table.rows.sort('Ratio null values')\r\n",
    "print(table)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Columns with high null values percentages:\n",
      "+=======================+=========+===================+\n",
      "|                       | Type    | Ratio null values |\n",
      "+=======================+=========+===================+\n",
      "| LIVELLI               | int64   | 0.0               |\n",
      "+-----------------------+---------+-------------------+\n",
      "| DROPOUT               | int64   | 0.0               |\n",
      "+-----------------------+---------+-------------------+\n",
      "| voto_orale_ita        | float64 | 0.113             |\n",
      "+-----------------------+---------+-------------------+\n",
      "| voto_orale_mat        | float64 | 0.114             |\n",
      "+-----------------------+---------+-------------------+\n",
      "| voto_scritto_ita      | float64 | 0.683             |\n",
      "+-----------------------+---------+-------------------+\n",
      "| voto_scritto_mat      | float64 | 0.683             |\n",
      "+-----------------------+---------+-------------------+\n",
      "| PesoClasse            | float64 | 0.946             |\n",
      "+-----------------------+---------+-------------------+\n",
      "| PesoScuola            | float64 | 0.946             |\n",
      "+-----------------------+---------+-------------------+\n",
      "| PesoTotale_Matematica | float64 | 0.946             |\n",
      "+-----------------------+---------+-------------------+\n",
      "| codice_orario         | float64 | 1.0               |\n",
      "+-----------------------+---------+-------------------+\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "columns_high_ratio_null_values = [\"codice_orario\", \"PesoClasse\", \"PesoScuola\", \"PesoTotale_Matematica\"]\r\n",
    "columns_low_ratio_null_values = [\r\n",
    "    \"voto_scritto_ita\",  # 0.683\r\n",
    "    \"voto_scritto_mat\",  # 0.113\r\n",
    "    \"voto_orale_ita\",  # 0.683\r\n",
    "    \"voto_orale_mat\"  # 0.114\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 - Ricerca colonne con un numero basso di valori distinti "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "print(\"Columns with unique values:\")\r\n",
    "\r\n",
    "table = BeautifulTable()\r\n",
    "table.columns.header = [\"\", \"Ratio distinct values\"]\r\n",
    "\r\n",
    "for col in original_dataset.columns:\r\n",
    "    ratio_unique_vals = round(original_dataset[col].nunique() / len(original_dataset), 3)\r\n",
    "    if ratio_unique_vals > 0.1:\r\n",
    "        table.rows.append([col, ratio_unique_vals]) \r\n",
    "\r\n",
    "table.columns.alignment = BeautifulTable.ALIGN_LEFT\r\n",
    "table.set_style(BeautifulTable.STYLE_SEPARATED)\r\n",
    "table.rows.sort('Ratio distinct values')\r\n",
    "print(table)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Columns with unique values:\n",
      "+==================+=======================+\n",
      "|                  | Ratio distinct values |\n",
      "+==================+=======================+\n",
      "| WLE_MAT_200_CORR | 0.464                 |\n",
      "+------------------+-----------------------+\n",
      "| pu_ma_no_corr    | 0.494                 |\n",
      "+------------------+-----------------------+\n",
      "| Unnamed: 0       | 1.0                   |\n",
      "+------------------+-----------------------+\n",
      "| CODICE_STUDENTE  | 1.0                   |\n",
      "+------------------+-----------------------+\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Come era prevedibile, `Unnamed: 0` (corrispondente alla colonna con l'indice della riga) e `CODICE_STUDENTE` presentano una proporzione di valori distinti sul totale delle righe pari a 1, in quanto trattasi di indici/codici che identificano univocamente gli studenti. Queste due colonne possono essere eliminate, dato che non portano alcuna informazione che possa aiutare a ravvisare relazioni tra gli studenti."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "columns_with_unique_values = [\"Unnamed: 0\", \"CODICE_STUDENTE\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 - Ricerca colonne con un singolo valore"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(\"Columns with just one value:\")\r\n",
    "for col in original_dataset.columns:\r\n",
    "    unique_vals = original_dataset[col].nunique()\r\n",
    "    if unique_vals == 1:\r\n",
    "        print(col)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Columns with just one value:\n",
      "macrotipologia\n",
      "livello\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similmente, queste colonne possono essere eliminate in quanto non distinguono in alcuna maniera gli studenti."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "columns_with_just_one_value = [\"macrotipologia\", \"livello\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 - Rimozione delle colonne superflue"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "\"\"\"\r\n",
    "cleaned_original_dataset: pd.DataFrame = original_dataset.drop(\r\n",
    "    columns_high_ratio_null_values + \r\n",
    "    columns_with_unique_values + \r\n",
    "    columns_with_just_one_value, \r\n",
    "    axis=1\r\n",
    ")\r\n",
    "cleaned_original_dataset.to_csv(cfg.CLEANED_DATASET)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "Attenzione: inutile eseguire se si è eseguita la precedente cella.\r\n",
    "\"\"\"\r\n",
    "cleaned_original_dataset = pd.read_csv(cfg.CLEANED_DATASET)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\azzal\\Progetti\\INVALSI\\MachineLearningProject\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "if \"Unnamed: 0\" in cleaned_original_dataset.columns:\r\n",
    "    cleaned_original_dataset.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 - Generalizzazione dataset per supportare altre coorti di studenti\r\n",
    "Addestrando la rete con le domande specifiche di un test INVALSI di uno specifico anno non si ottiene un classificatore riutilizzabile per coorti successive (le domande cambiano ogni anno). Pertanto, abbiamo pensato di mappare le feature inerenti alle domande in uno spazio più generico che permetta di cogliere la loro semantica piuttosto che la loro rappresentazione letterale; mediante le griglie di correzione fornite ai docenti, abbiamo notato che ogni domanda è caratterizzata da uno o più ambiti e processi: questa corrispondenza ha ispirato la trasformazione dello spazio di rappresentazione delle domande che riportiamo di seguito. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nel file `mapping_domande_ambiti_processi.py` posizionato nella cartella `src` abbiamo creato una mappa (`dict` in Python) le cui chiavi sono le domande e i valori gli ambiti e i processi corrispondenti. Dopo averlo importato nel notebook corrente, abbiamo estratto i distinti ambiti e processi per poi calcolarne il numero di domande che caratterizzano."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "list_ambiti_processi = [AP for val in MAPPING_DOMANDE_AMBITI_PROCESSI.values() for AP in val]\r\n",
    "ambiti_processi = set(list_ambiti_processi)\r\n",
    "conteggio_ambiti_processi = {AP: list_ambiti_processi.count(AP) for AP in ambiti_processi}   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abbiamo proceduto con l'aggiungere al dataset originale colonne recanti il nome degli ambiti e processi, inizializzate a 0 per tutte le righe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "\"\"\"\r\n",
    "dataset_with_ambiti_processi = cleaned_original_dataset.copy()\r\n",
    "for AP in ambiti_processi:\r\n",
    "    dataset_with_ambiti_processi[AP] = 0.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Col codice seguente, ogni studente sarà caratterizzato da un valore per ogni ambito e processo corrispondente alla proporzione di domande che vi si riferivano e a cui ha risposto correttamente: ad esempio, nel caso un processo `x` vada a caratterizzare 10 domande e lo studente risponda correttamente a 5 di queste ultime, allora il valore che quello studente presenterà sotto il processo `x` sarà pari a 5/10 = 0.5."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: rimozione feature sulle domande e aggiunta feature sugli ambiti e processi\r\n",
    "\"\"\"\r\n",
    "questions_columns = [col for col in list(cleaned_original_dataset) if re.search(\"^D\\d\", col)]\r\n",
    "\r\n",
    "for i, row in dataset_with_ambiti_processi.iterrows():\r\n",
    "    for question, APs in MAPPING_DOMANDE_AMBITI_PROCESSI.items():\r\n",
    "        if row[question] is True:   # se ha risposto correttamente\r\n",
    "            for AP in APs:\r\n",
    "                dataset_with_ambiti_processi.at[i, AP] += 1 / conteggio_ambiti_processi[AP]\r\n",
    "\r\n",
    "dataset_ap = dataset_with_ambiti_processi.drop(questions_columns, axis=1)\r\n",
    "\r\n",
    "dataset_ap.to_csv(cfg.CLEANED_DATASET_WITH_AP)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: rimozione feature sulle domande e aggiunta feature sugli ambiti e processi\r\n",
    "Attenzione: inutile eseguire se si è eseguita la precedente cella.\r\n",
    "\"\"\"\r\n",
    "dataset_ap = pd.read_csv(cfg.CLEANED_DATASET_WITH_AP)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "if \"Unnamed: 0\" in dataset_ap.columns:\r\n",
    "    dataset_ap.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.6 - Analisi della correlazione fra feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corr_matrix = dataset_ap.corr(method='pearson').round(2)\r\n",
    "corr_matrix.style.background_gradient(cmap='YlOrRd')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Emerge un'alta correlazione, 0.87, tra i voti della stessa materia, come prevedibile, mentre una correlazione abbastanza alta tra materie differenti, 0.75. La correlazione pari a 1.0 tra `pu_ma_gr` e `pu_ma_no` si spiega considerando che la seconda è la normalizzazione del valore della prima, per cui portano esattamente la stessa informazione.\r\n",
    "\r\n",
    "Abbiamo ritenuto interessante indagare la correlazione sussistente tra i voti agli scritti e agli orali, i punteggi finali ottenuti al test e gli ambiti e i processi: abbiamo previsto solo valori positivi di correlazione, in quanto a voti maggiori, corrispondono punteggi finali maggiori, come anche per i processi e ambiti. Per quanto riguarda invece la correlazione fra gli attributi `LIVELLI` e `DROPOUT`, ci attendiamo una correlazione negativa forte poiché la seconda contiene informazioni estratte dal valore della prima per mezzo della seguente mappatura:\r\n",
    "- `LIVELLI` $\\in$ {0, 1, 2, 3, 4, 5};\r\n",
    "- `DROPOUT` $\\in$ {0, 1};\r\n",
    "- se il valore contenuto nell'attributo `LIVELLI` è $\\leq$ 2 allora quello in `DROPOUT` è 1 (`True`), `0` (`False`) altrimenti."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "interesting_to_check_if_correlated_columns = [\r\n",
    "    # Alta correlazione fra voti della stessa materia, abbastanza correlate fra materie diverse\r\n",
    "    \"voto_scritto_ita\",\r\n",
    "    \"voto_orale_ita\",\r\n",
    "    \"voto_scritto_mat\",\r\n",
    "    \"voto_orale_mat\",\r\n",
    "    # Correlazione totale, abbastanza correlate con voti\r\n",
    "    \"pu_ma_no\",\r\n",
    "    # Target columns\r\n",
    "    \"LIVELLI\",\r\n",
    "    \"DROPOUT\"\r\n",
    "] + list(ambiti_processi)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "check_corr_dataset = dataset_ap[interesting_to_check_if_correlated_columns].copy()\r\n",
    "check_corr_dataset.rename(columns={\r\n",
    "    \"Rappresentare relazioni e dati e, in situazioni significative, utilizzare le rappresentazioni per ricavare informazioni, formulare giudizi e prendere decisioni\": \"Relazioni e dati\",\r\n",
    "    \"Acquisire progressivamente forme tipiche del pensiero matematico\": \"Pensiero matematico\",\r\n",
    "    \"Conoscere e padroneggiare i contenuti specifici della matematica\": \"Contenuti matematica\",\r\n",
    "    \"Utilizzare strumenti, modelli e rappresentazioni nel trattamento quantitativo dell'informazione in ambito scientifico, tecnologico, economico e sociale\": \"Rappresentazione quantitativa\",\r\n",
    "    \"Riconoscere in contesti diversi il carattere misurabile di oggetti e fenomeni, utilizzare strumenti di misura, misurare grandezze, stimare misure di grandezze\": \"Utilizzo strumenti misura\",\r\n",
    "    \"Risolvere problemi utilizzando strategie in ambiti diversi – numerico, geometrico, algebrico –\": \"Risoluzione problemi\",\r\n",
    "    \"Riconoscere le forme nello spazio e utilizzarle per la risoluzione di problemi geometrici o di modellizzazione\": \"Forme nello spazio\",\r\n",
    "    \"Conoscere e utilizzare algoritmi e procedure\": \"Algoritmi e procedure\"\r\n",
    "}, inplace=True)\r\n",
    "corr_result = check_corr_dataset.corr(method='pearson').round(2)\r\n",
    "# Ridenominazione colonne per ragioni di spazio e comprensibilità.\r\n",
    "corr_result.style.background_gradient(cmap='YlOrRd')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a6399_row0_col0, #T_a6399_row1_col1, #T_a6399_row2_col2, #T_a6399_row3_col3, #T_a6399_row4_col4, #T_a6399_row5_col5, #T_a6399_row6_col6, #T_a6399_row7_col7, #T_a6399_row8_col8, #T_a6399_row9_col9, #T_a6399_row10_col10, #T_a6399_row11_col11, #T_a6399_row12_col12, #T_a6399_row13_col13, #T_a6399_row14_col14, #T_a6399_row15_col15, #T_a6399_row16_col16, #T_a6399_row17_col17, #T_a6399_row18_col18 {\n",
       "  background-color: #800026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row0_col1, #T_a6399_row8_col14, #T_a6399_row14_col8 {\n",
       "  background-color: #ae0026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row0_col2, #T_a6399_row13_col17 {\n",
       "  background-color: #ce0c22;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row0_col3, #T_a6399_row9_col4 {\n",
       "  background-color: #df171d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row0_col4, #T_a6399_row9_col13, #T_a6399_row10_col12, #T_a6399_row17_col5 {\n",
       "  background-color: #fc592d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row0_col5, #T_a6399_row10_col11, #T_a6399_row11_col8, #T_a6399_row16_col5 {\n",
       "  background-color: #f54026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row0_col6 {\n",
       "  background-color: #fecc68;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row0_col7 {\n",
       "  background-color: #fece6a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row0_col8, #T_a6399_row3_col12, #T_a6399_row5_col0, #T_a6399_row10_col15, #T_a6399_row15_col2, #T_a6399_row17_col18 {\n",
       "  background-color: #fd8439;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row0_col9, #T_a6399_row15_col1, #T_a6399_row16_col1 {\n",
       "  background-color: #fd9841;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row0_col10, #T_a6399_row0_col14, #T_a6399_row4_col7, #T_a6399_row8_col1, #T_a6399_row12_col9, #T_a6399_row14_col1 {\n",
       "  background-color: #fd863a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row0_col11, #T_a6399_row1_col13, #T_a6399_row1_col14, #T_a6399_row3_col9, #T_a6399_row5_col9, #T_a6399_row10_col1, #T_a6399_row13_col1 {\n",
       "  background-color: #fd8239;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row0_col12, #T_a6399_row2_col15, #T_a6399_row12_col15 {\n",
       "  background-color: #fd9c42;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row0_col13, #T_a6399_row2_col9, #T_a6399_row16_col9, #T_a6399_row17_col10, #T_a6399_row17_col11 {\n",
       "  background-color: #fd8a3b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row0_col15 {\n",
       "  background-color: #feab49;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row0_col16, #T_a6399_row5_col15, #T_a6399_row17_col12 {\n",
       "  background-color: #fea245;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row0_col17, #T_a6399_row5_col7, #T_a6399_row16_col7 {\n",
       "  background-color: #feb953;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row0_col18 {\n",
       "  background-color: #fd7c37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row1_col0 {\n",
       "  background-color: #b00026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row1_col2 {\n",
       "  background-color: #e31a1c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row1_col3 {\n",
       "  background-color: #d30f20;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row1_col4, #T_a6399_row9_col11, #T_a6399_row16_col18 {\n",
       "  background-color: #fc512b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row1_col5 {\n",
       "  background-color: #f23924;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row1_col6, #T_a6399_row11_col6, #T_a6399_row13_col6 {\n",
       "  background-color: #fed16e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row1_col7 {\n",
       "  background-color: #fecb67;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row1_col8, #T_a6399_row15_col16, #T_a6399_row16_col2, #T_a6399_row18_col15 {\n",
       "  background-color: #fd8038;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row1_col9, #T_a6399_row12_col1 {\n",
       "  background-color: #fd933f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row1_col10, #T_a6399_row1_col11, #T_a6399_row11_col1, #T_a6399_row18_col1 {\n",
       "  background-color: #fd7e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row1_col12, #T_a6399_row3_col15, #T_a6399_row7_col8, #T_a6399_row7_col18, #T_a6399_row9_col15, #T_a6399_row12_col0, #T_a6399_row17_col16 {\n",
       "  background-color: #fd9941;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row1_col15 {\n",
       "  background-color: #fea848;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row1_col16, #T_a6399_row15_col0 {\n",
       "  background-color: #fd9e43;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row1_col17 {\n",
       "  background-color: #feb54f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row1_col18, #T_a6399_row9_col2, #T_a6399_row12_col2 {\n",
       "  background-color: #fd7836;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col0, #T_a6399_row9_col10 {\n",
       "  background-color: #d00d21;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col1, #T_a6399_row3_col0, #T_a6399_row3_col5, #T_a6399_row4_col9 {\n",
       "  background-color: #e51e1d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col3 {\n",
       "  background-color: #a80026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col4, #T_a6399_row5_col3, #T_a6399_row9_col5, #T_a6399_row14_col18 {\n",
       "  background-color: #f43d25;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col5 {\n",
       "  background-color: #e9261f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col6 {\n",
       "  background-color: #fed977;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row2_col7 {\n",
       "  background-color: #fec45f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row2_col8, #T_a6399_row9_col8, #T_a6399_row14_col2 {\n",
       "  background-color: #fd6e33;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col10, #T_a6399_row4_col0, #T_a6399_row9_col14, #T_a6399_row14_col16, #T_a6399_row15_col10, #T_a6399_row18_col12 {\n",
       "  background-color: #fc6c33;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col11, #T_a6399_row2_col18, #T_a6399_row3_col13, #T_a6399_row5_col11, #T_a6399_row7_col5, #T_a6399_row13_col2 {\n",
       "  background-color: #fc6330;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col12, #T_a6399_row3_col16, #T_a6399_row5_col12, #T_a6399_row8_col15, #T_a6399_row9_col16, #T_a6399_row14_col15, #T_a6399_row16_col15, #T_a6399_row17_col3, #T_a6399_row17_col14 {\n",
       "  background-color: #fd8c3c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col13, #T_a6399_row8_col2, #T_a6399_row9_col3, #T_a6399_row12_col3, #T_a6399_row12_col13 {\n",
       "  background-color: #fc6a32;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col14, #T_a6399_row5_col1, #T_a6399_row15_col14, #T_a6399_row16_col3, #T_a6399_row18_col9 {\n",
       "  background-color: #fd7234;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row2_col16 {\n",
       "  background-color: #fd903d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row2_col17, #T_a6399_row5_col17, #T_a6399_row9_col17 {\n",
       "  background-color: #feac49;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row3_col1 {\n",
       "  background-color: #d6111f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row3_col2 {\n",
       "  background-color: #aa0026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row3_col4, #T_a6399_row4_col15, #T_a6399_row5_col4 {\n",
       "  background-color: #ef3323;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row3_col6 {\n",
       "  background-color: #fedd7e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row3_col7, #T_a6399_row12_col7, #T_a6399_row15_col7 {\n",
       "  background-color: #fec15d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row3_col8, #T_a6399_row5_col13, #T_a6399_row9_col18, #T_a6399_row10_col2, #T_a6399_row15_col18 {\n",
       "  background-color: #fc6631;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row3_col10, #T_a6399_row4_col1, #T_a6399_row5_col10, #T_a6399_row5_col14, #T_a6399_row16_col14 {\n",
       "  background-color: #fc612f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row3_col11, #T_a6399_row7_col4, #T_a6399_row8_col3, #T_a6399_row11_col2, #T_a6399_row14_col3, #T_a6399_row16_col10, #T_a6399_row18_col16 {\n",
       "  background-color: #fc5d2e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row3_col14, #T_a6399_row8_col12, #T_a6399_row8_col16, #T_a6399_row10_col16, #T_a6399_row12_col14 {\n",
       "  background-color: #fc6832;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row3_col17, #T_a6399_row15_col17 {\n",
       "  background-color: #fea747;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row3_col18, #T_a6399_row12_col8, #T_a6399_row13_col15, #T_a6399_row16_col8 {\n",
       "  background-color: #fc5b2e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col2, #T_a6399_row10_col7, #T_a6399_row12_col5 {\n",
       "  background-color: #f64227;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col3 {\n",
       "  background-color: #f03523;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col5 {\n",
       "  background-color: #e1191d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col6 {\n",
       "  background-color: #fede80;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row4_col8 {\n",
       "  background-color: #cb0a22;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col10 {\n",
       "  background-color: #c30424;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col11 {\n",
       "  background-color: #bd0026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col12 {\n",
       "  background-color: #e2191c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col13 {\n",
       "  background-color: #c40524;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col14 {\n",
       "  background-color: #cd0b22;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col16, #T_a6399_row8_col18, #T_a6399_row18_col8 {\n",
       "  background-color: #e41c1d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col17, #T_a6399_row8_col10, #T_a6399_row11_col14, #T_a6399_row13_col10 {\n",
       "  background-color: #f84528;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row4_col18, #T_a6399_row8_col4, #T_a6399_row14_col4, #T_a6399_row17_col13 {\n",
       "  background-color: #c90823;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row5_col2, #T_a6399_row8_col13, #T_a6399_row12_col10 {\n",
       "  background-color: #fc4d2a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row5_col6, #T_a6399_row6_col0, #T_a6399_row6_col1, #T_a6399_row6_col2, #T_a6399_row6_col3, #T_a6399_row6_col4, #T_a6399_row6_col5, #T_a6399_row6_col7, #T_a6399_row6_col8, #T_a6399_row6_col9, #T_a6399_row6_col10, #T_a6399_row6_col11, #T_a6399_row6_col12, #T_a6399_row6_col13, #T_a6399_row6_col14, #T_a6399_row6_col15, #T_a6399_row6_col16, #T_a6399_row6_col17, #T_a6399_row6_col18 {\n",
       "  background-color: #ffffcc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row5_col8, #T_a6399_row5_col18, #T_a6399_row12_col18, #T_a6399_row13_col9, #T_a6399_row18_col2 {\n",
       "  background-color: #fc5f2f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row5_col16, #T_a6399_row9_col12, #T_a6399_row11_col0, #T_a6399_row12_col16, #T_a6399_row16_col12, #T_a6399_row17_col8, #T_a6399_row18_col0 {\n",
       "  background-color: #fd883b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row7_col0 {\n",
       "  background-color: #feba55;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col1 {\n",
       "  background-color: #feb34d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col2 {\n",
       "  background-color: #fea647;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col3, #T_a6399_row7_col13, #T_a6399_row14_col17, #T_a6399_row17_col9 {\n",
       "  background-color: #fd9f44;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col6 {\n",
       "  background-color: #feb852;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col9 {\n",
       "  background-color: #fead4a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col10, #T_a6399_row8_col5, #T_a6399_row10_col5, #T_a6399_row11_col5, #T_a6399_row14_col5, #T_a6399_row18_col5 {\n",
       "  background-color: #ed3022;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row7_col11, #T_a6399_row8_col17, #T_a6399_row10_col17 {\n",
       "  background-color: #fd9d43;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col12 {\n",
       "  background-color: #feb04b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col14, #T_a6399_row11_col17, #T_a6399_row18_col17 {\n",
       "  background-color: #fd9a42;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col15, #T_a6399_row17_col6 {\n",
       "  background-color: #febb56;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col16, #T_a6399_row17_col0 {\n",
       "  background-color: #fea948;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row7_col17 {\n",
       "  background-color: #fec561;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row8_col0, #T_a6399_row10_col0, #T_a6399_row13_col0, #T_a6399_row14_col0 {\n",
       "  background-color: #fd8e3c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row8_col6, #T_a6399_row18_col6 {\n",
       "  background-color: #fed673;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row8_col7, #T_a6399_row11_col7, #T_a6399_row14_col7, #T_a6399_row18_col7 {\n",
       "  background-color: #feb24c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row8_col9, #T_a6399_row15_col3 {\n",
       "  background-color: #fd7a37;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row8_col11, #T_a6399_row10_col8, #T_a6399_row11_col13, #T_a6399_row13_col11 {\n",
       "  background-color: #f74327;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row9_col0 {\n",
       "  background-color: #fd9740;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row9_col1, #T_a6399_row15_col9 {\n",
       "  background-color: #fd8f3d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row9_col6 {\n",
       "  background-color: #feca66;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row9_col7 {\n",
       "  background-color: #febe59;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row10_col3, #T_a6399_row11_col9, #T_a6399_row13_col3 {\n",
       "  background-color: #fc572c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row10_col4, #T_a6399_row13_col4 {\n",
       "  background-color: #c10325;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row10_col6, #T_a6399_row14_col6 {\n",
       "  background-color: #fed572;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row10_col9 {\n",
       "  background-color: #d41020;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row10_col13, #T_a6399_row14_col11 {\n",
       "  background-color: #f84628;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row10_col14, #T_a6399_row11_col18, #T_a6399_row14_col10 {\n",
       "  background-color: #e7231e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row10_col18, #T_a6399_row18_col10 {\n",
       "  background-color: #f33b25;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row11_col3, #T_a6399_row15_col5 {\n",
       "  background-color: #fc4f2a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row11_col4 {\n",
       "  background-color: #b90026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row11_col10, #T_a6399_row18_col14 {\n",
       "  background-color: #f43e26;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row11_col12 {\n",
       "  background-color: #c80723;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row11_col15, #T_a6399_row11_col16 {\n",
       "  background-color: #ec2c21;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row12_col4 {\n",
       "  background-color: #da141e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row12_col6, #T_a6399_row16_col6, #T_a6399_row17_col7 {\n",
       "  background-color: #fec965;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row12_col11, #T_a6399_row18_col4 {\n",
       "  background-color: #c70723;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row12_col17 {\n",
       "  background-color: #feae4a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row13_col5, #T_a6399_row16_col13 {\n",
       "  background-color: #ee3122;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row13_col7 {\n",
       "  background-color: #feb44e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row13_col8, #T_a6399_row14_col13, #T_a6399_row15_col13 {\n",
       "  background-color: #fa4a29;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row13_col12, #T_a6399_row14_col12 {\n",
       "  background-color: #fd7435;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row13_col14 {\n",
       "  background-color: #f94828;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row13_col16 {\n",
       "  background-color: #f13624;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row13_col18, #T_a6399_row15_col4 {\n",
       "  background-color: #e61f1d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row14_col9 {\n",
       "  background-color: #fd7636;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row15_col6 {\n",
       "  background-color: #febd57;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row15_col8 {\n",
       "  background-color: #fd7034;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row15_col11, #T_a6399_row18_col13 {\n",
       "  background-color: #e6211e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row15_col12 {\n",
       "  background-color: #fd923e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row16_col0 {\n",
       "  background-color: #fea044;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row16_col4 {\n",
       "  background-color: #dc151e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row16_col11 {\n",
       "  background-color: #e92720;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row16_col17, #T_a6399_row17_col15 {\n",
       "  background-color: #fea546;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row17_col1 {\n",
       "  background-color: #fea446;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row17_col2 {\n",
       "  background-color: #fd953f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a6399_row17_col4 {\n",
       "  background-color: #ed2e21;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row18_col3 {\n",
       "  background-color: #fc532b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a6399_row18_col11 {\n",
       "  background-color: #e8241f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a6399_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >voto_scritto_ita</th>\n",
       "      <th class=\"col_heading level0 col1\" >voto_orale_ita</th>\n",
       "      <th class=\"col_heading level0 col2\" >voto_scritto_mat</th>\n",
       "      <th class=\"col_heading level0 col3\" >voto_orale_mat</th>\n",
       "      <th class=\"col_heading level0 col4\" >pu_ma_no</th>\n",
       "      <th class=\"col_heading level0 col5\" >LIVELLI</th>\n",
       "      <th class=\"col_heading level0 col6\" >DROPOUT</th>\n",
       "      <th class=\"col_heading level0 col7\" >Relazioni e dati</th>\n",
       "      <th class=\"col_heading level0 col8\" >Dati e previsioni</th>\n",
       "      <th class=\"col_heading level0 col9\" >Pensiero matematico</th>\n",
       "      <th class=\"col_heading level0 col10\" >Relazioni e funzioni</th>\n",
       "      <th class=\"col_heading level0 col11\" >Numeri</th>\n",
       "      <th class=\"col_heading level0 col12\" >Contenuti matematica</th>\n",
       "      <th class=\"col_heading level0 col13\" >Spazio figure</th>\n",
       "      <th class=\"col_heading level0 col14\" >Rappresentazione quantitativa</th>\n",
       "      <th class=\"col_heading level0 col15\" >Utilizzo strumenti misura</th>\n",
       "      <th class=\"col_heading level0 col16\" >Risoluzione problemi</th>\n",
       "      <th class=\"col_heading level0 col17\" >Forme nello spazio</th>\n",
       "      <th class=\"col_heading level0 col18\" >Algoritmi e procedure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row0\" class=\"row_heading level0 row0\" >voto_scritto_ita</th>\n",
       "      <td id=\"T_a6399_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row0_col1\" class=\"data row0 col1\" >0.870000</td>\n",
       "      <td id=\"T_a6399_row0_col2\" class=\"data row0 col2\" >0.750000</td>\n",
       "      <td id=\"T_a6399_row0_col3\" class=\"data row0 col3\" >0.660000</td>\n",
       "      <td id=\"T_a6399_row0_col4\" class=\"data row0 col4\" >0.430000</td>\n",
       "      <td id=\"T_a6399_row0_col5\" class=\"data row0 col5\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row0_col6\" class=\"data row0 col6\" >-0.310000</td>\n",
       "      <td id=\"T_a6399_row0_col7\" class=\"data row0 col7\" >0.150000</td>\n",
       "      <td id=\"T_a6399_row0_col8\" class=\"data row0 col8\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row0_col9\" class=\"data row0 col9\" >0.300000</td>\n",
       "      <td id=\"T_a6399_row0_col10\" class=\"data row0 col10\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row0_col11\" class=\"data row0 col11\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row0_col12\" class=\"data row0 col12\" >0.290000</td>\n",
       "      <td id=\"T_a6399_row0_col13\" class=\"data row0 col13\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row0_col14\" class=\"data row0 col14\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row0_col15\" class=\"data row0 col15\" >0.270000</td>\n",
       "      <td id=\"T_a6399_row0_col16\" class=\"data row0 col16\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row0_col17\" class=\"data row0 col17\" >0.220000</td>\n",
       "      <td id=\"T_a6399_row0_col18\" class=\"data row0 col18\" >0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row1\" class=\"row_heading level0 row1\" >voto_orale_ita</th>\n",
       "      <td id=\"T_a6399_row1_col0\" class=\"data row1 col0\" >0.870000</td>\n",
       "      <td id=\"T_a6399_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row1_col2\" class=\"data row1 col2\" >0.650000</td>\n",
       "      <td id=\"T_a6399_row1_col3\" class=\"data row1 col3\" >0.720000</td>\n",
       "      <td id=\"T_a6399_row1_col4\" class=\"data row1 col4\" >0.450000</td>\n",
       "      <td id=\"T_a6399_row1_col5\" class=\"data row1 col5\" >0.400000</td>\n",
       "      <td id=\"T_a6399_row1_col6\" class=\"data row1 col6\" >-0.340000</td>\n",
       "      <td id=\"T_a6399_row1_col7\" class=\"data row1 col7\" >0.160000</td>\n",
       "      <td id=\"T_a6399_row1_col8\" class=\"data row1 col8\" >0.350000</td>\n",
       "      <td id=\"T_a6399_row1_col9\" class=\"data row1 col9\" >0.320000</td>\n",
       "      <td id=\"T_a6399_row1_col10\" class=\"data row1 col10\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row1_col11\" class=\"data row1 col11\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row1_col12\" class=\"data row1 col12\" >0.300000</td>\n",
       "      <td id=\"T_a6399_row1_col13\" class=\"data row1 col13\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row1_col14\" class=\"data row1 col14\" >0.350000</td>\n",
       "      <td id=\"T_a6399_row1_col15\" class=\"data row1 col15\" >0.280000</td>\n",
       "      <td id=\"T_a6399_row1_col16\" class=\"data row1 col16\" >0.280000</td>\n",
       "      <td id=\"T_a6399_row1_col17\" class=\"data row1 col17\" >0.230000</td>\n",
       "      <td id=\"T_a6399_row1_col18\" class=\"data row1 col18\" >0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row2\" class=\"row_heading level0 row2\" >voto_scritto_mat</th>\n",
       "      <td id=\"T_a6399_row2_col0\" class=\"data row2 col0\" >0.750000</td>\n",
       "      <td id=\"T_a6399_row2_col1\" class=\"data row2 col1\" >0.650000</td>\n",
       "      <td id=\"T_a6399_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row2_col3\" class=\"data row2 col3\" >0.880000</td>\n",
       "      <td id=\"T_a6399_row2_col4\" class=\"data row2 col4\" >0.520000</td>\n",
       "      <td id=\"T_a6399_row2_col5\" class=\"data row2 col5\" >0.480000</td>\n",
       "      <td id=\"T_a6399_row2_col6\" class=\"data row2 col6\" >-0.390000</td>\n",
       "      <td id=\"T_a6399_row2_col7\" class=\"data row2 col7\" >0.190000</td>\n",
       "      <td id=\"T_a6399_row2_col8\" class=\"data row2 col8\" >0.400000</td>\n",
       "      <td id=\"T_a6399_row2_col9\" class=\"data row2 col9\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row2_col10\" class=\"data row2 col10\" >0.410000</td>\n",
       "      <td id=\"T_a6399_row2_col11\" class=\"data row2 col11\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row2_col12\" class=\"data row2 col12\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row2_col13\" class=\"data row2 col13\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row2_col14\" class=\"data row2 col14\" >0.390000</td>\n",
       "      <td id=\"T_a6399_row2_col15\" class=\"data row2 col15\" >0.330000</td>\n",
       "      <td id=\"T_a6399_row2_col16\" class=\"data row2 col16\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row2_col17\" class=\"data row2 col17\" >0.270000</td>\n",
       "      <td id=\"T_a6399_row2_col18\" class=\"data row2 col18\" >0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row3\" class=\"row_heading level0 row3\" >voto_orale_mat</th>\n",
       "      <td id=\"T_a6399_row3_col0\" class=\"data row3 col0\" >0.660000</td>\n",
       "      <td id=\"T_a6399_row3_col1\" class=\"data row3 col1\" >0.720000</td>\n",
       "      <td id=\"T_a6399_row3_col2\" class=\"data row3 col2\" >0.880000</td>\n",
       "      <td id=\"T_a6399_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row3_col4\" class=\"data row3 col4\" >0.550000</td>\n",
       "      <td id=\"T_a6399_row3_col5\" class=\"data row3 col5\" >0.520000</td>\n",
       "      <td id=\"T_a6399_row3_col6\" class=\"data row3 col6\" >-0.430000</td>\n",
       "      <td id=\"T_a6399_row3_col7\" class=\"data row3 col7\" >0.200000</td>\n",
       "      <td id=\"T_a6399_row3_col8\" class=\"data row3 col8\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row3_col9\" class=\"data row3 col9\" >0.380000</td>\n",
       "      <td id=\"T_a6399_row3_col10\" class=\"data row3 col10\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row3_col11\" class=\"data row3 col11\" >0.460000</td>\n",
       "      <td id=\"T_a6399_row3_col12\" class=\"data row3 col12\" >0.380000</td>\n",
       "      <td id=\"T_a6399_row3_col13\" class=\"data row3 col13\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row3_col14\" class=\"data row3 col14\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row3_col15\" class=\"data row3 col15\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row3_col16\" class=\"data row3 col16\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row3_col17\" class=\"data row3 col17\" >0.290000</td>\n",
       "      <td id=\"T_a6399_row3_col18\" class=\"data row3 col18\" >0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row4\" class=\"row_heading level0 row4\" >pu_ma_no</th>\n",
       "      <td id=\"T_a6399_row4_col0\" class=\"data row4 col0\" >0.430000</td>\n",
       "      <td id=\"T_a6399_row4_col1\" class=\"data row4 col1\" >0.450000</td>\n",
       "      <td id=\"T_a6399_row4_col2\" class=\"data row4 col2\" >0.520000</td>\n",
       "      <td id=\"T_a6399_row4_col3\" class=\"data row4 col3\" >0.550000</td>\n",
       "      <td id=\"T_a6399_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row4_col5\" class=\"data row4 col5\" >0.550000</td>\n",
       "      <td id=\"T_a6399_row4_col6\" class=\"data row4 col6\" >-0.440000</td>\n",
       "      <td id=\"T_a6399_row4_col7\" class=\"data row4 col7\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row4_col8\" class=\"data row4 col8\" >0.760000</td>\n",
       "      <td id=\"T_a6399_row4_col9\" class=\"data row4 col9\" >0.660000</td>\n",
       "      <td id=\"T_a6399_row4_col10\" class=\"data row4 col10\" >0.800000</td>\n",
       "      <td id=\"T_a6399_row4_col11\" class=\"data row4 col11\" >0.830000</td>\n",
       "      <td id=\"T_a6399_row4_col12\" class=\"data row4 col12\" >0.680000</td>\n",
       "      <td id=\"T_a6399_row4_col13\" class=\"data row4 col13\" >0.800000</td>\n",
       "      <td id=\"T_a6399_row4_col14\" class=\"data row4 col14\" >0.760000</td>\n",
       "      <td id=\"T_a6399_row4_col15\" class=\"data row4 col15\" >0.620000</td>\n",
       "      <td id=\"T_a6399_row4_col16\" class=\"data row4 col16\" >0.670000</td>\n",
       "      <td id=\"T_a6399_row4_col17\" class=\"data row4 col17\" >0.570000</td>\n",
       "      <td id=\"T_a6399_row4_col18\" class=\"data row4 col18\" >0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row5\" class=\"row_heading level0 row5\" >LIVELLI</th>\n",
       "      <td id=\"T_a6399_row5_col0\" class=\"data row5 col0\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row5_col1\" class=\"data row5 col1\" >0.400000</td>\n",
       "      <td id=\"T_a6399_row5_col2\" class=\"data row5 col2\" >0.480000</td>\n",
       "      <td id=\"T_a6399_row5_col3\" class=\"data row5 col3\" >0.520000</td>\n",
       "      <td id=\"T_a6399_row5_col4\" class=\"data row5 col4\" >0.550000</td>\n",
       "      <td id=\"T_a6399_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row5_col6\" class=\"data row5 col6\" >-0.850000</td>\n",
       "      <td id=\"T_a6399_row5_col7\" class=\"data row5 col7\" >0.230000</td>\n",
       "      <td id=\"T_a6399_row5_col8\" class=\"data row5 col8\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row5_col9\" class=\"data row5 col9\" >0.380000</td>\n",
       "      <td id=\"T_a6399_row5_col10\" class=\"data row5 col10\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row5_col11\" class=\"data row5 col11\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row5_col12\" class=\"data row5 col12\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row5_col13\" class=\"data row5 col13\" >0.430000</td>\n",
       "      <td id=\"T_a6399_row5_col14\" class=\"data row5 col14\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row5_col15\" class=\"data row5 col15\" >0.300000</td>\n",
       "      <td id=\"T_a6399_row5_col16\" class=\"data row5 col16\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row5_col17\" class=\"data row5 col17\" >0.270000</td>\n",
       "      <td id=\"T_a6399_row5_col18\" class=\"data row5 col18\" >0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row6\" class=\"row_heading level0 row6\" >DROPOUT</th>\n",
       "      <td id=\"T_a6399_row6_col0\" class=\"data row6 col0\" >-0.310000</td>\n",
       "      <td id=\"T_a6399_row6_col1\" class=\"data row6 col1\" >-0.340000</td>\n",
       "      <td id=\"T_a6399_row6_col2\" class=\"data row6 col2\" >-0.390000</td>\n",
       "      <td id=\"T_a6399_row6_col3\" class=\"data row6 col3\" >-0.430000</td>\n",
       "      <td id=\"T_a6399_row6_col4\" class=\"data row6 col4\" >-0.440000</td>\n",
       "      <td id=\"T_a6399_row6_col5\" class=\"data row6 col5\" >-0.850000</td>\n",
       "      <td id=\"T_a6399_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row6_col7\" class=\"data row6 col7\" >-0.190000</td>\n",
       "      <td id=\"T_a6399_row6_col8\" class=\"data row6 col8\" >-0.370000</td>\n",
       "      <td id=\"T_a6399_row6_col9\" class=\"data row6 col9\" >-0.300000</td>\n",
       "      <td id=\"T_a6399_row6_col10\" class=\"data row6 col10\" >-0.360000</td>\n",
       "      <td id=\"T_a6399_row6_col11\" class=\"data row6 col11\" >-0.340000</td>\n",
       "      <td id=\"T_a6399_row6_col12\" class=\"data row6 col12\" >-0.290000</td>\n",
       "      <td id=\"T_a6399_row6_col13\" class=\"data row6 col13\" >-0.340000</td>\n",
       "      <td id=\"T_a6399_row6_col14\" class=\"data row6 col14\" >-0.360000</td>\n",
       "      <td id=\"T_a6399_row6_col15\" class=\"data row6 col15\" >-0.220000</td>\n",
       "      <td id=\"T_a6399_row6_col16\" class=\"data row6 col16\" >-0.290000</td>\n",
       "      <td id=\"T_a6399_row6_col17\" class=\"data row6 col17\" >-0.210000</td>\n",
       "      <td id=\"T_a6399_row6_col18\" class=\"data row6 col18\" >-0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row7\" class=\"row_heading level0 row7\" >Relazioni e dati</th>\n",
       "      <td id=\"T_a6399_row7_col0\" class=\"data row7 col0\" >0.150000</td>\n",
       "      <td id=\"T_a6399_row7_col1\" class=\"data row7 col1\" >0.160000</td>\n",
       "      <td id=\"T_a6399_row7_col2\" class=\"data row7 col2\" >0.190000</td>\n",
       "      <td id=\"T_a6399_row7_col3\" class=\"data row7 col3\" >0.200000</td>\n",
       "      <td id=\"T_a6399_row7_col4\" class=\"data row7 col4\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row7_col5\" class=\"data row7 col5\" >0.230000</td>\n",
       "      <td id=\"T_a6399_row7_col6\" class=\"data row7 col6\" >-0.190000</td>\n",
       "      <td id=\"T_a6399_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row7_col8\" class=\"data row7 col8\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row7_col9\" class=\"data row7 col9\" >0.210000</td>\n",
       "      <td id=\"T_a6399_row7_col10\" class=\"data row7 col10\" >0.590000</td>\n",
       "      <td id=\"T_a6399_row7_col11\" class=\"data row7 col11\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row7_col12\" class=\"data row7 col12\" >0.200000</td>\n",
       "      <td id=\"T_a6399_row7_col13\" class=\"data row7 col13\" >0.250000</td>\n",
       "      <td id=\"T_a6399_row7_col14\" class=\"data row7 col14\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row7_col15\" class=\"data row7 col15\" >0.200000</td>\n",
       "      <td id=\"T_a6399_row7_col16\" class=\"data row7 col16\" >0.230000</td>\n",
       "      <td id=\"T_a6399_row7_col17\" class=\"data row7 col17\" >0.170000</td>\n",
       "      <td id=\"T_a6399_row7_col18\" class=\"data row7 col18\" >0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row8\" class=\"row_heading level0 row8\" >Dati e previsioni</th>\n",
       "      <td id=\"T_a6399_row8_col0\" class=\"data row8 col0\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row8_col1\" class=\"data row8 col1\" >0.350000</td>\n",
       "      <td id=\"T_a6399_row8_col2\" class=\"data row8 col2\" >0.400000</td>\n",
       "      <td id=\"T_a6399_row8_col3\" class=\"data row8 col3\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row8_col4\" class=\"data row8 col4\" >0.760000</td>\n",
       "      <td id=\"T_a6399_row8_col5\" class=\"data row8 col5\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row8_col6\" class=\"data row8 col6\" >-0.370000</td>\n",
       "      <td id=\"T_a6399_row8_col7\" class=\"data row8 col7\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row8_col9\" class=\"data row8 col9\" >0.400000</td>\n",
       "      <td id=\"T_a6399_row8_col10\" class=\"data row8 col10\" >0.520000</td>\n",
       "      <td id=\"T_a6399_row8_col11\" class=\"data row8 col11\" >0.530000</td>\n",
       "      <td id=\"T_a6399_row8_col12\" class=\"data row8 col12\" >0.450000</td>\n",
       "      <td id=\"T_a6399_row8_col13\" class=\"data row8 col13\" >0.500000</td>\n",
       "      <td id=\"T_a6399_row8_col14\" class=\"data row8 col14\" >0.870000</td>\n",
       "      <td id=\"T_a6399_row8_col15\" class=\"data row8 col15\" >0.390000</td>\n",
       "      <td id=\"T_a6399_row8_col16\" class=\"data row8 col16\" >0.450000</td>\n",
       "      <td id=\"T_a6399_row8_col17\" class=\"data row8 col17\" >0.330000</td>\n",
       "      <td id=\"T_a6399_row8_col18\" class=\"data row8 col18\" >0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row9\" class=\"row_heading level0 row9\" >Pensiero matematico</th>\n",
       "      <td id=\"T_a6399_row9_col0\" class=\"data row9 col0\" >0.300000</td>\n",
       "      <td id=\"T_a6399_row9_col1\" class=\"data row9 col1\" >0.320000</td>\n",
       "      <td id=\"T_a6399_row9_col2\" class=\"data row9 col2\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row9_col3\" class=\"data row9 col3\" >0.380000</td>\n",
       "      <td id=\"T_a6399_row9_col4\" class=\"data row9 col4\" >0.660000</td>\n",
       "      <td id=\"T_a6399_row9_col5\" class=\"data row9 col5\" >0.380000</td>\n",
       "      <td id=\"T_a6399_row9_col6\" class=\"data row9 col6\" >-0.300000</td>\n",
       "      <td id=\"T_a6399_row9_col7\" class=\"data row9 col7\" >0.210000</td>\n",
       "      <td id=\"T_a6399_row9_col8\" class=\"data row9 col8\" >0.400000</td>\n",
       "      <td id=\"T_a6399_row9_col9\" class=\"data row9 col9\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row9_col10\" class=\"data row9 col10\" >0.740000</td>\n",
       "      <td id=\"T_a6399_row9_col11\" class=\"data row9 col11\" >0.490000</td>\n",
       "      <td id=\"T_a6399_row9_col12\" class=\"data row9 col12\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row9_col13\" class=\"data row9 col13\" >0.470000</td>\n",
       "      <td id=\"T_a6399_row9_col14\" class=\"data row9 col14\" >0.410000</td>\n",
       "      <td id=\"T_a6399_row9_col15\" class=\"data row9 col15\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row9_col16\" class=\"data row9 col16\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row9_col17\" class=\"data row9 col17\" >0.270000</td>\n",
       "      <td id=\"T_a6399_row9_col18\" class=\"data row9 col18\" >0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row10\" class=\"row_heading level0 row10\" >Relazioni e funzioni</th>\n",
       "      <td id=\"T_a6399_row10_col0\" class=\"data row10 col0\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row10_col1\" class=\"data row10 col1\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row10_col2\" class=\"data row10 col2\" >0.410000</td>\n",
       "      <td id=\"T_a6399_row10_col3\" class=\"data row10 col3\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row10_col4\" class=\"data row10 col4\" >0.800000</td>\n",
       "      <td id=\"T_a6399_row10_col5\" class=\"data row10 col5\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row10_col6\" class=\"data row10 col6\" >-0.360000</td>\n",
       "      <td id=\"T_a6399_row10_col7\" class=\"data row10 col7\" >0.590000</td>\n",
       "      <td id=\"T_a6399_row10_col8\" class=\"data row10 col8\" >0.520000</td>\n",
       "      <td id=\"T_a6399_row10_col9\" class=\"data row10 col9\" >0.740000</td>\n",
       "      <td id=\"T_a6399_row10_col10\" class=\"data row10 col10\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row10_col11\" class=\"data row10 col11\" >0.540000</td>\n",
       "      <td id=\"T_a6399_row10_col12\" class=\"data row10 col12\" >0.490000</td>\n",
       "      <td id=\"T_a6399_row10_col13\" class=\"data row10 col13\" >0.520000</td>\n",
       "      <td id=\"T_a6399_row10_col14\" class=\"data row10 col14\" >0.630000</td>\n",
       "      <td id=\"T_a6399_row10_col15\" class=\"data row10 col15\" >0.410000</td>\n",
       "      <td id=\"T_a6399_row10_col16\" class=\"data row10 col16\" >0.450000</td>\n",
       "      <td id=\"T_a6399_row10_col17\" class=\"data row10 col17\" >0.330000</td>\n",
       "      <td id=\"T_a6399_row10_col18\" class=\"data row10 col18\" >0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row11\" class=\"row_heading level0 row11\" >Numeri</th>\n",
       "      <td id=\"T_a6399_row11_col0\" class=\"data row11 col0\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row11_col1\" class=\"data row11 col1\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row11_col2\" class=\"data row11 col2\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row11_col3\" class=\"data row11 col3\" >0.460000</td>\n",
       "      <td id=\"T_a6399_row11_col4\" class=\"data row11 col4\" >0.830000</td>\n",
       "      <td id=\"T_a6399_row11_col5\" class=\"data row11 col5\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row11_col6\" class=\"data row11 col6\" >-0.340000</td>\n",
       "      <td id=\"T_a6399_row11_col7\" class=\"data row11 col7\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row11_col8\" class=\"data row11 col8\" >0.530000</td>\n",
       "      <td id=\"T_a6399_row11_col9\" class=\"data row11 col9\" >0.490000</td>\n",
       "      <td id=\"T_a6399_row11_col10\" class=\"data row11 col10\" >0.540000</td>\n",
       "      <td id=\"T_a6399_row11_col11\" class=\"data row11 col11\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row11_col12\" class=\"data row11 col12\" >0.790000</td>\n",
       "      <td id=\"T_a6399_row11_col13\" class=\"data row11 col13\" >0.530000</td>\n",
       "      <td id=\"T_a6399_row11_col14\" class=\"data row11 col14\" >0.520000</td>\n",
       "      <td id=\"T_a6399_row11_col15\" class=\"data row11 col15\" >0.640000</td>\n",
       "      <td id=\"T_a6399_row11_col16\" class=\"data row11 col16\" >0.620000</td>\n",
       "      <td id=\"T_a6399_row11_col17\" class=\"data row11 col17\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row11_col18\" class=\"data row11 col18\" >0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row12\" class=\"row_heading level0 row12\" >Contenuti matematica</th>\n",
       "      <td id=\"T_a6399_row12_col0\" class=\"data row12 col0\" >0.290000</td>\n",
       "      <td id=\"T_a6399_row12_col1\" class=\"data row12 col1\" >0.300000</td>\n",
       "      <td id=\"T_a6399_row12_col2\" class=\"data row12 col2\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row12_col3\" class=\"data row12 col3\" >0.380000</td>\n",
       "      <td id=\"T_a6399_row12_col4\" class=\"data row12 col4\" >0.680000</td>\n",
       "      <td id=\"T_a6399_row12_col5\" class=\"data row12 col5\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row12_col6\" class=\"data row12 col6\" >-0.290000</td>\n",
       "      <td id=\"T_a6399_row12_col7\" class=\"data row12 col7\" >0.200000</td>\n",
       "      <td id=\"T_a6399_row12_col8\" class=\"data row12 col8\" >0.450000</td>\n",
       "      <td id=\"T_a6399_row12_col9\" class=\"data row12 col9\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row12_col10\" class=\"data row12 col10\" >0.490000</td>\n",
       "      <td id=\"T_a6399_row12_col11\" class=\"data row12 col11\" >0.790000</td>\n",
       "      <td id=\"T_a6399_row12_col12\" class=\"data row12 col12\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row12_col13\" class=\"data row12 col13\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row12_col14\" class=\"data row12 col14\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row12_col15\" class=\"data row12 col15\" >0.330000</td>\n",
       "      <td id=\"T_a6399_row12_col16\" class=\"data row12 col16\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row12_col17\" class=\"data row12 col17\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row12_col18\" class=\"data row12 col18\" >0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row13\" class=\"row_heading level0 row13\" >Spazio figure</th>\n",
       "      <td id=\"T_a6399_row13_col0\" class=\"data row13 col0\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row13_col1\" class=\"data row13 col1\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row13_col2\" class=\"data row13 col2\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row13_col3\" class=\"data row13 col3\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row13_col4\" class=\"data row13 col4\" >0.800000</td>\n",
       "      <td id=\"T_a6399_row13_col5\" class=\"data row13 col5\" >0.430000</td>\n",
       "      <td id=\"T_a6399_row13_col6\" class=\"data row13 col6\" >-0.340000</td>\n",
       "      <td id=\"T_a6399_row13_col7\" class=\"data row13 col7\" >0.250000</td>\n",
       "      <td id=\"T_a6399_row13_col8\" class=\"data row13 col8\" >0.500000</td>\n",
       "      <td id=\"T_a6399_row13_col9\" class=\"data row13 col9\" >0.470000</td>\n",
       "      <td id=\"T_a6399_row13_col10\" class=\"data row13 col10\" >0.520000</td>\n",
       "      <td id=\"T_a6399_row13_col11\" class=\"data row13 col11\" >0.530000</td>\n",
       "      <td id=\"T_a6399_row13_col12\" class=\"data row13 col12\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row13_col13\" class=\"data row13 col13\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row13_col14\" class=\"data row13 col14\" >0.510000</td>\n",
       "      <td id=\"T_a6399_row13_col15\" class=\"data row13 col15\" >0.510000</td>\n",
       "      <td id=\"T_a6399_row13_col16\" class=\"data row13 col16\" >0.590000</td>\n",
       "      <td id=\"T_a6399_row13_col17\" class=\"data row13 col17\" >0.780000</td>\n",
       "      <td id=\"T_a6399_row13_col18\" class=\"data row13 col18\" >0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row14\" class=\"row_heading level0 row14\" >Rappresentazione quantitativa</th>\n",
       "      <td id=\"T_a6399_row14_col0\" class=\"data row14 col0\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row14_col1\" class=\"data row14 col1\" >0.350000</td>\n",
       "      <td id=\"T_a6399_row14_col2\" class=\"data row14 col2\" >0.390000</td>\n",
       "      <td id=\"T_a6399_row14_col3\" class=\"data row14 col3\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row14_col4\" class=\"data row14 col4\" >0.760000</td>\n",
       "      <td id=\"T_a6399_row14_col5\" class=\"data row14 col5\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row14_col6\" class=\"data row14 col6\" >-0.360000</td>\n",
       "      <td id=\"T_a6399_row14_col7\" class=\"data row14 col7\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row14_col8\" class=\"data row14 col8\" >0.870000</td>\n",
       "      <td id=\"T_a6399_row14_col9\" class=\"data row14 col9\" >0.410000</td>\n",
       "      <td id=\"T_a6399_row14_col10\" class=\"data row14 col10\" >0.630000</td>\n",
       "      <td id=\"T_a6399_row14_col11\" class=\"data row14 col11\" >0.520000</td>\n",
       "      <td id=\"T_a6399_row14_col12\" class=\"data row14 col12\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row14_col13\" class=\"data row14 col13\" >0.510000</td>\n",
       "      <td id=\"T_a6399_row14_col14\" class=\"data row14 col14\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row14_col15\" class=\"data row14 col15\" >0.390000</td>\n",
       "      <td id=\"T_a6399_row14_col16\" class=\"data row14 col16\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row14_col17\" class=\"data row14 col17\" >0.320000</td>\n",
       "      <td id=\"T_a6399_row14_col18\" class=\"data row14 col18\" >0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row15\" class=\"row_heading level0 row15\" >Utilizzo strumenti misura</th>\n",
       "      <td id=\"T_a6399_row15_col0\" class=\"data row15 col0\" >0.270000</td>\n",
       "      <td id=\"T_a6399_row15_col1\" class=\"data row15 col1\" >0.280000</td>\n",
       "      <td id=\"T_a6399_row15_col2\" class=\"data row15 col2\" >0.330000</td>\n",
       "      <td id=\"T_a6399_row15_col3\" class=\"data row15 col3\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row15_col4\" class=\"data row15 col4\" >0.620000</td>\n",
       "      <td id=\"T_a6399_row15_col5\" class=\"data row15 col5\" >0.300000</td>\n",
       "      <td id=\"T_a6399_row15_col6\" class=\"data row15 col6\" >-0.220000</td>\n",
       "      <td id=\"T_a6399_row15_col7\" class=\"data row15 col7\" >0.200000</td>\n",
       "      <td id=\"T_a6399_row15_col8\" class=\"data row15 col8\" >0.390000</td>\n",
       "      <td id=\"T_a6399_row15_col9\" class=\"data row15 col9\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row15_col10\" class=\"data row15 col10\" >0.410000</td>\n",
       "      <td id=\"T_a6399_row15_col11\" class=\"data row15 col11\" >0.640000</td>\n",
       "      <td id=\"T_a6399_row15_col12\" class=\"data row15 col12\" >0.330000</td>\n",
       "      <td id=\"T_a6399_row15_col13\" class=\"data row15 col13\" >0.510000</td>\n",
       "      <td id=\"T_a6399_row15_col14\" class=\"data row15 col14\" >0.390000</td>\n",
       "      <td id=\"T_a6399_row15_col15\" class=\"data row15 col15\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row15_col16\" class=\"data row15 col16\" >0.390000</td>\n",
       "      <td id=\"T_a6399_row15_col17\" class=\"data row15 col17\" >0.290000</td>\n",
       "      <td id=\"T_a6399_row15_col18\" class=\"data row15 col18\" >0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row16\" class=\"row_heading level0 row16\" >Risoluzione problemi</th>\n",
       "      <td id=\"T_a6399_row16_col0\" class=\"data row16 col0\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row16_col1\" class=\"data row16 col1\" >0.280000</td>\n",
       "      <td id=\"T_a6399_row16_col2\" class=\"data row16 col2\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row16_col3\" class=\"data row16 col3\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row16_col4\" class=\"data row16 col4\" >0.670000</td>\n",
       "      <td id=\"T_a6399_row16_col5\" class=\"data row16 col5\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row16_col6\" class=\"data row16 col6\" >-0.290000</td>\n",
       "      <td id=\"T_a6399_row16_col7\" class=\"data row16 col7\" >0.230000</td>\n",
       "      <td id=\"T_a6399_row16_col8\" class=\"data row16 col8\" >0.450000</td>\n",
       "      <td id=\"T_a6399_row16_col9\" class=\"data row16 col9\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row16_col10\" class=\"data row16 col10\" >0.450000</td>\n",
       "      <td id=\"T_a6399_row16_col11\" class=\"data row16 col11\" >0.620000</td>\n",
       "      <td id=\"T_a6399_row16_col12\" class=\"data row16 col12\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row16_col13\" class=\"data row16 col13\" >0.590000</td>\n",
       "      <td id=\"T_a6399_row16_col14\" class=\"data row16 col14\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row16_col15\" class=\"data row16 col15\" >0.390000</td>\n",
       "      <td id=\"T_a6399_row16_col16\" class=\"data row16 col16\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row16_col17\" class=\"data row16 col17\" >0.300000</td>\n",
       "      <td id=\"T_a6399_row16_col18\" class=\"data row16 col18\" >0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row17\" class=\"row_heading level0 row17\" >Forme nello spazio</th>\n",
       "      <td id=\"T_a6399_row17_col0\" class=\"data row17 col0\" >0.220000</td>\n",
       "      <td id=\"T_a6399_row17_col1\" class=\"data row17 col1\" >0.230000</td>\n",
       "      <td id=\"T_a6399_row17_col2\" class=\"data row17 col2\" >0.270000</td>\n",
       "      <td id=\"T_a6399_row17_col3\" class=\"data row17 col3\" >0.290000</td>\n",
       "      <td id=\"T_a6399_row17_col4\" class=\"data row17 col4\" >0.570000</td>\n",
       "      <td id=\"T_a6399_row17_col5\" class=\"data row17 col5\" >0.270000</td>\n",
       "      <td id=\"T_a6399_row17_col6\" class=\"data row17 col6\" >-0.210000</td>\n",
       "      <td id=\"T_a6399_row17_col7\" class=\"data row17 col7\" >0.170000</td>\n",
       "      <td id=\"T_a6399_row17_col8\" class=\"data row17 col8\" >0.330000</td>\n",
       "      <td id=\"T_a6399_row17_col9\" class=\"data row17 col9\" >0.270000</td>\n",
       "      <td id=\"T_a6399_row17_col10\" class=\"data row17 col10\" >0.330000</td>\n",
       "      <td id=\"T_a6399_row17_col11\" class=\"data row17 col11\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row17_col12\" class=\"data row17 col12\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row17_col13\" class=\"data row17 col13\" >0.780000</td>\n",
       "      <td id=\"T_a6399_row17_col14\" class=\"data row17 col14\" >0.320000</td>\n",
       "      <td id=\"T_a6399_row17_col15\" class=\"data row17 col15\" >0.290000</td>\n",
       "      <td id=\"T_a6399_row17_col16\" class=\"data row17 col16\" >0.300000</td>\n",
       "      <td id=\"T_a6399_row17_col17\" class=\"data row17 col17\" >1.000000</td>\n",
       "      <td id=\"T_a6399_row17_col18\" class=\"data row17 col18\" >0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a6399_level0_row18\" class=\"row_heading level0 row18\" >Algoritmi e procedure</th>\n",
       "      <td id=\"T_a6399_row18_col0\" class=\"data row18 col0\" >0.360000</td>\n",
       "      <td id=\"T_a6399_row18_col1\" class=\"data row18 col1\" >0.370000</td>\n",
       "      <td id=\"T_a6399_row18_col2\" class=\"data row18 col2\" >0.430000</td>\n",
       "      <td id=\"T_a6399_row18_col3\" class=\"data row18 col3\" >0.450000</td>\n",
       "      <td id=\"T_a6399_row18_col4\" class=\"data row18 col4\" >0.770000</td>\n",
       "      <td id=\"T_a6399_row18_col5\" class=\"data row18 col5\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row18_col6\" class=\"data row18 col6\" >-0.370000</td>\n",
       "      <td id=\"T_a6399_row18_col7\" class=\"data row18 col7\" >0.260000</td>\n",
       "      <td id=\"T_a6399_row18_col8\" class=\"data row18 col8\" >0.650000</td>\n",
       "      <td id=\"T_a6399_row18_col9\" class=\"data row18 col9\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row18_col10\" class=\"data row18 col10\" >0.550000</td>\n",
       "      <td id=\"T_a6399_row18_col11\" class=\"data row18 col11\" >0.630000</td>\n",
       "      <td id=\"T_a6399_row18_col12\" class=\"data row18 col12\" >0.440000</td>\n",
       "      <td id=\"T_a6399_row18_col13\" class=\"data row18 col13\" >0.640000</td>\n",
       "      <td id=\"T_a6399_row18_col14\" class=\"data row18 col14\" >0.540000</td>\n",
       "      <td id=\"T_a6399_row18_col15\" class=\"data row18 col15\" >0.420000</td>\n",
       "      <td id=\"T_a6399_row18_col16\" class=\"data row18 col16\" >0.480000</td>\n",
       "      <td id=\"T_a6399_row18_col17\" class=\"data row18 col17\" >0.340000</td>\n",
       "      <td id=\"T_a6399_row18_col18\" class=\"data row18 col18\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1569e088cd0>"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La nostra previsione è stata confermata."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nonostante questa nostra analisi sulla correlazione fra gli attributi presenti nel dataset, abbiamo deciso di tenere quante più colonne sfruttando la capacità della rete di maneggiare un numero elevato di feature e di determinare automaticamente la loro importanza relativa."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.7 - Analisi dei tipi e gestione delle colonne"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Lista colonne e tipi:\")\r\n",
    "\r\n",
    "table = BeautifulTable()\r\n",
    "table.columns.header = [\"\", \"Type\"]\r\n",
    "\r\n",
    "for col in dataset_ap.columns :\r\n",
    "    table.rows.append([col, dataset_ap[col].dtypes])\r\n",
    "        \r\n",
    "table.columns.alignment = BeautifulTable.ALIGN_LEFT\r\n",
    "table.set_style(BeautifulTable.STYLE_SEPARATED)\r\n",
    "print(table)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Le colonne DROPOUT e LIVELLI non sono considerate fra le feature in quanto colonne target (in particolare, DROPOUT è una trasformazione di LIVELLI).\r\n",
    "continuous_features = columns_low_ratio_null_values + \\\r\n",
    "                      [\"pu_ma_gr\", \"pu_ma_no\", \"Fattore_correzione_new\", \"Cheating\", \"WLE_MAT\", \"WLE_MAT_200\", \"WLE_MAT_200_CORR\",\r\n",
    "                       \"pu_ma_no_corr\"] + \\\r\n",
    "                      list(ambiti_processi) # Feature sui voti, feature elencate, ambiti e processi\r\n",
    "if cfg.FILL_NAN == \"remove\":\r\n",
    "    continuous_features.remove(\"voto_scritto_ita\")\r\n",
    "    continuous_features.remove(\"voto_orale_ita\")\r\n",
    "ordinal_features = [\"n_stud_prev\", \"n_classi_prev\"]\r\n",
    "int_categorical_features = [\r\n",
    "    \"CODICE_SCUOLA\", \"CODICE_PLESSO\", \"CODICE_CLASSE\", \"campione\", \"prog\",\r\n",
    "]\r\n",
    "str_categorical_features = [\r\n",
    "    \"sesso\", \"mese\", \"anno\", \"luogo\", \"eta\", \"freq_asilo_nido\", \"freq_scuola_materna\",\r\n",
    "    \"luogo_padre\", \"titolo_padre\", \"prof_padre\", \"luogo_madre\", \"titolo_madre\", \"prof_madre\",\r\n",
    "    \"regolarità\", \"cittadinanza\", \"cod_provincia_ISTAT\", \"Nome_reg\",\r\n",
    "    \"Cod_reg\", \"Areageo_3\", \"Areageo_4\", \"Areageo_5\", \"Areageo_5_Istat\"\r\n",
    "]\r\n",
    "bool_features = [\"Pon\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.8 - Gestione dei valori nulli\r\n",
    "Il dataset in considerazione presenta molti valori nulli.  \r\n",
    "Vi sono diverse tecniche per gestirli, tra cui:\r\n",
    "- sostituzione del valore nullo con un indice di sintesi (media, mediana) della colonna in considerazione: la scelta dell'indice dipende da vari fattori, tra cui la forma della distribuzione del certo attributo;\r\n",
    "- rimozione della riga corrispondente: viene adottata solitamente quando la colonna presenta pochi valori nulli e/o la riga presenta molti valori nulli;\r\n",
    "- rimozione della colonna corrispondente: viene adottata solitamente quando la colonna presenta un alto numero di valori nulli."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "dataset_ap[\"sigla_provincia_istat\"].fillna(value=\"ND\", inplace=True)\r\n",
    "\r\n",
    "if cfg.FILL_NAN == \"remove\":\r\n",
    "    # Rimuovere colonne voti ita (molti valori nulli).\r\n",
    "    # Rimuovere record con dati nulli in voti mat (meno valori nulli).\r\n",
    "    dataset_ap.drop([\"voto_scritto_ita\", \"voto_orale_ita\"], axis=1, inplace=True)\r\n",
    "    dataset_ap.dropna(subset=[\"voto_scritto_mat\", \"voto_orale_mat\"], inplace=True)\r\n",
    "else :\r\n",
    "    for col in columns_low_ratio_null_values : \r\n",
    "        if cfg.FILL_NAN == \"median\":\r\n",
    "            replaced_value = dataset_ap[col].median()\r\n",
    "        elif cfg.FILL_NAN == \"mean\":\r\n",
    "            replaced_value = dataset_ap[col].mean()\r\n",
    "\r\n",
    "        dataset_ap[col].fillna(value=replaced_value, inplace=True)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 - Machine Learning\r\n",
    "## 3.1 - Suddivisione dataset in training e test\r\n",
    "Avendo un dataset di una sola coorte di studenti relativi alle prove di un anno, non si dispone di un set per fare testing: Idealmente, l'insieme di testing dovrebbe referirsi ad una coorte diversa da quella su cui è stato effettuato il training. Nel caso di specie, tuttavia, si dispone dei dati relativi ad un'unica coorte, per cui abbiamo pensato di eseguire lo split in questo modo:\r\n",
    "\r\n",
    "1. dataset diviso in training set (default 80%) e test set (default 20%);\r\n",
    "2. training set (ottenuto dalla suddivisione al punto 1) diviso in training set (default 80%) e validation set (default 20%)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "df_training_set, df_test_set = train_test_split(dataset_ap, test_size=cfg.TEST_SET_PERCENT, random_state=19)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 - Analisi dello sbilanciamento del dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "nr_nodrop, nr_drop = np.bincount(dataset_ap['DROPOUT'])\r\n",
    "total_records = nr_drop + nr_nodrop\r\n",
    "nl = '\\n'\r\n",
    "print(\r\n",
    "    f\"Total number of records: {total_records}{nl}\\\r\n",
    "        {nl}\\\r\n",
    "Total num. DROPOUT: {nr_drop}{nl}\\\r\n",
    "Total num. NO DROPOUT: {nr_nodrop}{nl}\\\r\n",
    "    {nl}\\\r\n",
    "Ratio DROPOUT/TOTAL: {round(nr_drop / total_records, 2)}{nl}\\\r\n",
    "Ratio NO DROPOUT/TOTAL: {round(nr_nodrop / total_records, 2)}{nl}\\\r\n",
    "    {nl}\\\r\n",
    "Ratio DROPOUT/NO DROPOUT: {round(nr_drop / nr_nodrop, 2)}\"\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of records: 342226\n",
      "        \n",
      "Total num. DROPOUT: 116782\n",
      "Total num. NO DROPOUT: 225444\n",
      "    \n",
      "Ratio DROPOUT/TOTAL: 0.34\n",
      "Ratio NO DROPOUT/TOTAL: 0.66\n",
      "    \n",
      "Ratio DROPOUT/NO DROPOUT: 0.52\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le due classi (i.e. target del classificatore) appaiono leggermente sbilanciate: in dettaglio, la classe dei soggetti che manifestano dropout ha una cardinalità inferiore della classe in cui non si è avuto dropout."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.1 - Gestione dello sbilanciamento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vi sono diverse tecniche per gestire lo sbilanciamento tra le classi di un attributo, tra cui:\r\n",
    "- **ricampionamento dei dati**: \r\n",
    "    - *random over sampling*: selezionare randomicamente delle istanze della classe sotto rappresentata e duplicarle fino a quando le cardinalità delle classi si equivalgono; aumenta il rischio di overfitting;\r\n",
    "    - *random under sampling*: rimuovere istanze della classe sovrarappresentata fintanto che le cardinalità delle classi si equivalgono; causa una riduzione del training set;\r\n",
    "    - *cluster-based over-sampling*: esecuzione dell'algoritmo *k-means* sulle istanze della classe maggiormente rappresentata e su quelle della classe meno rappresentata in modo indipendente, per poi compiere oversampling sui cluster ottenuti fin tanto che le cardinalità dei cluster di una stessa classe si equivalgono come anche le cardinalità delle classi nel loro complesso;\r\n",
    "- **generazione di dati sintetici**: \r\n",
    "    - *SMOTE* (Synthetic Minority Over-sampling Technique): selezionare due o più istanze simili della classe sotto rappresentata e modificare leggermente il valore di un attributo alla volta di un ammontare inferiore alla differenza tra le istanze simili; evita l'overfitting (a patto che vi siano poche attributi) ma aumenta il rumore;\r\n",
    "- **cambiare la natura del problema**: da classificazione a *anomaly detection* o *change detection*;\r\n",
    "- **penalizzazione delle classificazioni errate sulla classe sottorappresentata**: *Cost-sensitive Training*;\r\n",
    "- **monitorare metriche diverse dall'accuratezza**, in quanto solitamente si ottengono ottimi valori di accuratezza con dati sbilanciati, perché il modello classifica tutti gli input come appartenenti alla classe più numerosa.\r\n",
    "\r\n",
    "Alla luce di queste tecniche e considerate le peculiarità del nostro dataset emerse durante la EDA, abbiamo ritenuto che la tecnica migliore al caso nostro fosse il *random under sampling*: il nostro dataset presenta un alto numero di istanze e le classi dell'attributo target presentano un sbilanciamento poco accentuato per cui conviene ricorrere ad un sottocampionamento randomico, evitando così il rischio di overfitting, piuttosto che al sovracampionamento.\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: campionamento\r\n",
    "\"\"\"\r\n",
    "if cfg.SAMPLING_TO_PERFORM == \"random_undersampling\":\r\n",
    "    # class_nodrop contiene i record della classe sovrarappresentata, ovvero SENZA DROPOUT.\r\n",
    "    class_nodrop = df_training_set[df_training_set['DROPOUT'] == False]\r\n",
    "    # class_drop contiene i record della classe sottorappresentata, ovvero CON DROPOUT.\r\n",
    "    class_drop = df_training_set[df_training_set['DROPOUT'] == True]\r\n",
    "\r\n",
    "    # Sotto campionamento di class_drop in modo che abbia stessa cardinalità di class_nodrop.\r\n",
    "    class_nodrop = class_nodrop.sample(len(class_drop), random_state=19)\r\n",
    "\r\n",
    "    print(f'Class NO DROPOUT: {len(class_nodrop):,}')\r\n",
    "    print(f'Classe DROPOUT: {len(class_drop):,}')\r\n",
    "\r\n",
    "    df_training_set = class_drop.append(class_nodrop)\r\n",
    "    df_training_set = df_training_set.sample(frac=1, random_state=19)\r\n",
    "else:\r\n",
    "    categorical_features_indexes = [i for i in range(len(df_training_set.columns)) if\r\n",
    "                                    df_training_set.columns[i] in str_categorical_features + int_categorical_features]\r\n",
    "\r\n",
    "    df_training_set = df_training_set.apply(lambda col: pd.factorize(col)[0] if col.name in str_categorical_features else col)\r\n",
    "    df_test_set = df_test_set.apply(lambda col: pd.factorize(col)[0] if col.name in str_categorical_features else col)\r\n",
    "\r\n",
    "    sm = SMOTENC(categorical_features=categorical_features_indexes, random_state=19)\r\n",
    "    X_train, y_train = sm.fit_resample(\r\n",
    "        df_training_set[[col for col in df_training_set.columns if col != 'DROPOUT']],\r\n",
    "        df_training_set['DROPOUT']\r\n",
    "    )\r\n",
    "    df_training_set = pd.concat([X_train, y_train], axis=1)\r\n",
    "\r\n",
    "    X_test, y_test = sm.fit_resample(\r\n",
    "        df_test_set[[col for col in df_test_set.columns if col != 'DROPOUT']],\r\n",
    "        df_test_set['DROPOUT']\r\n",
    "    )\r\n",
    "    df_test_set = pd.concat([X_test, y_test], axis=1)\r\n",
    "\r\n",
    "    # Se SMOTENC viene eseguito, ogni feature categorica stringa viene trasformata in feature categorica intera.\r\n",
    "    int_categorical_features = int_categorical_features + str_categorical_features\r\n",
    "    str_categorical_features = []"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Class NO DROPOUT: 93,278\n",
      "Classe DROPOUT: 93,278\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nonostante la scelta di orientarci verso il *random under sampling* abbiamo inserito il codice per poter eseguire l'*over sampling* mediante SMOTENC. Maggiori dettagli su questa tecnica sono presentati nella sezione [Risultati ottenuti](#risultati)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ecco che il training set è stato bilanciato."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "if \"Unnamed: 0\" in df_training_set.columns:\r\n",
    "    df_training_set.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 - Preprocessing per creazione del modello di Deep Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.1 - Suddivisione del dataset in training e validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "df_training_set, df_validation_set = train_test_split(df_training_set, test_size=cfg.VALIDATION_SET_PERCENT, random_state=19)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2 - Conversione dei dati da DataFrame (Pandas) a Dataset (Tensorflow/Keras)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def convert_dropout_column_to_one_hot(dropout_col):\r\n",
    "    dropout_col_one_hot = []\r\n",
    "    for dc in dropout_col:\r\n",
    "        if dc == 1:\r\n",
    "            dropout_col_one_hot.append([1, 0])\r\n",
    "        else:\r\n",
    "            dropout_col_one_hot.append([0, 1])\r\n",
    "    return dropout_col_one_hot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "def pd_dataframe_to_tf_dataset(dataframe: pd.DataFrame):\r\n",
    "    copied_df = dataframe.copy()\r\n",
    "    if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "        dropout_col = copied_df.pop(\"DROPOUT\")\r\n",
    "        dropout_col = convert_dropout_column_to_one_hot(dropout_col)\r\n",
    "        copied_df.drop(\"LIVELLI\", axis=1, inplace=True)\r\n",
    "    elif cfg.PROBLEM_TYPE == \"regression\":\r\n",
    "        # La colonna target LIVELLI viene presa, invertiti i valori (0 -> 5, 1 -> 4, ..., 5 -> 0) e poi li divido per 5.\r\n",
    "        # Questo viene fatto per poter associare a valori sopra 0.6 (corrispondente all'originale 4) il concetto di\r\n",
    "        # \"Dropout Sì\" e a quelli inferiori il concetto di \"Dropout no\".\r\n",
    "        dropout_col = copied_df.pop(\"LIVELLI\")\r\n",
    "        dropout_col = dropout_col.subtract(5)\r\n",
    "        dropout_col = dropout_col.abs()\r\n",
    "        dropout_col = dropout_col.divide(5) # Normalizzazione dei valori della colonna da [0..5] a [0..1].\r\n",
    "        copied_df.drop(\"DROPOUT\", axis=1, inplace=True)\r\n",
    "    else: # cfg.PROBLEM_TYPE == \"pure_regression\"\r\n",
    "        dropout_col = copied_df.pop(\"LIVELLI\")\r\n",
    "        dropout_col = dropout_col.divide(5) # Normalizzazione dei valori della colonna da [0..5] a [0..1].\r\n",
    "        copied_df.drop(\"DROPOUT\", axis=1, inplace=True)\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    Dato che il dataframe ha dati eterogenei lo convertiamo a dizionario (i.e. dict(copied_df)),\r\n",
    "    in cui le chiavi sono i nomi delle colonne e i valori sono i valori della colonna.\r\n",
    "    Infine bisogna indicare la colonna target.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((dict(copied_df), dropout_col))\r\n",
    "    tf_dataset = tf_dataset.shuffle(buffer_size=len(copied_df), seed=19)\r\n",
    "    return tf_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "ds_training_set = pd_dataframe_to_tf_dataset(df_training_set)\r\n",
    "ds_validation_set = pd_dataframe_to_tf_dataset(df_validation_set)\r\n",
    "ds_test_set = pd_dataframe_to_tf_dataset(df_test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.3 - Suddivisione del dataset in batch\r\n",
    "Abbiamo proceduto col suddividere i vari set di dati in batch al fine di realizzare la variante del metodo di ottimizzazione del **gradient descent**, nota come **mini-batch gradient descent**: dato un insieme di mini-batch, si alimentare il modello con uno di essi, vi si calcola il gradiente e quindi si procedere all'aggiornamento dei pesi, per poi ripetere tale processo col mini-batch successivo; così facendo, si riduce la complessità computazionale che sarebbe richiesta dalla versione classica del *gradient descent* che considera l'intero training set ad ogni iterazione."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "ds_training_set = ds_training_set.batch(cfg.BATCH_SIZE, drop_remainder=True)\r\n",
    "ds_validation_set = ds_validation_set.batch(cfg.BATCH_SIZE, drop_remainder=True)\r\n",
    "ds_test_set = ds_test_set.batch(cfg.BATCH_SIZE, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.4 - Creazione dei layer di input per ogni feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "input_layers = {}\r\n",
    "for name, column in df_training_set.items():\r\n",
    "    if name in [\"DROPOUT\", \"LIVELLI\"]:\r\n",
    "        continue\r\n",
    "\r\n",
    "    if name in continuous_features:\r\n",
    "        dtype = tf.float32\r\n",
    "    elif name in ordinal_features or name in int_categorical_features or name in bool_features:\r\n",
    "        dtype = tf.int64\r\n",
    "    else:  # name in str_categorical_features\r\n",
    "        dtype = tf.string\r\n",
    "\r\n",
    "    input_layers[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.5 - Encoding delle feature in base al loro tipo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "preprocessed_features = []\r\n",
    "\r\n",
    "def stack_dict(inputs, fun=tf.stack):\r\n",
    "    values = []\r\n",
    "    for key in sorted(inputs.keys()):\r\n",
    "        values.append(tf.cast(inputs[key], tf.float32))\r\n",
    "\r\n",
    "    return fun(values, axis=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# Preprocessing colonne con dati booleani\r\n",
    "for name in bool_features:\r\n",
    "    inp = input_layers[name]\r\n",
    "    inp = inp[:, tf.newaxis]\r\n",
    "    float_value = tf.cast(inp, tf.float32)\r\n",
    "    preprocessed_features.append(float_value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# Preprocessing colonne con dati interi ordinali\r\n",
    "ordinal_inputs = {}\r\n",
    "for name in ordinal_features:\r\n",
    "    ordinal_inputs[name] = input_layers[name]\r\n",
    "\r\n",
    "normalizer = Normalization(axis=-1)\r\n",
    "normalizer.adapt(stack_dict(dict(df_training_set[ordinal_features])))\r\n",
    "ordinal_inputs = stack_dict(ordinal_inputs)\r\n",
    "ordinal_normalized = normalizer(ordinal_inputs)\r\n",
    "preprocessed_features.append(ordinal_normalized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# Preprocessing colonne con dati continui\r\n",
    "continuous_inputs = {}\r\n",
    "for name in continuous_features:\r\n",
    "    continuous_inputs[name] = input_layers[name]\r\n",
    "\r\n",
    "normalizer = Normalization(axis=-1)\r\n",
    "normalizer.adapt(stack_dict(dict(df_training_set[continuous_features])))\r\n",
    "continuous_inputs = stack_dict(continuous_inputs)\r\n",
    "continuous_normalized = normalizer(continuous_inputs)\r\n",
    "preprocessed_features.append(continuous_normalized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per i dati categorici di tipo stringa o intero abbiamo adottato la *one-hot encoding*; tale codifica consiste nel mappare ogni categoria ad un array contenente $n$-1 elementi settati a 0 e un elemento settato a 1, con $n$ pari al numero delle categorie: l'indice dell'elemento settato ad 1 permette di distinguere le diverse categorie."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# Preprocessing colonne con dati categorici stringa\r\n",
    "for name in str_categorical_features:\r\n",
    "    vocab = sorted(set(df_training_set[name]))\r\n",
    "\r\n",
    "    lookup = StringLookup(vocabulary=vocab, output_mode='one_hot')\r\n",
    "\r\n",
    "    x = input_layers[name][:, tf.newaxis]\r\n",
    "    x = lookup(x)\r\n",
    "\r\n",
    "    preprocessed_features.append(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Preprocessing colonne con dati categorici interi\r\n",
    "for name in int_categorical_features:\r\n",
    "    vocab = sorted(set(df_training_set[name]))\r\n",
    "\r\n",
    "    lookup = IntegerLookup(vocabulary=vocab, output_mode='one_hot')\r\n",
    "\r\n",
    "    x = input_layers[name][:, tf.newaxis]\r\n",
    "    x = lookup(x)\r\n",
    "\r\n",
    "    preprocessed_features.append(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 - Assemblaggio dei vari layer e creazione del modello\r\n",
    "\r\n",
    "### 3.4.1 - Funzioni di attivazione\r\n",
    "Abbiamo studiato e approfondito le principali funzioni di attivazione, passandone in rassegna le peculiarità, i pro e i contro. Alla luce di tali nuove conoscenze, abbiamo optato per le seguenti scelte:\r\n",
    "- *Leaky ReLU* per gli hidden layer: \r\n",
    "    - abbiamo preferito la variante *Leaky ReLU* piuttosto che la classica ReLU, in quanto la prima, prevedendo una leggera pendenza a sinistra dell'origine, evita il *Dying ReLU Problem*, di cui soffre invece la ReLU;\r\n",
    "    - il codominio ha cardinalità infinita (dato che per input > 0, diventa la funzione di identità), per cui non vi è una significativa perdita di informazione nel passaggio da un layer al successivo;\r\n",
    "    - a fronte di somme pesate negative in input, restituisce valori negativi molto bassi che portano a calcoli non pesanti computazionalmente, registrando così tempi minori di addestramento per il modello nel complesso.  \r\n",
    "    <img src=\"./src/img/leaky_relu.png\" width=\"50%\"/>\r\n",
    "- *Sigmoid* o *Softmax* per l'output layer: sono le funzione che vengono [consigliate](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/) come *best practice* per il livello di output rispettivamente in problemi di regressione e classificazione.\r\n",
    "    - la *Sigmoid* è la funzione che simula più fedelmente l'attivazione dei neuroni nel cervello: mappa valori negativi a valori tanto più prossimi allo 0 quanto maggiore è il loro valore assoluto e valori positivi a valori tanto più prossimi ad 1 quanto maggiore è il loro valore;\\\r\n",
    "    <img src=\"./src/img/sigmoid.png\" width=\"50%\"/>\r\n",
    "    - *Softmax* (anche nota come *Funzione esponenziale normalizzata*) comprime un vettore $k$-dimensionale di valori reali arbitrari in un vettore $k$-dimensionale di valori compresi in un intervallo [0,1] la cui somma è 1.  \r\n",
    "    <img src=\"./src/img/softmax.png\" width=\"50%\"/>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2 - Inizializzazione dei pesi\r\n",
    "L'inizializzazione dei pesi è un aspetto molto importante nella progettazione di una rete neurale: essi incidono in larga misura sulla somma pesata che si accumula in ogni neurone e che viene passata in input alla sua funzione di attivazione, determinando anche la misura in cui l'informazione passa da un layer al successivo.\r\n",
    "\r\n",
    "I modelli di reti neurali vengono addestrati mediante l'algoritmo di ottimizzazione noto come **discesa del gradiente**, che modifica iterativamente i pesi della rete con l'obiettivo di minimizzare una funzione di costo: alla fine dell'addestramento, si giunge a quella combinazione di pesi che permettono alla rete di avere una certa performance nelle previsioni.\r\n",
    "L'algoritmo di ottimizzazione richiede un punto di partenza nello spazio dei possibili valori dei pesi e spesso la sua velocità di convergenza o effettiva convergenza è fortemente determinata dall'inizializzazione iniziale dei pesi.\r\n",
    "\r\n",
    "In letteratura, si suggeriscono diverse funzioni di inizializzazione a seconda della funzione di attivazione del layer in oggetto:\r\n",
    "- con ReLU o Leaky ReLU, è consigliato il metodo di inizializzazione **He** che calcola i pesi a partire dalla distribuzione di probabilità gaussiana con media 0 e deviazione standard pari a $\\sqrt{\\frac{2}{n}}$, con $n$ il numero di input che arrivano al neurone;\r\n",
    "    - conseguentemente, l'altezza della gaussiana dipende dal numero di input che il neurone riceve: all'aumentare del numero di questi input, l'altezza aumenta e la gaussiana si restringe, diminuendo la probabilità di avere pesi alti. Così i neuroni tendono meno a saturare, permettendo alla rete di raggiungere il massimo dell'accuratezza in meno tempo. Infatti, tale accorgimento non migliora l'accuratezza, ma solo il tempo che la rete impiega a raggiungere quella massima;\r\n",
    "    \r\n",
    "<img src=\"./src/img/He.png\" width=\"50%\"/>\r\n",
    "\r\n",
    "- con Sigmoid o Tanh, inizializzazione **Glorot** (anche nota *Xavier*, dal nome di *Xavier Glorot*, un ricercatore presso Google DeepMind): questo metodo, per ogni neurone, calcola i diversi pesi da una distribuzione di probabilità uniforme che ha come dominio l'intervallo $[-\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}]$, con $n$ il numero di input che arrivano al neurone considerato;\r\n",
    "    - l'immagine seguente mostra il restringersi del range in cui cadono tali pesi (nell'ordinata) all'aumentare del numero di input entranti nel neurone (nell'ascissa);\r\n",
    "    \r\n",
    "<img src=\"./src/img/Xavier.png\" width=\"50%\"/>\r\n",
    "\r\n",
    "- esiste anche una variante di **Glorot**, chiamata **Normalized Xavier Weight Initialization**: essa estrae i valori da una distribuzione di probabilità uniforme, tuttavia il suo dominio è l'intervallo $[-\\frac{\\sqrt{6}}{\\sqrt{n + m}}, \\frac{\\sqrt{6}}{\\sqrt{n + m}}]$, con $n$ il numero di input al nodo in oggetto (in ascissa), $m$ il numero di output del layer di cui fa parte quel neurone (ossia il numero di neuroni di quel layer).\r\n",
    "    \r\n",
    "\r\n",
    "<img src=\"./src/img/Normalized_Xavier.png\" width=\"50%\"/>\r\n",
    "\r\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "preprocessed = tf.concat(preprocessed_features, axis=-1) # Tensore\r\n",
    "\r\n",
    "preprocessor = tf.keras.Model(input_layers, preprocessed) # Tanti input layer quante le feature\r\n",
    "\r\n",
    "# inizializzatore che verrà usato per i pesi dei layer con ReLU / LeakyReLU\r\n",
    "initializer_hidden_layer = tf.keras.initializers.HeNormal(seed=19)\r\n",
    "# inizializzatore che verrà usato per i pesi dei layer con sigmoid\r\n",
    "initializer_output_layer = tf.keras.initializers.GlorotNormal(seed=19)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.3 - Regolarizzazione per evitare overfitting\r\n",
    "Si ha **overfitting** ogniqualvolta il modello apprende i dati troppo bene, ossia ne incamera il rumore statistico: conseguentemente, il modello presenta un *alto errore di generalizzazione* (scarse performance quando il modello è valutato su dati nuovi, come quelli dell'insieme di test).\r\n",
    "Il rischio di overfitting si fa più concreto quando la cardinalità del training set è bassa e/o la rete neurale è molto grande.\r\n",
    "\r\n",
    "Per ridurre al minimo il rischio di overfitting, vi sono molti metodi di **regolarizzazione**, tra cui il **dropout**: esso approssima l'addestramento in parallelo di un gran numero di reti neurali con architetture differenti; consiste nell'assegnare ad ogni neurone di un certo layer una probabilità $1-p$ che sia ignorato e $p$ che non sia ignorato, per cui si concretizza in un sottocampionamento degli output dei vari layer.\r\n",
    "La probabilità $p$ viene settato a livello del singolo layer; in letteratura, sono suggeriti valori prossimi a 0.5 per i layer nascosti e prossimi a 1 per i layer visibili, mentre di non settare alcun dropout per il layer di output. Noi abbiamo rispettato tali suggerimenti, come è possibile constatare nel codice che segue.\r\n",
    "\r\n",
    "Un effetto collaterale del dropout è la riduzione della capacità rappresentativa della rete, per cui è conveniente solo a fronte di reti neurali grandi: nel caso la rete abbia dimensioni modeste, occorre aumentarne prima l'ampiezza e la profondità e poi settare il dropout ai vari layer. Questo motivo è alla base della nostra scelta di raddoppiare i valori di default degli iperparametri relativi al numero di layer e al numero di neuroni nelle configurazioni in cui è presente il dropout. \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.4 - Batch Normalization\r\n",
    "Tra le innumerevoli architetture esperite, ve ne sono state alcune con un alto numero di hidden layer, nell'ordine delle decine; avendo riscontrato significativi rallentamenti nell'addestramento, abbiamo effettuato diverse ricerche in seguito alle quali abbiamo scoperto la **batch normalization**: trattasi di una tecnica che standardizza gli input al layer cui è applicata per ogni mini-batch di addestramento. In questo modo, i valori in input sono minori e centrati, quindi manipolabili in maniera più agevole: conseguentemente il processo di apprendimento risulta essere più stabile e veloce, impiegando un numero di epoche inferiore.  \r\n",
    "Gli studi a tal proposito in letteratura suggeriscono di sperimentare l'applicazione di tale processo di standardizzazione in tre punti differenti:\r\n",
    "- tra l'hidden layer e la sua funzione di attivazione;\r\n",
    "- dopo la funzione di attivazione di un hidden layer;\r\n",
    "- subito prima del layer di output. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "body = tf.keras.Sequential()\r\n",
    "\r\n",
    "if cfg.DROPOUT_LAYER:\r\n",
    "    body.add(tf.keras.layers.Dropout(rate=cfg.DROPOUT_INPUT_LAYER_RATE, seed=19))  # aggiunta dropout a layer di input\r\n",
    "\r\n",
    "# segue l'aggiunta degli hidden layers\r\n",
    "for _ in range(cfg.NUMBER_OF_LAYERS):\r\n",
    "    body.add(tf.keras.layers.Dense(cfg.NEURONS, kernel_initializer=initializer_hidden_layer))\r\n",
    "\r\n",
    "    if cfg.BATCH_NORMALIZATION == \"dense_batch_activation\":\r\n",
    "        body.add(tf.keras.layers.BatchNormalization())\r\n",
    "\r\n",
    "    if cfg.ACTIVATION_LAYER == \"leaky_relu\":\r\n",
    "        body.add(tf.keras.layers.LeakyReLU())\r\n",
    "    else:\r\n",
    "        body.add(tf.keras.layers.ReLU())\r\n",
    "\r\n",
    "    if cfg.DROPOUT_LAYER:\r\n",
    "        body.add(tf.keras.layers.Dropout(rate=cfg.DROPOUT_HIDDEN_LAYER_RATE, seed=19))\r\n",
    "\r\n",
    "    if cfg.BATCH_NORMALIZATION == \"dense_activation_batch\":\r\n",
    "        body.add(tf.keras.layers.BatchNormalization())\r\n",
    "\r\n",
    "if cfg.BATCH_NORMALIZATION == \"before_output\":\r\n",
    "    body.add(tf.keras.layers.BatchNormalization())\r\n",
    "\r\n",
    "# segue l'aggiunta dell'output layer\r\n",
    "if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "    body.add(tf.keras.layers.Dense(2, activation=\"softmax\", kernel_initializer=initializer_output_layer))\r\n",
    "else: # cfg.PROBLEM_TYPE == \"regression\" or cfg.PROBLEM_TYPE == \"pure_regression\"\r\n",
    "    body.add(tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer_output_layer))\r\n",
    "\r\n",
    "x = preprocessor(input_layers)\r\n",
    "\r\n",
    "result = body(x)\r\n",
    "\r\n",
    "model = tf.keras.Model(input_layers, result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualizzazione tabellare del modello\r\n",
    "from keras.utils.vis_utils import plot_model\r\n",
    "model.summary()\r\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.5 - Funzione di costo e metriche di performance\r\n",
    "Riguardo alla funzione di costo, abbiamo scelto per il nostro modello la **Cross-entropy**: come riportato dal libro \"Pattern Recognition and Machine Learning\" (2006), utilizzare la cross-entropy piuttosto che la somma dei quadrati come funzione di costo nei problemi di classificazione porta ad un apprendimento più veloce e ad una generalizzazione maggiore.  \r\n",
    "Trattasi di una misura derivante dal campo della *Teoria dell'informazione*: nella sua accezione più generale, si concretizza nel calcolare la differenza (in termini più precisi, l'entropia totale) tra due distribuzioni di probabilità, dato una variabile aleatoria o un insieme di eventi. Nell'ambito del *Machine Learning*, tali due distribuzioni sono:\r\n",
    "- la distribuzione della variabile target e\r\n",
    "- l'approssimazione della distribuzione della variabile target restituita dal predittore.\r\n",
    "\r\n",
    "Pertanto, la cross-entropy è un valore positivo indicante l'entropia totale necessaria affinchè i valori della variabile target siano rappresentati/trasmessi mediante la distribuzione approssimata delle predizioni.\r\n",
    "\r\n",
    "[<img src=\"./src/img/cross-entropy.png\" width=\"50%\"/>](https://i.stack.imgur.com/gNip2.png)\r\n",
    "\r\n",
    "Più le due distribuzioni divergono, maggiore è la cross-entropy: ciò è dovuto all'informazione ( = $-\\log_2{P(x)}$) maggiore a fronte di predizioni molto distanti da 1.\r\n",
    "\r\n",
    "[<img src=\"./src/img/information.png\" width=\"50%\"/>](https://i.stack.imgur.com/gNip2.png)\r\n",
    "\r\n",
    "A livello implementativo, nel caso il problema in esame sia visto come un problema di:\r\n",
    "    \r\n",
    "- **regressione**, l'output compreso nell'intervallo $[0, 1]$ viene valutato con la funzione di costo `BinaryCrossentropy()` poiché ci interessa sapere solamente quando l'output è inferiore del threshold fissato (0.4) oppure maggiore;\r\n",
    "- **classificazione**, l'output è un vettore con tante componenti quante le classi di output (in questo caso due, \"dropout sì\" e \"dropout no\"), in cui ogni componente è una misura di probabilità di far parte di quella classe, il tutto valutato con la funzione di costo `CategoricalCrossEntropy()`.\r\n",
    "\r\n",
    "In seconda analisi, abbiamo pensato di utilizzare come funzione di costo l'**Errore quadratico medio** o **Mean Squared Error** (MSE) che è una misura della discrepanza quadratica media fra i valori della colonna target e le predizioni restituite dal predittore.\r\n",
    "\r\n",
    "$MSE = (\\frac{1}{n})\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "    main_metric = tf.keras.metrics.Accuracy(name=\"acc\")\r\n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy()\r\n",
    "elif cfg.PROBLEM_TYPE == \"regression\":\r\n",
    "    # 0.6 perché dopo il preprocessing, i LIVELLI in [3,4,5] è DROPOUT = True, LIVELLI in [0,1,2] è DROPOUT = False\r\n",
    "    main_metric = tf.keras.metrics.BinaryAccuracy(name=\"bin_acc\", threshold=0.6)  \r\n",
    "    loss_function = tf.keras.losses.BinaryCrossentropy()\r\n",
    "else: # cfg.PROBLEM_TYPE == \"pure_regression\"\r\n",
    "    main_metric = tf.keras.metrics.MeanAbsoluteError(name=\"mae\")\r\n",
    "    loss_function = tf.keras.losses.MeanSquaredError()\r\n",
    "\r\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=cfg.LEARNING_RATE),\r\n",
    "              loss=loss_function,\r\n",
    "              metrics=[\r\n",
    "                main_metric,\r\n",
    "                tf.keras.metrics.FalsePositives(name=\"fp\"),\r\n",
    "                tf.keras.metrics.FalseNegatives(name=\"fn\"),\r\n",
    "                tf.keras.metrics.TruePositives(name=\"tp\"),\r\n",
    "                tf.keras.metrics.TrueNegatives(name=\"tn\"),\r\n",
    "                tf.keras.metrics.Precision(name=\"prec\"),\r\n",
    "                tf.keras.metrics.Recall(name=\"rec\")\r\n",
    "              ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.6 - Early stopping\r\n",
    "Mano a mano che procedavameno nello sperimentare differenti architetture, abbiamo riscontrato qualche dubbio circa il numero giusto di epoche per l'apprendimento.  \r\n",
    "In effetti, ricerche a tal proposito, hanno evidenziato che trattasi di un iperparametro il cui settaggio è di fondamentale e delicata importanza; in dettaglio, settare:\r\n",
    "- un numero troppo elevato di epoche, può portare al rischio dell'overfitting del modello sul training set mentre,\r\n",
    "- un numero troppo basso può essere la causa di un apprendimento troppo grossolano degli schemi celati dietro ai dati.\r\n",
    "\r\n",
    "Uno dei metodi suggeriti in letteratura è il c.d. **Early stopping**: esso permette di specificare un numero di epoche arbitrariamente alto, in quanto gestisce l'effettiva fine dell'addestramento sulla base delle variazioni registrate da una certa metrica specificata. \r\n",
    "Nel caso del nostro progetto abbiamo specificato come metrica da monitorare il valore restituito dalla funzione di costo sul validation set.  \r\n",
    "Col parametro `mode` specifichiamo la direzione della variazione della metrica da considerarsi come miglioramento.\r\n",
    "Per evitare che l'addestramento venga stoppato non appena il valore della funzione di costo non migliora (nel caso di specie, migliora sta per diminuisce), abbiamo settato un ulteriore argomento, `patience` che indica il numero di epoche che occorre attendere prima di fermare l'addestramento in caso non ci siano miglioramenti; così facendo, possiamo contemplare la situazione in cui la metrica monitorata sperimenti un plateau o addirittura un peggioramento (nel caso di specie, sta per aumento) per qualche epoca, prima di tornare a migliorare.  \r\n",
    "Ulteriormente abbiamo settato a `True` il parametro `restore_best_weight`, allo scopo di ripristinare i valori dell'ultima epoca in cui la metrica monitorata è migliorata: questo permette di non evitare che la rete rimanga coi pesi dell'ultima epoca fin cui si è addestrata, grazie al parametro `patience`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "\"\"\"\r\n",
    "Definizione dello stopper per evitare che la reti continui ad addestrarsi quando non ci sono miglioramenti della loss \r\n",
    "(val_loss = funzione di costo sul validation set) per piu' di 5 epoche\r\n",
    "\"\"\"\r\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\r\n",
    "                              patience=5,\r\n",
    "                              mode=\"min\",\r\n",
    "                              restore_best_weights=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.7 - Salvataggio del miglior modello\r\n",
    "Abbiamo provveduto alla definizione di una callback per salvare il modello migliore: il salvataggio avviene in un file con estenzione `.h5` per mezzo del modulo `h5py`.  \r\n",
    "Così facendo, in via successiva, è possibile caricare il modello in oggetto col codice seguente, fornire degli input e ottenere gli output corrispondenti.\r\n",
    "\r\n",
    "```python\r\n",
    "from keras.models import load_model\r\n",
    "saved_model = load_model('best_model.h5')\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "\"\"\"\r\n",
    "Definizione della callback che permette durante il training di salvare il miglior modello calcolato.\r\n",
    "\"\"\"\r\n",
    "model_checkpoint = ModelCheckpoint(\"best_\" + cfg.PROBLEM_TYPE + \".h5\", monitor='val_loss', mode='min', save_best_only=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.8 - Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"[Training]\")\r\n",
    "history = model.fit(ds_training_set,\r\n",
    "          epochs=cfg.EPOCH,\r\n",
    "          batch_size=cfg.BATCH_SIZE,\r\n",
    "          validation_data=ds_validation_set,\r\n",
    "          callbacks=([early_stopper] if cfg.EARLY_STOPPING else []) + [PlotLossesKeras(), model_checkpoint],\r\n",
    "          verbose=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.9 - Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"[Test]\")\r\n",
    "score = model.evaluate(ds_test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Results with test dataset')\r\n",
    "print('Loss:', round(score[0], 4))\r\n",
    "print('Accuracy:', round(score[1], 4))\r\n",
    "print('False positives:', int(score[2]))\r\n",
    "print('False negatives:', int(score[3]))\r\n",
    "print('True positives:', int(score[4]))\r\n",
    "print('True negatives:', int(score[5]))\r\n",
    "print('Precision: ', round(score[6], 4))\r\n",
    "print('Recall: ', round(score[7], 4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"risultati\"></a>\r\n",
    "# 4 - Risultati ottenuti dall'esecuzione del modello"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abbiamo cercato di trovare la configurazione migliore per risolvere questo tipo di problema avviando numerosi job grazie al Cluster HPC del DISI, di cui viene trattato più in dettaglio in [Esecuzione su Cluster HPC](#cluster).\r\n",
    "\r\n",
    "Tutte le configurazioni di iperparametri sono state testate vedendo il problema come **regressione**, cercando di predirre o che gli studenti facciano dropout (oppure no) o che ottengano un certo livello in uscita (i.e. colonna LIVELLI) e come **classificazione**, cercando di predirre la probabilità che gli studenti facciano e non facciano dropout.\r\n",
    "\r\n",
    "Abbiamo svolto più iterazioni:\r\n",
    "- la prima è servita a capire quali iperparametri fornivano i risultati più convincenti da usare nella successiva iterazione;\r\n",
    "- la seconda ha permesso di testare se configurazioni più avanzate con gli iperparametri più importanti scoperti nell'iterazione precedente fornissero risultati migliori;\r\n",
    "- la terza ci ha permesso di confermare la bontà degli iperparametri trovati alla seconda iterazione."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 - Prima iterazione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La configurazione di default (sia per il problema trattato come classificazione che regressione) è la seguente:\r\n",
    "\r\n",
    "Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| 50 | 128 | 32 | 10, Leaky ReLU | 0.001 | No | mediana | No |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nelle tabella che seguono vengono indicati i valori di accuratezza riscontrati e delle note (quando necessario).\r\n",
    "\r\n",
    "**N.B.** Solo le celle che differiscono dalla configurazione di default vengono compilate."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.1 - Classificazione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "\r\n",
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| base_model | - | - | - | - | - | - | - | - | 0.3451 | 0.2581 | 0.2809 | - |\r\n",
    "| batch128 | - | - | 128 | - | - | - | - | - | 0.2990 | 0.1936 | 0.2076 | - |\r\n",
    "| neurons256 | - | 256 | - | - | - | - | - | - | 0.6569 | 0.3675 | 0.3995 | - |\r\n",
    "| batch128neurons256 | - | 256 | 128 | - | - | - | - | - | 0.4621 | 0.3638 | 0.3744 | - |\r\n",
    "| dense5 | - | - | - | 5, Leaky ReLU | - | - | - | - | 0.3202 | 0.1477 | 0.1645 | - |\r\n",
    "| dense15 | - | - | - | 15, Leaky ReLU | - | - | - | - | 0.5303 | 0.3234 | 0.3038 | - |\r\n",
    "| dense15 | - | - | - | 15, Leaky ReLU | - | - | - | - | 0.5303 | 0.3234 | 0.3038 | - |\r\n",
    "| dropout | - | 256 | - | - | - | Sì | - | - | 0.0024 | 0.0000 | 0.0000 | - |\r\n",
    "| epoch100 | 100 | - | - | - | - | - | - | - | 0.5000 | 0.2945 | 0.3135 | - |\r\n",
    "| fillmean | - | - | - | - | - | - | media | - | 0.3538 | 0.2022 | 0.2070 | - |\r\n",
    "| fillremove | - | - | - | - | - | - | rimozione | - | 0.3538 | 0.2022 | 0.2070 | Le colonne con alto tasso di valori nulli vengono rimosse, mentre quelle con un basso tasso vengono rimossi i record con quella feature a null. |\r\n",
    "| lr001 | - | - | - | - | - | - | - | - | 0.7400 | 0.7131 | 0.6720 | - |\r\n",
    "| relu | - | - | - | 10, ReLU | - | - | - | - | 0.4603 | 0.1242 | 0.1639 | - |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.2 - Regressione (dropout)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| base_model | - | - | - | - | - | - | - | - | 0.1975 | 0.1872 | 0.2428 | - |\r\n",
    "| batch128 | - | - | 128 | - | - | - | - | - | 0.1976 | 0.1901 | 0.2472 | - |\r\n",
    "| neurons256 | - | 256 | - | - | - | - | - | - | 0.1973 | 0.1873 | 0.2425 | - |\r\n",
    "| batch128neurons256 | - | 256 | 128 | - | - | - | - | - | 0.1976 | 0.1901 | 0.2463 | - |\r\n",
    "| dense5 | - | - | - | 5, Leaky ReLU | - | - | - | - | 0.1975 | 0.1857 | 0.2401 | - |\r\n",
    "| dense15 | - | - | - | 15, Leaky ReLU | - | - | - | - | 0.1975 | 0.1906 | 0.2469 | - |\r\n",
    "| dropout | - | 256 | - | - | - | Sì | - | - | 0.1906 | 0.1964 | 0.2563 | - |\r\n",
    "| epoch100 | 100 | - | - | - | - | - | - | - | 0.1975 | 0.1839 | 0.2374 | - |\r\n",
    "| fillmean | - | - | - | - | - | - | media | - | 0.1975 | 0.1871 | 0.2435 | - |\r\n",
    "| fillremove | - | - | - | - | - | - | rimozione | - | 0.1869 | 0.1751 | 0.2140 | Le colonne con alto tasso di valori nulli vengono rimosse, mentre quelle con un basso tasso vengono rimossi i record con quella feature a null. |\r\n",
    "| lr001 | - | - | - | - | - | - | - | - | 0.1497 | 0.0400 | 0.0505 | - |\r\n",
    "| relu | - | - | - | 10, ReLU | - | - | - | - | 0.1975 | 0.1893 | 0.2453 | - |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.3 - Regressione (livelli)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | MAE in training | MAE in validation | MAE in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| base_model | - | - | - | - | - | - | - | - | 0.0160 | 0.1818 | 0.1819 | - |\r\n",
    "| batch128 | - | - | 128 | - | - | - | - | - | 0.0116 | 0.1754 | 0.1769 | - |\r\n",
    "| neurons256 | - | 256 | - | - | - | - | - | - | 0.4396 | 0.4385 | 0.3604 | - |\r\n",
    "| batch128neurons256 | - | 256 | 128 | - | - | - | - | - | 0.4651 | 0.4630 | 0.4787 | - |\r\n",
    "| dense5 | - | - | - | 5, Leaky ReLU | - | - | - | - | 0.0176 | 0.1785 | 0.1824 | - |\r\n",
    "| dense15 | - | - | - | 15, Leaky ReLU | - | - | - | - | 0.6443 | 0.6179 | 0.6035 | - |\r\n",
    "| dropout | - | 256 | - | - | - | Sì | - | - | 0.3995 | 0.3680 | 0.3596 | - |\r\n",
    "| epoch100 | 100 | - | - | - | - | - | - | - | 0.0154 | 0.1801 | 0.1812 | - |\r\n",
    "| fillmean | - | - | - | - | - | - | media | - | 0.0146 | 0.1808 | 0.1841 | - |\r\n",
    "| fillremove | - | - | - | - | - | - | rimozione | - | 0.0171 | 0.1792 | 0.1807 | Le colonne con alto tasso di valori nulli vengono rimosse, mentre quelle con un basso tasso vengono rimossi i record con quella feature a null. |\r\n",
    "| lr001 | - | - | - | - | - | - | - | - | 0.4413 | 0.4398 | 0.3614 | - |\r\n",
    "| relu | - | - | - | 10, ReLU | - | - | - | - | 0.0111 | 0.1791 | 0.1824 | - |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 - Seconda iterazione\r\n",
    "\r\n",
    "Analizzando le tabelle della prima iterazione osserviamo che gli iperparametri più importanti sono il numero dei neuroni, il numero di hidden layer e il learning rate.\r\n",
    "\r\n",
    "Per questa ragione, abbiamo ideato delle configurazioni con decisamente più neuroni e learning rate più elevati, tenendo però più basso il numero di hidden layer, ottenendo i risultati che seguono. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2.1 - Classificazione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| 512N_001LR | - | 512 | - | - | 0.01 | - | - | - | 0.7292 | 0.6546 | 0.7322 | - |\r\n",
    "| 512N_7L_01LR_DrTrue | - | 512 | - | 7, Leaky ReLU | 0.1 | Sì | - | - | 0.5803 | 0.5965 | 0.4735 | - |\r\n",
    "| 512N_7L_001LR | - | 512 | - | - | 7, Leaky ReLU | 0.01 | - | - | 0.7561 | 0.6419 | 0.5438 | - |\r\n",
    "| 512N_7L_001LR_DrTrue | - | - | - | 7, Leaky ReLU | 0.01 | Sì | - | - | 0.5782 | 0.5473 | 0.4065 | - |\r\n",
    "| before_output | - | - | - | - | - | - | - | Prima del layer di output | $1.8430 \\cdot 10^-4$ | $6.7003 \\cdot 10^-5$ | $7.3082 \\cdot 10^-5$ | - |\r\n",
    "| dense_activation_batch | - | - | - | - | - | - | - | ... | 0.1437 | 0.0710 | 0.0659 | - |\r\n",
    "| dense_batch_activation | - | - | - | - | - | - | - | ... | 0.1307 | 0.0809 | 0.0900 | - |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2.2 - Regressione (dropout)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| 512N_001LR | - | 512 | - | - | 0.01 | - | - | - | 0.1550 | 0.1637 | 0.2117 | - |\r\n",
    "| 512N_7L_01LR_DrTrue | - | 512 | - | 7, Leaky ReLU | 0.1 | Sì | - | - | 0.1155 | 0.1964 | 0.2562 | - |\r\n",
    "| 512N_7L_001LR | - | 512 | - | - | 7, Leaky ReLU | 0.01 | - | - | 0.1534 | 0.1963 | 0.2562 | - |\r\n",
    "| 512N_7L_001LR_DrTrue | - | - | - | 7, Leaky ReLU | 0.01 | Sì | - | - | 0.1168 | 0.0033 | 0.0019 | - |\r\n",
    "| before_output | - | - | - | - | - | - | - | Prima del layer di output | 0.1975 | 0.1817 | 0.2352 | - |\r\n",
    "| dense_activation_batch | - | - | - | - | - | - | - | ... | 0.1975 | 0.1923 | 0.2504 | - |\r\n",
    "| dense_batch_activation | - | - | - | - | - | - | - | ... | 0.1975 | 0.1862 | 0.2413 | - |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2.3 - Regressione (livelli)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | MAE in training | MAE in validation | MAE in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| 512N_001LR | - | 512 | - | - | 0.01 | - | - | - | 0.4413 | 0.4398 | 0.3614 | - |\r\n",
    "| 512N_7L_01LR_DrTrue | - | 512 | - | 7, Leaky ReLU | 0.1 | Sì | - | - | 0.4413 | 0.4398 | 0.3614 | - |\r\n",
    "| 512N_7L_001LR | - | 512 | - | - | 7, Leaky ReLU | 0.01 | - | - | 0.4413 | 0.4398 | 0.3614 | - |\r\n",
    "| 512N_7L_001LR_DrTrue | - | - | - | 7, Leaky ReLU | 0.01 | Sì | - | - | 0.4667 | 0.4305 | 0.3551 | - |\r\n",
    "| before_output | - | - | - | - | - | - | - | Prima del layer di output | 0.0373 | 0.1850 | 0.1876 | - |\r\n",
    "| dense_activation_batch | - | - | - | - | - | - | - | ... | 0.0477 | 0.1863 | 0.1876 | - |\r\n",
    "| dense_batch_activation | - | - | - | - | - | - | - | ... | 0.0447 | 0.1832 | 0.1851 | - |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 - Ultimi tentativi"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Considerati i buoni risultati ottenuti in particolar modo dalla configurazione chiamata `512N_001LR` abbiamo provato a tenere fisso il numero di neuroni e di aumentare il numero di hidden layer. Al contempo abbiamo provato anche a tenere il learning rate a 0.01 ma il framework Tensorflow non è stato in grado di proseguire per problemi a runtime. Abbiamo quindi provato a ridurre questo tasso fino a 0.005, ovvero la metà di quello desiderato."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.1 - Classificazione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| great | - | 512 | - | 15, Leaky ReLU | 0.005 | - | - | - | 0.7230 | 0.7161 | 0.7138 | - |\r\n",
    "| smotenc | - | - | - | - | - | - | - | - | ND | ND | ND | Dataset diviso in training 50%, validation 25%, test 25%, sampling eseguito con SMOTENC. |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.2 - Regressione (dropout)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| great | - | 512 | - | 15, Leaky ReLU | 0.005 | - | - | - | 0.1476 | 0.0955 | 0.1215 | - |\r\n",
    "| smotenc | - | - | - | - | - | - | - | - | ND | ND | ND | Dataset diviso in training 50%, validation 25%, test 25%, sampling eseguito con SMOTENC. |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.3 - Regressione (livelli)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| great | - | 512 | - | 15, Leaky ReLU | 0.005 | - | - | - | 0.4413 | 0.4398 | 0.3614 | - |\r\n",
    "| smotenc | - | - | - | - | - | - | - | - | ND | ND | ND | Dataset diviso in training 50%, validation 25%, test 25%, sampling eseguito con SMOTENC. |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 - Conclusioni\r\n",
    "\r\n",
    "Per quanto riguarda il problema di classificazione, nonostante l'aumento di hidden layer notiamo come la configurazione `great` non fornisca risultati particolarmente migliori rispetto a quelli forniti da quella su cui ci eravamo basati, `512N_001LR`, aumentando al contempo il costo computazionale. Possiamo quindi ritenere la configurazione `512N_001LR` come soddisfacente per la risoluzione di questo problema.\r\n",
    "A scopo illustrativo, mostriamo l'andamento durante il training dell'accuratezza e del valore della funzione di costo.\r\n",
    "\r\n",
    "<img src=\"./src/img/512N_001LR/classification/accuracy.png\" widht=\"45%\" />\r\n",
    "<img src=\"./src/img/512N_001LR/classification/loss.png\" widht=\"45%\" />\r\n",
    "\r\n",
    "Per quanto riguarda il problema di regressione sulla colonna target DROPOUT e su quella LIVELLI, non abbiamo purtroppo ottenuto risultati soddisfacenti e per cui non riteniamo opportuno individuare il modello migliore, come fatto per quanto riguarda la classificazione."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"cluster\"></a>\r\n",
    "# 5 - Esecuzione su Cluster HPC\r\n",
    "Per via della numerosità e complessità computazionale dei test nella fase di progettazione dell'architettura della rete, abbiamo pensato di usufruire del servizio di High Performance Computing su cluster dipartimentale con GPU offerto dal dipartimento DISI.\r\n",
    "\r\n",
    "Dopo aver fatto richiesta, i nostri account istituzionali sono stati abilitati all'accesso ai sistemi dipartimentali e al cluster stesso: la macchina di nostro interesse è `slurm.cs.unibo.it` su cui si trova lo schedulatore del cluster. \r\n",
    "\r\n",
    "In dettaglio, il cluster utilizza uno schedulatore [SLURM](https://slurm.schedmd.com/overview.html) per la distribuzione \r\n",
    "dei job. Pertanto, per effettuare il submit di un job, abbiamo predisposto nella nostra area di lavoro un file di script \r\n",
    "SLURM contenente le direttive per la configurazione del job di interesse.  \r\n",
    "Sulla scia delle [raccomandazioni](https://disi.unibo.it/it/dipartimento/servizi-tecnici-e-amministrativi/servizi-informatici/utilizzo-cluster-hpc/unibo.tiles.multi.links_attachments/e7809f6f52644346a912b99dd2280788/@@objects-download/11906a7e3d2345278c0fc5ee68ce7975/file/IstruzioniUsoClusterGPU.pdf) contenute nelle istruzioni consegnateci dai tecnici del DISI, abbiamo proceduto col creare sulla macchina `slurm` un virtual environment \r\n",
    "Python, in cui, mediante il comando `pip3 install` abbiamo installato le dipendenze necessarie (specificate nel file di testo `requirements.txt`).\r\n",
    "\r\n",
    "Abbiamo creato differenti file di script SLURM per le diverse architetture neurali progettate. A titolo di esempio, riportiamo il contenuto dello script di default.\r\n",
    "\r\n",
    "```bash\r\n",
    "#!/bin/bash\r\n",
    "#SBATCH --job-name=base_model\r\n",
    "#SBATCH --mail-type=ALL\r\n",
    "#SBATCH --mail-user=tommaso.azzalin@studio.unibo.it\r\n",
    "#SBATCH --time=10:00:00\r\n",
    "#SBATCH --nodes=1\r\n",
    "#SBATCH --ntasks-per-node=1\r\n",
    "#SBATCH --output=base_model\r\n",
    "#SBATCH --gres=gpu:1\r\n",
    "\r\n",
    "cd  ../\r\n",
    "\r\n",
    ". venv/bin/activate # per attivare il virtual environment python\r\n",
    "\r\n",
    "pip3 install -r requirements.txt\r\n",
    "\r\n",
    "python3 src/base.py\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 - Lavori futuri\r\n",
    "Abbiamo intenzione di continuare a lavorare a questo progetto, perché riteniamo sia un ottimo banco di prova per maturare nuove conoscenze e competenze relative al Machine Learning. In dettaglio, prevediamo di concentrare gli sforzi futuro lungo le seguenti direzioni:\r\n",
    "- gestione dello sbilanciamento del training set mediante:\r\n",
    "    - *SMOTENC* e *K-Means*, in quanto permette di evitare la riduzione di cardinalità del training set che inevitabilmente segue al *random under-sampling*, senza rischiare l'overfitting proprio del *random over-sampling*;\r\n",
    "    - *Random Weighted Sampling* per internalizzare nell'architettura della rete la gestione dello sbilanciamento;\r\n",
    "- miglioramento del modello ai fini della regressione."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "dea9b1862afc9acd5187c15b3c157615bc525d15a5086a5de3d47d6af39a43d8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}