{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modello di deep learning per predizione dropout scolastico su dati INVALSI\n",
    "Progetto del corso di **Intelligenza Artificiale**, A.A. 2020/2021\n",
    "\n",
    "**LM Informatica**, **Alma Mater Studiorum - Università di Bologna**\n",
    "\n",
    "Realizzato da:\n",
    "- Marco Ferrati, matr. 983546\n",
    "- Michele Perlino, matr. 983733\n",
    "- Tommaso Azzalin, matr. 985911"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per aprire il notebook corrente sulla piattaforma Google Colab cliccare su [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MickPerl/MachineLearningProject/blob/main/AzzalinFerratiPerlino-INVALSI.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup dell'ambiente"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installazione delle librerie necessarie"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip3 install --no-cache-dir -r requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import delle librerie fondamentali per l'analisi dei dati"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\r\n",
    "from beautifultable import BeautifulTable\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import IntegerLookup\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping\r\n",
    "from imblearn.over_sampling import SMOTENC\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "from livelossplot import PlotLossesKeras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Com'è ben noto, il machine learning è un ambito in cui la sperimentazione occupa un ruolo molto importante: una volta consolidati i fondamenti teorici, abbiamo dovuto sperimentare innumerevoli architetture neurali differenti, tracciando di volta in volta le performance raggiunte.\\\n",
    "Al fine di rendere il codice del notebook quanto più flessibile, abbiamo creato lo script python `config.py` contenente la definizione degli iperparametri della rete; in dettaglio, in questo script, per ogni iperparametro, si verifica l'esistenza di una variabile d'ambiente col medesimo nome: in caso positivo, si assegna il valore di quest'ultima, mentre, in caso negativo, si assegna un valore di default.\n",
    "\n",
    "`LEARNING_RATE = float(getenv(key=\"LEARNING_RATE\", default=\"0.001\"))`\n",
    "\n",
    "Per settare variabili d'ambiente in maniera batch, abbiamo creato dei file con estensione `.env`:\n",
    "- se si opera da PowerShell, è sufficiente dare il comando `Set-PsEnv` per definire tutte le variabili d'ambiente presenti nel file `env` che si trova nella directory corrente;\n",
    "- se si opera da una shell Unix, si può impostare una variabile d'ambiente alla volta con `export nome_variabile=valore`: il contenuto di una variabile può essere controllato con `echo $nome_variabile`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import src.config as cfg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Configuration\")\r\n",
    "if cfg.check_config() > 0:\r\n",
    "    print(\"The configuration is incorrect. Please fix it before continuing otherwise you could encounter issues while working with this notebook.\")\r\n",
    "\r\n",
    "cfg.print_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All'interno della directory `src`, abbiamo creato altri due script python (`mapping_domande_ambiti_processi.py` e `column_converters`) contenente del codice che si è voluto separare da quello del notebook, per questioni di ordine e leggibilità. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from src.mapping_domande_ambiti_processi import MAPPING_DOMANDE_AMBITI_PROCESSI\r\n",
    "from src.column_converters import COLUMN_CONVERTERS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import del dataset originale"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "\"\"\"\r\n",
    "original_dataset = pd.read_csv(cfg.ORIGINAL_DATASET, sep=';', converters=COLUMN_CONVERTERS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA del dataset originale\n",
    "Il codice che segue realizza il processo EDA, acronimo che sta per *exploratory data analysis*, al fine di scandagliare i punti di attenzione del dataset a disposizione."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "original_dataset.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Come restituito dalla funzione `info()`, il dataset presenta $342226$ righe e $104$ colonne."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "original_dataset.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il dataset contiene svariate informazioni relative a studenti che hanno ultimato il ciclo di studi delle superiori: l'obiettivo del progetto è la progettazione e implementazione di un classificatore capace di predire, sulla base dei risultati conseguiti al test INVALSI di terza media, quali studenti registreranno un *dropout*.\\\n",
    "Il concetto di *dropout* può essere declinato su due livelli:\n",
    "- **implicito**: si dice che lo studente ha registrato un dropout implicito nel caso in cui non abbia acquisito le minime conoscenze e competenze, per cui presenta lacune formative;\n",
    "- **esplicito**: si dice che lo studente ha registrato un dropout esplicito nel caso in cui non abbia conseguito il diploma."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ricerca di colonne con alte percentuali di valori nulli"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Columns with high null values percentages:\")\r\n",
    "\r\n",
    "table = BeautifulTable()\r\n",
    "table.columns.header = [\"\", \"Type\",\"Ratio null values\"]\r\n",
    "\r\n",
    "for col in original_dataset.columns :\r\n",
    "    ratio_null_values = original_dataset[col].isnull().mean().round(3)\r\n",
    "    if ratio_null_values > 0:\r\n",
    "        table.rows.append([col, str(original_dataset[col].dtypes), ratio_null_values])\r\n",
    "table.rows.append(['LIVELLI', str(original_dataset['LIVELLI'].dtypes), original_dataset['LIVELLI'].isnull().mean().round(3)])\r\n",
    "table.rows.append(['DROPOUT', str(original_dataset['DROPOUT'].dtypes), original_dataset['DROPOUT'].isnull().mean().round(3)])\r\n",
    "        \r\n",
    "table.columns.alignment = BeautifulTable.ALIGN_LEFT\r\n",
    "table.set_style(BeautifulTable.STYLE_SEPARATED)\r\n",
    "table.rows.sort('Ratio null values')\r\n",
    "print(table)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns_high_ratio_null_values = [\"codice_orario\", \"PesoClasse\", \"PesoScuola\", \"PesoTotale_Matematica\"]\r\n",
    "columns_low_ratio_null_values = [\r\n",
    "    \"voto_scritto_ita\",  # 0.683\r\n",
    "    \"voto_scritto_mat\",  # 0.113\r\n",
    "    \"voto_orale_ita\",  # 0.683\r\n",
    "    \"voto_orale_mat\"  # 0.114\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ricerca colonne con un numero basso di valori distinti "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Columns with unique values:\")\r\n",
    "\r\n",
    "table = BeautifulTable()\r\n",
    "table.columns.header = [\"\", \"Ratio distinct values\"]\r\n",
    "\r\n",
    "for col in original_dataset.columns:\r\n",
    "    ratio_unique_vals = round(original_dataset[col].nunique() / len(original_dataset), 3)\r\n",
    "    if ratio_unique_vals  > 0.1 :\r\n",
    "        table.rows.append([col, ratio_unique_vals]) \r\n",
    "\r\n",
    "table.columns.alignment = BeautifulTable.ALIGN_LEFT\r\n",
    "table.set_style(BeautifulTable.STYLE_SEPARATED)\r\n",
    "table.rows.sort('Ratio distinct values')\r\n",
    "print(table)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Come era prevedibile, `Unnamed: 0` e `CODICE_STUDENTE` presentano una proporzione di valori distinti sul totale delle righe pari a 1, in quanto trattasi di indici/codici che identificano univocamente gli studenti. Queste due colonne potranno essere eliminate, dato che non portano alcuna informazione che possa aiutare a ravvisare relazioni tra gli studenti."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns_with_unique_values = [\"Unnamed: 0\", \"CODICE_STUDENTE\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ricerca colonne con un singolo valore"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Columns with just one value:\")\r\n",
    "for col in original_dataset.columns:\r\n",
    "    unique_vals = original_dataset[col].nunique()\r\n",
    "    if unique_vals == 1:\r\n",
    "        print(col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similmente, queste colonne possono essere eliminate in quanto non distinguono in alcuna maniera gli studenti."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns_with_just_one_value = [\"macrotipologia\", \"livello\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rimozione delle colonne superflue"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "\"\"\"\r\n",
    "cleaned_original_dataset: pd.DataFrame = original_dataset.drop(\r\n",
    "    columns_high_ratio_null_values + \r\n",
    "    columns_with_unique_values + \r\n",
    "    columns_with_just_one_value, \r\n",
    "    axis=1\r\n",
    ")\r\n",
    "cleaned_original_dataset.to_csv(cfg.CLEANED_DATASET)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "Attenzione: inutile eseguire se si è eseguita la precedente cella.\r\n",
    "\"\"\"\r\n",
    "cleaned_original_dataset = pd.read_csv(cfg.CLEANED_DATASET)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if \"Unnamed: 0\" in cleaned_original_dataset.columns:\r\n",
    "    cleaned_original_dataset.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generalizzazione dataset per supportare altre coorti di studenti\n",
    "Addestrando la rete con le domande specifiche di un certo test INVALSI non si ottiene un classificatore riutilizzabile per coorti successive. Pertanto, abbiamo pensato di mappare le feature inerenti alle domande in uno spazio più generico che permetta di cogliere la semantica delle domande piuttosto che la loro rappresentazione letterale; mediante le griglie di correzione fornite ai docenti, abbiamo notato che ogni domanda è caratterizzato da uno o più ambiti e processi: questa corrispondenza ha ispirato la trasformazione dello spazio di rappresentazione delle domande che riportiamo di seguito. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nel file `mapping_domande_ambiti_processi` posizionato nella cartella `src` abbiamo creato un dizionario le cui chiavi sono le domande e i valori gli ambiti e i processi corrispondenti. Dopo averlo importato nel notebook corrente, abbiamo estratto i distinti ambiti e processi per poi calcolarne il numero di domande che caratterizzano."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_ambiti_processi = [AP for val in MAPPING_DOMANDE_AMBITI_PROCESSI.values() for AP in val]\r\n",
    "ambiti_processi = set(list_ambiti_processi)\r\n",
    "conteggio_ambiti_processi = {AP: list_ambiti_processi.count(AP) for AP in ambiti_processi}   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abbiamo proceduto con l'aggiungere al dataset originale colonne recanti il nome degli ambiti e processi, inizializzate a 0 per tutte le righe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "\"\"\"\r\n",
    "dataset_with_ambiti_processi = cleaned_original_dataset.copy()\r\n",
    "for AP in ambiti_processi:\r\n",
    "    dataset_with_ambiti_processi[AP] = 0.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Col codice seguente, ogni studente sarà caratterizzato da un valore per ogni ambito e processo corrispondente alla proporzione di domande che vi si riferivano e a cui ha risposto correttamente: ad esempio, nel caso un processo `x` vada a caratterizzare 10 domande e lo studente risponda correttamente a 5 di queste ultime, allora il valore che quello studente presenterà sotto il processo `x` sarà pari a $0.5$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: rimozione feature sulle domande e aggiunta feature sugli ambiti e processi\r\n",
    "\"\"\"\r\n",
    "questions_columns = [col for col in list(cleaned_original_dataset) if re.search(\"^D\\d\", col)]\r\n",
    "\r\n",
    "for i, row in dataset_with_ambiti_processi.iterrows():\r\n",
    "    for question, APs in MAPPING_DOMANDE_AMBITI_PROCESSI.items():\r\n",
    "        if row[question] is True:   # se ha risposto correttamente\r\n",
    "            for AP in APs:\r\n",
    "                dataset_with_ambiti_processi.at[i, AP] += 1 / conteggio_ambiti_processi[AP]\r\n",
    "\r\n",
    "dataset_ap = dataset_with_ambiti_processi.drop(questions_columns, axis=1)\r\n",
    "\r\n",
    "dataset_ap.to_csv(cfg.CLEANED_DATASET_WITH_AP)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: rimozione feature sulle domande e aggiunta feature sugli ambiti e processi\r\n",
    "Attenzione: inutile eseguire se si è eseguita la precedente cella.\r\n",
    "\"\"\"\r\n",
    "dataset_ap = pd.read_csv(cfg.CLEANED_DATASET_WITH_AP)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if \"Unnamed: 0\" in dataset_ap.columns:\r\n",
    "    dataset_ap.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analisi della correlazione fra feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corr_matrix = dataset_ap.corr(method='pearson').round(2)\r\n",
    "corr_matrix.style.background_gradient(cmap='YlOrRd')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Emerge un'alta correlazione ($0.87$) tra i voti della stessa materia, come era prevedibile, mentre una correlazione abbastanza alta tra materie differenti ($0.75$). La correlazione pari a $1$ tra `pu_ma_gr` e `pu_ma_no` si spiega considerando che la seconda è la normalizzazione della prima, per cui portano esattamente la stessa informazione.\n",
    "\n",
    "Abbiamo ritenuto interessante indagare la correlazione sussistente tra i voti agli scritti e agli orali, i punteggi finali ottenuti al test e gli ambiti e i processi: abbiamo previsto solo valori positivi di correlazione, in quanto a voti maggiori, corrispondono punteggi finali maggiori, come anche per i processi e ambiti. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "interesting_to_check_if_correlated_columns = [\r\n",
    "    # Alta correlazione fra voti della stessa materia, abbastanza correlate fra materie diverse\r\n",
    "    \"voto_scritto_ita\",\r\n",
    "    \"voto_orale_ita\",\r\n",
    "    \"voto_scritto_mat\",\r\n",
    "    \"voto_orale_mat\",\r\n",
    "    # Correlazione totale, abbastanza correlate con voti\r\n",
    "    \"pu_ma_no\",\r\n",
    "    # Target columns\r\n",
    "    \"LIVELLI\",\r\n",
    "    \"DROPOUT\"\r\n",
    "] + list(ambiti_processi)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "check_corr_dataset = dataset_ap[interesting_to_check_if_correlated_columns].corr(method='pearson').round(2)\r\n",
    "check_corr_dataset.style.background_gradient(cmap='YlOrRd')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La nostra previsione è stata confermata."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analisi dei tipi delle colonne"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Lista colonne e tipi:\")\r\n",
    "\r\n",
    "table = BeautifulTable()\r\n",
    "table.columns.header = [\"\", \"Type\"]\r\n",
    "\r\n",
    "for col in dataset_ap.columns :\r\n",
    "    table.rows.append([col, dataset_ap[col].dtypes])\r\n",
    "        \r\n",
    "table.columns.alignment = BeautifulTable.ALIGN_LEFT\r\n",
    "table.set_style(BeautifulTable.STYLE_SEPARATED)\r\n",
    "print(table)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Le colonne DROPOUT e LIVELLI non sono considerate in quanto colonne target (in particolare, DROPOUT è una regressione di LIVELLI)\r\n",
    "continuous_features = columns_low_ratio_null_values + \\\r\n",
    "                      [\"pu_ma_gr\", \"pu_ma_no\", \"Fattore_correzione_new\", \"Cheating\", \"WLE_MAT\", \"WLE_MAT_200\", \"WLE_MAT_200_CORR\",\r\n",
    "                       \"pu_ma_no_corr\"] + \\\r\n",
    "                      list(ambiti_processi) # Feature sui voti, feature elencate, ambiti e processi\r\n",
    "if cfg.FILL_NAN == \"remove\":\r\n",
    "    continuous_features.remove(\"voto_scritto_ita\")\r\n",
    "    continuous_features.remove(\"voto_orale_ita\")\r\n",
    "ordinal_features = [\"n_stud_prev\", \"n_classi_prev\"]\r\n",
    "int_categorical_features = [\r\n",
    "    \"CODICE_SCUOLA\", \"CODICE_PLESSO\", \"CODICE_CLASSE\", \"campione\", \"prog\",\r\n",
    "]\r\n",
    "str_categorical_features = [\r\n",
    "    \"sesso\", \"mese\", \"anno\", \"luogo\", \"eta\", \"freq_asilo_nido\", \"freq_scuola_materna\",\r\n",
    "    \"luogo_padre\", \"titolo_padre\", \"prof_padre\", \"luogo_madre\", \"titolo_madre\", \"prof_madre\",\r\n",
    "    \"regolarità\", \"cittadinanza\", \"cod_provincia_ISTAT\", \"Nome_reg\",\r\n",
    "    \"Cod_reg\", \"Areageo_3\", \"Areageo_4\", \"Areageo_5\", \"Areageo_5_Istat\"\r\n",
    "]\r\n",
    "bool_features = [\"Pon\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gestione dei valori nulli\n",
    "Il dataset in considerazione presenta molti valori nulli.\\\n",
    "Vi sono diverse tecniche per gestirli, tra cui:\n",
    "- sostituzione del valore nullo con un indice di sintesi (media, mediana) della colonna in considerazione: la scelta dell'indice dipende da vari fattori, tra cui la forma della distribuzione del certo attributo;\n",
    "- rimozione della riga corrispondente: viene adottata solitamente quando la colonna presenta pochi valori nulli e/o la riga presenta molti valori nulli;\n",
    "- rimozione della colonna corrispondente: viene adottata solitamente quando la colonna presenta un alto numero di valori nulli."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_ap[\"sigla_provincia_istat\"].fillna(value=\"ND\", inplace=True)\r\n",
    "\r\n",
    "if cfg.FILL_NAN == \"remove\":\r\n",
    "    # Rimuovere colonne voti ita\r\n",
    "    # Rimuovere record con dati nulli in voti mat\r\n",
    "    dataset_ap.drop([\"voto_scritto_ita\", \"voto_orale_ita\"], axis=1, inplace=True)\r\n",
    "    dataset_ap.dropna(subset=[\"voto_scritto_mat\", \"voto_orale_mat\"], inplace=True)\r\n",
    "else :\r\n",
    "    for col in columns_low_ratio_null_values : \r\n",
    "        if cfg.FILL_NAN == \"median\":\r\n",
    "            replaced_value = dataset_ap[col].median()\r\n",
    "        elif cfg.FILL_NAN == \"mean\":\r\n",
    "            replaced_value = dataset_ap[col].mean()\r\n",
    "\r\n",
    "        dataset_ap[col].fillna(value=replaced_value, inplace=True)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning\n",
    "## Suddivisione dataset in training e test\n",
    "Avendo un dataset di una sola coorte di studenti relativi alle prove di un anno, non si dispone di un set per fare testing: Idealmente, l'insieme di testing dovrebbe referirsi ad una coorte diversa da quella su cui è stato effettuato il training. Nel caso di specie, tuttavia, si dispone dei dati relativi ad un'unica coorte, per cui abbiamo pensato di eseguire lo split in questo modo:\n",
    "\n",
    "1. dataset diviso in train_dataset (default 80%) e test_dataset (default 20%);\n",
    "2. train_dataset diviso in train_dataset (default 80%) e validation_dataset (default 20%)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_training_set, df_test_set = train_test_split(dataset_ap, test_size=cfg.TEST_SET_PERCENT, random_state=19)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analisi dello sbilanciamento del dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nr_nodrop, nr_drop = np.bincount(dataset_ap['DROPOUT'])\r\n",
    "total_records = nr_drop + nr_nodrop\r\n",
    "nl = '\\n'\r\n",
    "print(\r\n",
    "    f\"Total number of records: {total_records}{nl}\\\r\n",
    "        {nl}\\\r\n",
    "Total num. DROPOUT: {nr_drop}{nl}\\\r\n",
    "Total num. NO DROPOUT: {nr_nodrop}{nl}\\\r\n",
    "    {nl}\\\r\n",
    "Ratio DROPOUT/TOTAL: {round(nr_drop / total_records, 2)}{nl}\\\r\n",
    "Ratio NO DROPOUT/TOTAL: {round(nr_nodrop / total_records, 2)}{nl}\\\r\n",
    "    {nl}\\\r\n",
    "Ratio DROPOUT/NO DROPOUT: {round(nr_drop / nr_nodrop, 2)}\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le due classi (i.e. target del classificatore) appaiono leggermente sbilanciate: in dettaglio, la classe dei soggetti che manifestano dropout ha una cardinalità inferiore della classe in cui non si è avuto dropout."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gestione dello sbilanciamento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vi sono diverse tecniche per gestire lo sbilanciamento tra le classi di un attributo, tra cui:\n",
    "- **ricampionamento dei dati**: \n",
    "    - *random over sampling*: selezionare randomicamente delle istanze della classe sotto rappresentata e duplicarle fino a quando le cardinalità delle classi si equivalgono; aumenta il rischio di overfitting;\n",
    "    - *random under sampling*: rimuovere istanze della classe sovrarappresentata fintanto che le cardinalità delle classi si equivalgono; causa una riduzione del training set;\n",
    "    - *cluster-based over-sampling*: esecuzione dell'algoritmo *k-means* indipendentemente sulle istanze della classe maggiormente rappresentata e su quelle della classe meno rappresentata per poi compiere oversampling sui cluster ottenuti fin tanto che le cardinalità dei cluster di una stessa classe si equivalgono come anche le cardinalità delle classi nel loro complesso;\n",
    "- **generazione di dati sintetici**: \n",
    "    - *SMOTE* (Synthetic Minority Over-sampling Technique): selezionare due o più istanze simili della classe sotto rappresentata e modificare leggermente il valore di un attributo alla volta di un ammontare inferiore alla differenza tra le istanze simili; evita l'overfitting (a patto che vi siano poche attributi) ma aumenta il rumore;\n",
    "- **cambiare la natura del problema**: da classificazione a *anomaly detection* o *change detection*;\n",
    "- **penalizzazione delle classificazioni errate sulla classe sottorappresentata**: *Cost-sensitive Training*;\n",
    "- **monitorare metriche diverse dall'accuratezza**, in quanto solitamente si ottengono ottimi valori di accuratezza con dati sbilanciati, perché il modello classifica tutti gli input come appartenenti alla classe più numerosa.\n",
    "\n",
    "Alla luce di queste tecniche e considerate le peculiarità del nostro dataset emerse durante la EDA, abbiamo ritenuto che la tecnica migliore al caso nostro fosse il *random over sampling*: il nostro dataset presenta un alto numero di istanze e le classi dell'attributo target presentano un sbilanciamento poco accentuato per cui conviene ricorrere ad un sottocampionamento randomico, evitando così il rischio di overfitting, piuttosto che al sovracampionamento.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: campionamento\r\n",
    "\"\"\"\r\n",
    "if cfg.SAMPLING_TO_PERFORM == \"random_undersampling\":\r\n",
    "    # class_nodrop contiene i record della classe sovrarappresentata, ovvero SENZA DROPOUT.\r\n",
    "    class_nodrop = df_training_set[df_training_set['DROPOUT'] == False]\r\n",
    "    # class_drop contiene i record della classe sottorappresentata, ovvero CON DROPOUT.\r\n",
    "    class_drop = df_training_set[df_training_set['DROPOUT'] == True]\r\n",
    "\r\n",
    "    # Sotto campionamento di class_drop in modo che abbia stessa cardinalità di class_nodrop\r\n",
    "    class_nodrop = class_nodrop.sample(len(class_drop), random_state=19)\r\n",
    "\r\n",
    "    print(f'Class NO DROPOUT: {len(class_nodrop):,}')\r\n",
    "    print(f'Classe DROPOUT: {len(class_drop):,}')\r\n",
    "\r\n",
    "    df_training_set = class_drop.append(class_nodrop)\r\n",
    "    df_training_set = df_training_set.sample(frac=1, random_state=19)\r\n",
    "else:\r\n",
    "    categorical_features_indexes = [i for i in range(len(df_training_set.columns)) if df_training_set.columns[i] in str_categorical_features + int_categorical_features]\r\n",
    "    sm = SMOTENC(categorical_features = categorical_features_indexes, random_state=19)\r\n",
    "    X, y = sm.fit_resample(\r\n",
    "        df_training_set[[col for col in df_training_set.columns if col != 'DROPOUT']],\r\n",
    "        df_training_set['DROPOUT']\r\n",
    "    )\r\n",
    "    df_training_set = pd.concat([X, y], axis = 1)\r\n",
    "    # TODO: per farlo funzionare bisogna convertire le stringhe a interi https://stackoverflow.com/questions/65280842/smote-could-not-convert-string-to-float "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ecco che il training set è stato bilanciato."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if \"Unnamed: 0\" in df_training_set.columns:\r\n",
    "    df_training_set.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing per creazione del modello di Deep Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Suddivisione del dataset in training e validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_training_set, df_validation_set = train_test_split(df_training_set, test_size=cfg.VALIDATION_SET_PERCENT, random_state=19)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conversione dei dati da DataFrame (Pandas) a Dataset (Tensorflow/Keras)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def convert_dropout_to_one_hot(dropout_col):\r\n",
    "    dropout_col_one_hot = []\r\n",
    "    for dc in dropout_col:\r\n",
    "        if dc == 1:\r\n",
    "            dropout_col_one_hot.append([1, 0])\r\n",
    "        else:\r\n",
    "            dropout_col_one_hot.append([0, 1])\r\n",
    "    return dropout_col_one_hot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pd_dataframe_to_tf_dataset(dataframe: pd.DataFrame):\r\n",
    "    copied_df = dataframe.copy()\r\n",
    "    if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "        dropout_col = copied_df.pop(\"DROPOUT\")\r\n",
    "        dropout_col = convert_dropout_to_one_hot(dropout_col)\r\n",
    "        copied_df.drop(\"LIVELLI\", axis=1, inplace=True)\r\n",
    "    else:\r\n",
    "        dropout_col = copied_df.pop(\"LIVELLI\")\r\n",
    "        dropout_col = dropout_col.divide(other = 5)\r\n",
    "        copied_df.drop(\"DROPOUT\", axis=1, inplace=True)\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    Dato che il dataframe ha dati eterogenei lo convertiamo a dizionario,\r\n",
    "    in cui le chiavi sono i nomi delle colonne e i valori sono i valori della colonna.\r\n",
    "    Infine bisogna indicare la colonna target.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((dict(copied_df), dropout_col))\r\n",
    "    tf_dataset = tf_dataset.shuffle(buffer_size=len(copied_df), seed=19)\r\n",
    "    return tf_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds_training_set = pd_dataframe_to_tf_dataset(df_training_set)\r\n",
    "ds_validation_set = pd_dataframe_to_tf_dataset(df_validation_set)\r\n",
    "ds_test_set = pd_dataframe_to_tf_dataset(df_test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Suddivisione del dataset in batch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds_training_set = ds_training_set.batch(cfg.BATCH_SIZE, drop_remainder=True)\r\n",
    "ds_validation_set = ds_validation_set.batch(cfg.BATCH_SIZE, drop_remainder=True)\r\n",
    "ds_test_set = ds_test_set.batch(cfg.BATCH_SIZE, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creazione dei layer di input per ogni feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_layers = {}\r\n",
    "for name, column in df_training_set.items():\r\n",
    "    if name in [\"DROPOUT\", \"LIVELLI\"]:\r\n",
    "        continue\r\n",
    "\r\n",
    "    if name in continuous_features:\r\n",
    "        dtype = tf.float32\r\n",
    "    elif name in ordinal_features or name in int_categorical_features or name in bool_features:\r\n",
    "        dtype = tf.int64\r\n",
    "    else:  # str_categorical_features\r\n",
    "        dtype = tf.string\r\n",
    "\r\n",
    "    input_layers[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoding delle feature in base al loro tipo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preprocessed_features = []\r\n",
    "\r\n",
    "def stack_dict(inputs, fun=tf.stack):\r\n",
    "    values = []\r\n",
    "    for key in sorted(inputs.keys()):\r\n",
    "        values.append(tf.cast(inputs[key], tf.float32))\r\n",
    "\r\n",
    "    return fun(values, axis=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing colonne con dati booleani\r\n",
    "for name in bool_features:\r\n",
    "    inp = input_layers[name]\r\n",
    "    inp = inp[:, tf.newaxis]\r\n",
    "    float_value = tf.cast(inp, tf.float32)\r\n",
    "    preprocessed_features.append(float_value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing colonne con dati interi ordinali\r\n",
    "ordinal_inputs = {}\r\n",
    "for name in ordinal_features:\r\n",
    "    ordinal_inputs[name] = input_layers[name]\r\n",
    "\r\n",
    "normalizer = Normalization(axis=-1)\r\n",
    "normalizer.adapt(stack_dict(dict(df_training_set[ordinal_features])))\r\n",
    "ordinal_inputs = stack_dict(ordinal_inputs)\r\n",
    "ordinal_normalized = normalizer(ordinal_inputs)\r\n",
    "preprocessed_features.append(ordinal_normalized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing colonne con dati continui\r\n",
    "continuous_inputs = {}\r\n",
    "for name in continuous_features:\r\n",
    "    continuous_inputs[name] = input_layers[name]\r\n",
    "\r\n",
    "normalizer = Normalization(axis=-1)\r\n",
    "normalizer.adapt(stack_dict(dict(df_training_set[continuous_features])))\r\n",
    "continuous_inputs = stack_dict(continuous_inputs)\r\n",
    "continuous_normalized = normalizer(continuous_inputs)\r\n",
    "preprocessed_features.append(continuous_normalized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per i dati categorici di tipo stringa o intero abbiamo adottato la *one-hot encoding*; tale codifica consiste nel mappare ogni categoria ad un array contenente n-1 elementi settati a 0 e 1 elemento settato a 1, con n pari al numero delle categorie: l'indice dell'elemento settato ad 1 permette di distinguere le diverse categorie."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing colonne con dati categorici stringa\r\n",
    "for name in str_categorical_features:\r\n",
    "    vocab = sorted(set(df_training_set[name]))\r\n",
    "\r\n",
    "    lookup = StringLookup(vocabulary=vocab, output_mode='one_hot')\r\n",
    "\r\n",
    "    x = input_layers[name][:, tf.newaxis]\r\n",
    "    x = lookup(x)\r\n",
    "\r\n",
    "    preprocessed_features.append(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing colonne con dati categorici interi\r\n",
    "for name in int_categorical_features:\r\n",
    "    vocab = sorted(set(df_training_set[name]))\r\n",
    "\r\n",
    "    lookup = IntegerLookup(vocabulary=vocab, output_mode='one_hot')\r\n",
    "\r\n",
    "    x = input_layers[name][:, tf.newaxis]\r\n",
    "    x = lookup(x)\r\n",
    "\r\n",
    "    preprocessed_features.append(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assemblaggio dei vari layer e creazione del modello\n",
    "\n",
    "### Funzioni di attivazione\n",
    "Abbiamo studiato e approfondito le principali funzioni di attivazione, passandone in rassegna le peculiarità, i pro e i contro. Alla luce di tali nuove conoscenze, abbiamo optato per le seguenti scelte:\n",
    "- *Leaky ReLU* per gli hidden layer: \n",
    "    - abbiamo preferito la variante *Leaky ReLU* piuttosto che la classica ReLU, in quanto la prima, prevedendo una leggera pendenza a sinistra dell'origine, evita il *Dying ReLU Problem*, di cui soffre invece la ReLU;\n",
    "        - SOLO PER I MIEI COLLEGHI --> la ReLU restituisce 0 per tutte le somme pesate negative, per cui frequenti sono i neuroni che non si attivano (muoiono), non permettendo al gradiente di fluire durante la backpropagation (quindi non vi è aggiornamento dei pesi);\n",
    "    - il codominio ha cardinalità infinita (dato che per input > 0, diventa la funzione di identità), per cui non vi è una significativa perdita di informazione nel passaggio da un layer al successivo;\n",
    "    - a fronte di somme pesate negative in input, restituisce valori negativi molto bassi che portano a calcoli non pesanti computazionalmente, registrando così tempi minori di addestramento per il modello nel complesso.\n",
    "![](./src/img/leaky_relu.png)\n",
    "- *Sigmoid* per l'output layer: è la funzione che viene [consigliata](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/) come *best practice* per il livello di output in problemi di classificazione binaria."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inizializzazione dei pesi\r\n",
    "L'inizializzazione dei pesi è un aspetto molto importante nella progettazione di una rete neurale: essi incidono in larga misura sulla somma pesata che si accumula in ogni neurone e che viene passata in input alla sua funzione di attivazione, determinando anche la misura in cui l'informazione passa da un layer al successivo.\r\n",
    "\r\n",
    "I modelli di reti neurali vengono addestrati mediante l'algoritmo di ottimizzazione noto come **discesa del gradiente stocastico**, che modifica iterativamente i pesi della rete con l'obiettivo di minimizzare una funzione di costo: alla fine dell'addestramento, si giunge a quella combinazione di pesi che permettono alla rete di avere una certa performance nelle previsioni.\r\n",
    "L'algoritmo di ottimizzazione richiede un punto di partenza nello spazio dei possibili valori dei pesi e spesso la sua velocità di convergenza o effettiva convergenza è fortemente determinata dall'inizializzazione iniziale dei pesi.\r\n",
    "\r\n",
    "In letteratura, si suggeriscono diverse funzioni di inizializzazione a seconda della funzione di attivazione del layer in oggetto:\r\n",
    "- con ReLU o Leaky ReLU, è consigliato il metodo di inizializzazione **He** che calcola i pesi a partire dalla distribuzione di probabilità gaussiana con media 0 e deviazione standard pari a $\\sqrt{\\frac{2}{n}}$, con n il numero di input che arrivano al neurone;\r\n",
    "    - conseguentemente, l'altezza della gaussiana dipende dal numero di input che il neurone riceve: all'aumentare del numero di questi input, l'altezza aumenta e la gaussiana si restringe, diminuendo la probabilità di avere pesi alti. Così i neuroni tendono meno a saturare, permettendo alla rete di raggiungere il massimo dell'accuratezza in meno tempo. Infatti, tale accorgimento non migliora l'accuratezza, ma solo il tempo che la rete impiega a raggiungere quella massima.\r\n",
    "[<img src=\"./src/img/He.png\" width=\"50%\"/>](https://alessiomorselli.github.io/DeepLearning/web/pages/documentation/gaussian2.png)\r\n",
    "- con Sigmoid o Tanh, inizializzazione **Glorot** (anche nota *Xavier*, dal nome di *Xavier Glorot*, un ricercatore presso Google DeepMind): questo metodo, per ogni neurone, calcola i diversi pesi da una distribuzione di probabilità uniforme che ha come dominio il range $-(1/\\sqrt{n})$ e $(1/\\sqrt{n})$, con $n$ il numero di input che arrivano al neurone considerato.\r\n",
    "    - l'immagine seguente mostra il restringersi del range in cui cadono tali pesi all'aumentare del numero di input entranti nel neurone.\r\n",
    "[<img src=\"./src/img/Xavier.png\" width=\"50%\"/>](https://machinelearningmastery.com/wp-content/uploads/2021/01/Plot-of-Range-of-Xavier-Weight-Initialization-with-Inputs-from-One-to-One-Hundred-.png)\r\n",
    "    - esiste anche una variante di tale metodo, chiamata **Normalized Xavier Weight Initialization**: essa estrae i valori da una distribuzione di probabilità uniforme, tuttavia il suo dominio è compreso tra $-(\\sqrt{6}/\\sqrt{n + m})$ e $(\\sqrt{6}/\\sqrt{n + m})$, dove $n$ è il numero di input al nodo in oggetto e $m$ è il numero di output del layer di cui fa parte quel neurone (ossia il numero di neuroni di quel layer).\r\n",
    "[<img src=\"./src/img/Normalized_Xavier.png\" width=\"50%\"/>](https://machinelearningmastery.com/wp-content/uploads/2021/01/Plot-of-Range-of-Normalized-Xavier-Weight-Initialization-with-Inputs-from-One-to-One-Hundred.png)\r\n",
    "\r\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preprocessed = tf.concat(preprocessed_features, axis=-1) # Tensore\r\n",
    "\r\n",
    "preprocessor = tf.keras.Model(input_layers, preprocessed) # Tanti input layer quante le feature\r\n",
    "\r\n",
    "initializer_hidden_layer = tf.keras.initializers.HeNormal(seed=19) # inizializzatore che verrà usato per i pesi dei layer con ReLU / LeakyReLU\r\n",
    "initializer_output_layer = tf.keras.initializers.GlorotNormal(seed=19) # inizializzatore che verrà usato per i pesi dei layer con sigmoid"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regolarizzazione per evitare overfitting\n",
    "Si ha **overfitting** ogniqualvolta il modello apprende i dati troppo bene, ossia ne incamera il rumore statistico: conseguentemente, il modello presenta un *alto errore di generalizzazione* (scarse performance quando il modello è valutato su dati nuovi, come quelli dell'insieme di test).\n",
    "Il rischio di overfitting si fa più concreto quando la cardinalità del training set è bassa e/o la rete neurale è molto grande.\n",
    "\n",
    "Per ridurre al minimo il rischio di overfitting, vi sono molti metodi di **regolarizzazione**, tra cui il **dropout**: esso approssima l'addestramento in parallelo di un gran numero di reti neurali con architetture differenti; consiste nell'assegnare ad ogni neurone di un certo layer una probabilità 1-p che sia ignorato e p che non sia ignorato, per cui si concretizza in un sottocampionamento degli output dei vari layer.\n",
    "La probabilità p viene settato a livello del singolo layer; in letteratura, sono suggeriti valori prossimi a $0.5$ per i layer nascosti e prossimi a 1 per i layer visibili, mentre di non settare alcun dropout per il layer di output. Noi abbiamo rispettato tali suggerimenti, come è possibile constatare nel codice che segue.\n",
    "\n",
    "Un effetto collaterale del dropout è la riduzione della capacità rappresentativa della rete, per cui è conveniente solo a fronte di reti neurali grandi: nel caso la rete abbia dimensioni modeste, occorre aumentarne prima l'ampiezza e la profondità e poi settare il dropout ai vari layer. Questo motivo è alla base della nostra scelta di raddoppiare i valori di default degli iperparametri relativi al numero di layer e al numero di neuroni nelle configurazioni in cui è presente il dropout. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "body = tf.keras.Sequential()\r\n",
    "if cfg.DROPOUT_LAYER:\r\n",
    "    body.add(tf.keras.layers.Dropout(rate=cfg.DROPOUT_INPUT_LAYER_RATE, seed=19)) # aggiunta dropout a layer di input\r\n",
    "\r\n",
    "# segue l'aggiunta degli hidden layers\r\n",
    "for _ in range(cfg.NUMBER_OF_LAYERS):\r\n",
    "    body.add(tf.keras.layers.Dense(cfg.NEURONS, kernel_initializer=initializer_hidden_layer))\r\n",
    "    if cfg.ACTIVATION_LAYER == \"leaky_relu\":\r\n",
    "        body.add(tf.keras.layers.LeakyReLU())\r\n",
    "    else:\r\n",
    "        body.add(tf.keras.layers.ReLU())\r\n",
    "    \r\n",
    "    if cfg.DROPOUT_LAYER:\r\n",
    "        body.add(tf.keras.layers.Dropout(rate=cfg.DROPOUT_HIDDEN_LAYER_RATE, seed=19))\r\n",
    "\r\n",
    "# segue l'aggiunta dell'output layer\r\n",
    "if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "    body.add(tf.keras.layers.Dense(2, activation=\"softmax\", kernel_initializer=initializer_output_layer))\r\n",
    "else:\r\n",
    "    body.add(tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer_output_layer))\r\n",
    "\r\n",
    "x = preprocessor(input_layers)\r\n",
    "\r\n",
    "result = body(x)\r\n",
    "\r\n",
    "model = tf.keras.Model(input_layers, result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualizzazione tabellare del modello\r\n",
    "from keras.utils.vis_utils import plot_model\r\n",
    "model.summary()\r\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compilazione del modello e scelta delle metriche"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "    accuracy = tf.keras.metrics.Accuracy(name=\"acc\")\r\n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy()\r\n",
    "else:\r\n",
    "    accuracy = tf.metrics.BinaryAccuracy(name=\"bin_acc\", threshold=0.4) # 0.4 perché LIVELLI in [0,1,2] è DROPOUT = True, LIVELLI in [3,4,5] è DROPOUT = False\r\n",
    "    loss_function = tf.keras.losses.BinaryCrossentropy()\r\n",
    "\r\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=cfg.LEARNING_RATE),\r\n",
    "              loss=loss_function,\r\n",
    "              metrics=[\r\n",
    "                accuracy,\r\n",
    "                tf.keras.metrics.FalsePositives(name=\"fp\"),\r\n",
    "                tf.keras.metrics.FalseNegatives(name=\"fn\"),\r\n",
    "                tf.keras.metrics.TruePositives(name=\"tp\"),\r\n",
    "                tf.keras.metrics.TrueNegatives(name=\"tn\"),\r\n",
    "                tf.keras.metrics.Precision(name=\"prec\"),\r\n",
    "                tf.keras.metrics.Recall(name=\"rec\")\r\n",
    "              ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Early stopping\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Definizione dello stopper per evitare che la reti continui ad addestrarsi quando non ci sono miglioramenti della loss \r\n",
    "(val_loss = funzione di costo sul validation set) per piu' di 3 epoche\r\n",
    "\"\"\"\r\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\r\n",
    "                              patience=5,\r\n",
    "                              mode=\"min\",\r\n",
    "                              restore_best_weights=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(ds_training_set,\r\n",
    "          epochs=cfg.EPOCH,\r\n",
    "          batch_size=cfg.BATCH_SIZE,\r\n",
    "          validation_data=ds_validation_set,\r\n",
    "          callbacks=([early_stopper] if cfg.EARLY_STOPPING else []) + [PlotLossesKeras()],\r\n",
    "          verbose=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score = model.evaluate(ds_test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Results with test dataset')\r\n",
    "print('Loss:', round(score[0], 4))\r\n",
    "print('Accuracy:', round(score[1], 4))\r\n",
    "print('False positives:', int(score[2]))\r\n",
    "print('False negatives:', int(score[3]))\r\n",
    "print('True positives:', int(score[4]))\r\n",
    "print('True negatives:', int(score[5]))\r\n",
    "print('Precision: ', round(score[6], 4))\r\n",
    "print('Recall: ', round(score[7], 4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calcolo manuale delle metriche a partire dalla matrice di confusione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def convert_df_for_prediction(dataframe: pd.DataFrame):\r\n",
    "    copied_df = dataframe.copy()\r\n",
    "    ds = tf.data.Dataset.from_tensor_slices(dict(copied_df))\r\n",
    "\r\n",
    "    return ds.batch(cfg.BATCH_SIZE, drop_remainder=True)\r\n",
    "\r\n",
    "target_col = \"DROPOUT\" if cfg.PROBLEM_TYPE == \"classification\" else \"LIVELLI\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_x = convert_df_for_prediction(df_training_set[[col for col in df_training_set.columns if col not in [\"DROPOUT\", \"LIVELLI\"]]])\r\n",
    "training_y = df_training_set[target_col]\r\n",
    "if len(training_x)*cfg.BATCH_SIZE - len(training_y) != 0:\r\n",
    "    training_y = training_y.head(len(training_x)*cfg.BATCH_SIZE - len(training_y))\r\n",
    "if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "    training_y = convert_dropout_to_one_hot(training_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "validation_x = convert_df_for_prediction(df_validation_set[[col for col in df_validation_set.columns if col not in [\"DROPOUT\", \"LIVELLI\"]]])\r\n",
    "validation_y = df_validation_set[target_col]\r\n",
    "if len(validation_x)*cfg.BATCH_SIZE - len(validation_y) != 0:\r\n",
    "    validation_y = validation_y.head((len(validation_x)*cfg.BATCH_SIZE) - len(validation_y))\r\n",
    "if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "    validation_y = convert_dropout_to_one_hot(validation_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_x = convert_df_for_prediction(df_test_set[[col for col in df_test_set.columns if col not in [\"DROPOUT\", \"LIVELLI\"]]])\r\n",
    "test_y = df_test_set[target_col]\r\n",
    "if (len(test_x)*cfg.BATCH_SIZE) - len(test_y) != 0:\r\n",
    "    test_y = test_y.head(len(test_x)*cfg.BATCH_SIZE - len(test_y))\r\n",
    "if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "    test_y = convert_dropout_to_one_hot(test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predicted_training_y = model.predict(training_x)\r\n",
    "predicted_validation_y = model.predict(validation_x)\r\n",
    "predicted_test_y = model.predict(test_x)\r\n",
    "\r\n",
    "if cfg.PROBLEM_TYPE == \"regression\":\r\n",
    "    predicted_training_y = np.round(predicted_training_y * 5)\r\n",
    "    predicted_validation_y = np.round(predicted_validation_y * 5)\r\n",
    "    predicted_test_y = np.round(predicted_test_y * 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_confusion_matrix = tf.math.confusion_matrix(labels=training_y, predictions=predicted_training_y).numpy()\r\n",
    "validation_confusion_matrix = tf.math.confusion_matrix(labels=validation_y, predictions=predicted_validation_y).numpy()\r\n",
    "test_confusion_matrix = tf.math.confusion_matrix(labels=test_y, predictions=predicted_test_y).numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_metrics(label, confusion_matrix):\r\n",
    "    true_positives = np.diag(confusion_matrix) # vettore in cui ogni cella è il numero di TP per la classe\r\n",
    "    false_positives = confusion_matrix.sum(axis=0) - true_positives\r\n",
    "    false_negatives = confusion_matrix.sum(axis=1) - true_positives\r\n",
    "    true_negatives = confusion_matrix.sum() - (true_positives.sum() + false_positives.sum() + false_negatives.sum())\r\n",
    "\r\n",
    "    accuracy = (true_positives.sum() + true_negatives.sum()) / confusion_matrix.sum()\r\n",
    "    precision = true_positives.sum() / (true_positives.sum() + false_positives.sum())\r\n",
    "    recall = true_positives.sum() / (true_positives.sum() + false_negatives.sum())\r\n",
    "\r\n",
    "    print(f\"Confusion Matrix Name: {label}\")\r\n",
    "    print(confusion_matrix)\r\n",
    "    print(f\"- TP: {int(true_positives.sum())}\")\r\n",
    "    print(f\"- TN: {int(true_negatives.sum())}\")\r\n",
    "    print(f\"- FP: {int(false_positives.sum())}\")\r\n",
    "    print(f\"- FN: {int(false_negatives.sum())}\")\r\n",
    "    print(f\"- Accuracy: {round(accuracy, 4)}\") # Attenzione: si tratta della Binary Accuracy\r\n",
    "    print(f\"- Precision: {round(precision, 4)}\")\r\n",
    "    print(f\"- Recall: {round(recall, 4)}\")\r\n",
    "    print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "compute_metrics(\"Training\", training_confusion_matrix)\r\n",
    "compute_metrics(\"Validation\", validation_confusion_matrix)\r\n",
    "compute_metrics(\"Test\", test_confusion_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Risultati ottenuti"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "| Epoche | Neuroni | Batch | Tecnica di sampling | Layer lineari | Tasso apprendimento | Dropout | Tasso di dropout | Accuratezza in training | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "|     10 |     128 |    32 |       Undersampling |            10 |               0.001 |      No |                - |                  0.2339 |              0.2336 | C'erano colonne con alta correlazione. |\r\n",
    "|     50 |     128 |    32 |       Undersampling |            10 |               0.001 |      No |                - |                  0.6938 |              0.6874 | C'erano colonne con alta correlazione. |\r\n",
    "|     50 |     256 |    32 |       Undersampling |            10 |               0.001 |      No |                - |                  0.6944 |              0.6873 | C'erano colonne con alta correlazione. |\r\n",
    "|     50 |     128 |   128 |       Undersampling |            10 |               0.001 |      No |                - |                  0.2386 |              0.2341 | C'erano colonne con alta correlazione. Perché è così pessimo il risultato? |\r\n",
    "|   | | | | | | | | | |\r\n",
    "|     50 |     128 |    32 |       Undersampling |             5 |               0.001 |      No |                - |                         |                     |                                   |\r\n",
    "|     50 |     128 |    32 |       Undersampling |            15 |               0.001 |      No |                - |                         |                     |                                   |\r\n",
    "|     50 |     128 |    32 |       Undersampling |            10 |               0.001 |      No |                - |                         |                     | ReLU, anziché Leaky ReLU          |\r\n",
    "|     50 |     256 |    32 |       Undersampling |            10 |               0.001 |      No |                - |                         |                     |           |\r\n",
    "|     50 |     256 |   128 |       Undersampling |            10 |               0.001 |      No |                - |                         |                     |           |\r\n",
    "|     50 |     128 |   128 |       Undersampling |            10 |               0.001 |      No |                - |                         |                     |           |\r\n",
    "|     50 |     128 |   32  |       Undersampling |            10 |               0.001 |      No |                - |                         |                     |  Median per fill_NA       |\r\n",
    "|     50 |     128 |   32  |       Undersampling |            10 |               0.001 |      No |                - |                         |                     |  Mean per fill_NA       |\r\n",
    "|     50 |     128 |   32  |       Undersampling |            10 |               0.001 |      No |                - |                         |                     |  Remove per fill_NA       |\r\n",
    "|     50 |     128 |   32  |       Undersampling |            10 |               0.01 |      No |                - |                         |                     |        |\r\n",
    "|     50 |     128 |   32  |       Undersampling |            10 |               0.001 |      No |                - |                         |                     |   tanh anzichè ReLU/Leaky ReLU     |\r\n",
    "|    100 |     128 |   32  |       Undersampling |            10 |               0.001 |      No |                - |                         |                     |  Senza early stopping     |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per info su come formattare Markdown: [clicca qui](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Esecuzione su Cluster HPC\n",
    "Per via della numerosità e complessità computazionale dei test nella fase di progettazione dell'architettura della rete, abbiamo pensato di usufruire del servizio di High Performance Computing su cluster dipartimentale con GPU offerto dal dipartimento DISI.\n",
    "\n",
    "Dopo aver fatto richiesta, i nostri account istituzionali sono stati abilitati all'accesso ai sistemi dipartimentali e al cluster stesso: la macchina di nostro interesse è `slurm.cs.unibo.it` su cui si trova lo schedulatore del cluster. \n",
    "\n",
    "In dettaglio, il cluster utilizza uno schedulatore [SLURM](https://slurm.schedmd.com/overview.html) per la distribuzione \n",
    "dei job. Pertanto, per effettuare il submit di un job, abbiamo predisposto nella nostra area di lavoro un file di script \n",
    "SLURM contenente le direttive per la configurazione del job di interesse.\\\n",
    "Sulla scia delle [raccomandazioni](https://disi.unibo.it/it/dipartimento/servizi-tecnici-e-amministrativi/servizi-informatici/utilizzo-cluster-hpc/unibo.tiles.multi.links_attachments/e7809f6f52644346a912b99dd2280788/@@objects-download/11906a7e3d2345278c0fc5ee68ce7975/file/IstruzioniUsoClusterGPU.pdf) contenute nelle istruzioni consegnateci dai tecnici del DISI, abbiamo proceduto col creare sulla macchina slurm un virtual environment \n",
    "Python, in cui, mediante il comando `pip3 install` abbiamo installato le dipendenze necessarie (specificate nel file di testo `requirements.txt`).\n",
    "\n",
    "Abbiamo creato differenti file di script SLURM per le diverse architetture neurali progettate. A titolo di esempio, riportiamo il contenuto dello script di default.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=base_model\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=tommaso.azzalin@studio.unibo.it\n",
    "#SBATCH --time=10:00:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --output=base_model\n",
    "#SBATCH --gres=gpu:1\n",
    "\n",
    "cd  ../\n",
    "\n",
    ". venv/bin/activate # per attivare il virtual environment python\n",
    "\n",
    "pip3 install -r requirements.txt\n",
    "\n",
    "python3 src/base.py\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lavori futuri\r\n",
    "Abbiamo intenzione di continuare a lavorare a questo progetto, perché riteniamo sia un ottimo banco di prova per maturare nuove conoscenze e competenze relative al Machine Learning. In dettaglio, prevediamo di concentrare gli sforzi futuro lungo le seguenti direzioni:\r\n",
    "- gestione dello sbilanciamento del training set mediante:\r\n",
    "    - *SMOTE*, in quanto permette di evitare la riduzione di cardinalità del training set che inevitabilmente segue al *random under-sampling*, senza rischiare l'overfitting proprio del *random over-sampling*;\r\n",
    "    - *Random Weighted Sampling*."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}