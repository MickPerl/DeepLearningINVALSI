{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modello di deep learning per predizione dropout scolastico su dati INVALSI\r\n",
    "Progetto del corso di **Intelligenza Artificiale**, A.A. 2020/2021\r\n",
    "\r\n",
    "**LM Informatica**, **Alma Mater Studiorum - Università di Bologna**\r\n",
    "\r\n",
    "Realizzato da:\r\n",
    "- Marco Ferrati, matr. 983546\r\n",
    "- Michele Perlino, matr. 983733\r\n",
    "- Tommaso Azzalin, matr. 985911"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 - Setup dell'ambiente"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 - Installazione delle librerie necessarie"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip3 install --no-cache-dir -r requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 - Import delle librerie fondamentali per l'analisi dei dati"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\r\n",
    "from beautifultable import BeautifulTable\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import IntegerLookup\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping\r\n",
    "from imblearn.over_sampling import SMOTENC\r\n",
    "from livelossplot import PlotLossesKeras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Com'è ben noto, il machine learning è un ambito in cui la sperimentazione occupa un ruolo molto importante: una volta consolidati i fondamenti teorici, abbiamo dovuto sperimentare innumerevoli architetture neurali differenti, tracciando di volta in volta le performance raggiunte.\\\r\n",
    "Al fine di rendere il codice del notebook quanto più flessibile, abbiamo creato il file Python `config.py` contenente la definizione degli iperparametri della rete; in dettaglio, in questo script, per ogni iperparametro, si verifica l'esistenza di una variabile d'ambiente col medesimo nome: in caso positivo, si assegna il valore di quest'ultima, mentre, in caso negativo, si assegna un valore di default.\r\n",
    "\r\n",
    "Ad esempio:\r\n",
    "\r\n",
    "`LEARNING_RATE = float(getenv(key=\"LEARNING_RATE\", default=\"0.001\"))`\r\n",
    "\r\n",
    "Per impostare variabili d'ambiente in maniera batch, abbiamo creato dei file con estensione `.env`:\r\n",
    "- se si opera da PowerShell, è sufficiente dare il comando `Set-PsEnv` (installabile mediante comando `Install-Module -Name Set-PsEnv`) per definire tutte le variabili d'ambiente presenti nel file `.env` che si trova nella directory corrente;\r\n",
    "- se si opera da una shell Unix, si può impostare una variabile d'ambiente alla volta con `export nome_variabile=valore`: il contenuto di una variabile può essere controllato con `echo $nome_variabile`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import src.config as cfg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Configuration\")\r\n",
    "if cfg.check_config() > 0:\r\n",
    "    print(\"The configuration is incorrect. Please fix it before continuing otherwise you could encounter issues while working with this notebook.\")\r\n",
    "\r\n",
    "cfg.print_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "All'interno della directory `src`, abbiamo creato altri due file Python (`mapping_domande_ambiti_processi.py` e `column_converters.py`) contenente del codice che si è voluto separare da quello del notebook, per questioni di ordine e leggibilità. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from src.mapping_domande_ambiti_processi import MAPPING_DOMANDE_AMBITI_PROCESSI\r\n",
    "from src.column_converters import COLUMN_CONVERTERS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nella cartella `src`, di cui sopra, sono presenti altri due file Python `invalsi.py` e `save_plots.py`. Il primo contiene del codice equivalente a quello presente in questo notebook, ma che può essere eseguito da linea di comando come un qualsiasi script Python; la sua creazione si è resa necessaria per l'esecuzione degli script attraverso i job avviati sul Cluster HPC del DISI, descritto nella sezione [Esecuzione su Cluster HPC](#cluster). Il secondo contiene del codice per memorizzare dei grafici sotto forma di file immagine contenenti gli andamenti delle metriche che permettono di valutare il funzionamento di un'architettura neurale."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 - Import del dataset originale"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "\"\"\"\r\n",
    "original_dataset = pd.read_csv(cfg.ORIGINAL_DATASET, sep=';', converters=COLUMN_CONVERTERS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 - Exploratory Data Analysis del dataset originale\r\n",
    "Il codice che segue realizza il processo EDA, acronimo che sta per *Exploratory Data Analysis*, al fine di scandagliare i punti di attenzione del dataset a disposizione."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "original_dataset.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Come restituito dalla funzione `info()`, il dataset presenta 342226 righe e 104 colonne."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "original_dataset.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il dataset contiene svariate informazioni relative a studenti che hanno ultimato il ciclo di studi delle superiori: l'obiettivo del progetto è la progettazione e implementazione di un classificatore capace di predire, sulla base dei risultati conseguiti al test INVALSI di terza media, quali studenti registreranno un *dropout*.\\\r\n",
    "Il concetto di *dropout* può essere declinato su due livelli:\r\n",
    "- **implicito**: si dice che lo studente ha registrato un dropout implicito nel caso in cui non abbia acquisito le minime conoscenze e competenze, per cui presenta lacune formative;\r\n",
    "- **esplicito**: si dice che lo studente ha registrato un dropout esplicito nel caso in cui non abbia conseguito il diploma."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 - Ricerca di colonne con alte percentuali di valori nulli"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Columns with high null values percentages:\")\r\n",
    "\r\n",
    "table = BeautifulTable()\r\n",
    "table.columns.header = [\"\", \"Type\",\"Ratio null values\"]\r\n",
    "\r\n",
    "for col in original_dataset.columns :\r\n",
    "    ratio_null_values = original_dataset[col].isnull().mean().round(3)\r\n",
    "    if ratio_null_values > 0:\r\n",
    "        table.rows.append([col, str(original_dataset[col].dtypes), ratio_null_values])\r\n",
    "table.rows.append(['LIVELLI', str(original_dataset['LIVELLI'].dtypes), original_dataset['LIVELLI'].isnull().mean().round(3)])\r\n",
    "table.rows.append(['DROPOUT', str(original_dataset['DROPOUT'].dtypes), original_dataset['DROPOUT'].isnull().mean().round(3)])\r\n",
    "        \r\n",
    "table.columns.alignment = BeautifulTable.ALIGN_LEFT\r\n",
    "table.set_style(BeautifulTable.STYLE_SEPARATED)\r\n",
    "table.rows.sort('Ratio null values')\r\n",
    "print(table)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns_high_ratio_null_values = [\"codice_orario\", \"PesoClasse\", \"PesoScuola\", \"PesoTotale_Matematica\"]\r\n",
    "columns_low_ratio_null_values = [\r\n",
    "    \"voto_scritto_ita\",  # 0.683\r\n",
    "    \"voto_scritto_mat\",  # 0.113\r\n",
    "    \"voto_orale_ita\",  # 0.683\r\n",
    "    \"voto_orale_mat\"  # 0.114\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 - Ricerca colonne con un numero basso di valori distinti "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Columns with unique values:\")\r\n",
    "\r\n",
    "table = BeautifulTable()\r\n",
    "table.columns.header = [\"\", \"Ratio distinct values\"]\r\n",
    "\r\n",
    "for col in original_dataset.columns:\r\n",
    "    ratio_unique_vals = round(original_dataset[col].nunique() / len(original_dataset), 3)\r\n",
    "    if ratio_unique_vals  > 0.1 :\r\n",
    "        table.rows.append([col, ratio_unique_vals]) \r\n",
    "\r\n",
    "table.columns.alignment = BeautifulTable.ALIGN_LEFT\r\n",
    "table.set_style(BeautifulTable.STYLE_SEPARATED)\r\n",
    "table.rows.sort('Ratio distinct values')\r\n",
    "print(table)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Come era prevedibile, `Unnamed: 0` (corrispondente alla colonna con l'indice della riga) e `CODICE_STUDENTE` presentano una proporzione di valori distinti sul totale delle righe pari a 1, in quanto trattasi di indici/codici che identificano univocamente gli studenti. Queste due colonne possono essere eliminate, dato che non portano alcuna informazione che possa aiutare a ravvisare relazioni tra gli studenti."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns_with_unique_values = [\"Unnamed: 0\", \"CODICE_STUDENTE\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 - Ricerca colonne con un singolo valore"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Columns with just one value:\")\r\n",
    "for col in original_dataset.columns:\r\n",
    "    unique_vals = original_dataset[col].nunique()\r\n",
    "    if unique_vals == 1:\r\n",
    "        print(col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similmente, queste colonne possono essere eliminate in quanto non distinguono in alcuna maniera gli studenti."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns_with_just_one_value = [\"macrotipologia\", \"livello\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 - Rimozione delle colonne superflue"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "\"\"\"\r\n",
    "cleaned_original_dataset: pd.DataFrame = original_dataset.drop(\r\n",
    "    columns_high_ratio_null_values + \r\n",
    "    columns_with_unique_values + \r\n",
    "    columns_with_just_one_value, \r\n",
    "    axis=1\r\n",
    ")\r\n",
    "cleaned_original_dataset.to_csv(cfg.CLEANED_DATASET)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "Attenzione: inutile eseguire se si è eseguita la precedente cella.\r\n",
    "\"\"\"\r\n",
    "cleaned_original_dataset = pd.read_csv(cfg.CLEANED_DATASET)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if \"Unnamed: 0\" in cleaned_original_dataset.columns:\r\n",
    "    cleaned_original_dataset.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 - Generalizzazione dataset per supportare altre coorti di studenti\r\n",
    "Addestrando la rete con le domande specifiche di un test INVALSI di uno specifico anno non si ottiene un classificatore riutilizzabile per coorti successive (le domande cambiano ogni anno). Pertanto, abbiamo pensato di mappare le feature inerenti alle domande in uno spazio più generico che permetta di cogliere la loro semantica piuttosto che la loro rappresentazione letterale; mediante le griglie di correzione fornite ai docenti, abbiamo notato che ogni domanda è caratterizzata da uno o più ambiti e processi: questa corrispondenza ha ispirato la trasformazione dello spazio di rappresentazione delle domande che riportiamo di seguito. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nel file `mapping_domande_ambiti_processi.py` posizionato nella cartella `src` abbiamo creato una mappa (`dict` in Python) le cui chiavi sono le domande e i valori gli ambiti e i processi corrispondenti. Dopo averlo importato nel notebook corrente, abbiamo estratto i distinti ambiti e processi per poi calcolarne il numero di domande che caratterizzano."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list_ambiti_processi = [AP for val in MAPPING_DOMANDE_AMBITI_PROCESSI.values() for AP in val]\r\n",
    "ambiti_processi = set(list_ambiti_processi)\r\n",
    "conteggio_ambiti_processi = {AP: list_ambiti_processi.count(AP) for AP in ambiti_processi}   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abbiamo proceduto con l'aggiungere al dataset originale colonne recanti il nome degli ambiti e processi, inizializzate a 0 per tutte le righe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: analisi dataset\r\n",
    "\"\"\"\r\n",
    "dataset_with_ambiti_processi = cleaned_original_dataset.copy()\r\n",
    "for AP in ambiti_processi:\r\n",
    "    dataset_with_ambiti_processi[AP] = 0.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Col codice seguente, ogni studente sarà caratterizzato da un valore per ogni ambito e processo corrispondente alla proporzione di domande che vi si riferivano e a cui ha risposto correttamente: ad esempio, nel caso un processo `x` vada a caratterizzare 10 domande e lo studente risponda correttamente a 5 di queste ultime, allora il valore che quello studente presenterà sotto il processo `x` sarà pari a 5/10 = 0.5."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: rimozione feature sulle domande e aggiunta feature sugli ambiti e processi\r\n",
    "\"\"\"\r\n",
    "questions_columns = [col for col in list(cleaned_original_dataset) if re.search(\"^D\\d\", col)]\r\n",
    "\r\n",
    "for i, row in dataset_with_ambiti_processi.iterrows():\r\n",
    "    for question, APs in MAPPING_DOMANDE_AMBITI_PROCESSI.items():\r\n",
    "        if row[question] is True:   # se ha risposto correttamente\r\n",
    "            for AP in APs:\r\n",
    "                dataset_with_ambiti_processi.at[i, AP] += 1 / conteggio_ambiti_processi[AP]\r\n",
    "\r\n",
    "dataset_ap = dataset_with_ambiti_processi.drop(questions_columns, axis=1)\r\n",
    "\r\n",
    "dataset_ap.to_csv(cfg.CLEANED_DATASET_WITH_AP)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: rimozione feature sulle domande e aggiunta feature sugli ambiti e processi\r\n",
    "Attenzione: inutile eseguire se si è eseguita la precedente cella.\r\n",
    "\"\"\"\r\n",
    "dataset_ap = pd.read_csv(cfg.CLEANED_DATASET_WITH_AP)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if \"Unnamed: 0\" in dataset_ap.columns:\r\n",
    "    dataset_ap.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.6 - Analisi della correlazione fra feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corr_matrix = dataset_ap.corr(method='pearson').round(2)\r\n",
    "corr_matrix.style.background_gradient(cmap='YlOrRd')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Emerge un'alta correlazione, 0.87, tra i voti della stessa materia, come prevedibile, mentre una correlazione abbastanza alta tra materie differenti, 0.75. La correlazione pari a 1.0 tra `pu_ma_gr` e `pu_ma_no` si spiega considerando che la seconda è la normalizzazione del valore della prima, per cui portano esattamente la stessa informazione.\r\n",
    "\r\n",
    "Abbiamo ritenuto interessante indagare la correlazione sussistente tra i voti agli scritti e agli orali, i punteggi finali ottenuti al test e gli ambiti e i processi: abbiamo previsto solo valori positivi di correlazione, in quanto a voti maggiori, corrispondono punteggi finali maggiori, come anche per i processi e ambiti. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "interesting_to_check_if_correlated_columns = [\r\n",
    "    # Alta correlazione fra voti della stessa materia, abbastanza correlate fra materie diverse\r\n",
    "    \"voto_scritto_ita\",\r\n",
    "    \"voto_orale_ita\",\r\n",
    "    \"voto_scritto_mat\",\r\n",
    "    \"voto_orale_mat\",\r\n",
    "    # Correlazione totale, abbastanza correlate con voti\r\n",
    "    \"pu_ma_no\",\r\n",
    "    # Target columns\r\n",
    "    \"LIVELLI\",\r\n",
    "    \"DROPOUT\"\r\n",
    "] + list(ambiti_processi)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "check_corr_dataset = dataset_ap[interesting_to_check_if_correlated_columns].corr(method='pearson').round(2)\r\n",
    "check_corr_dataset.style.background_gradient(cmap='YlOrRd')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La nostra previsione è stata confermata."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.7 - Analisi dei tipi e gestione delle colonne"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Lista colonne e tipi:\")\r\n",
    "\r\n",
    "table = BeautifulTable()\r\n",
    "table.columns.header = [\"\", \"Type\"]\r\n",
    "\r\n",
    "for col in dataset_ap.columns :\r\n",
    "    table.rows.append([col, dataset_ap[col].dtypes])\r\n",
    "        \r\n",
    "table.columns.alignment = BeautifulTable.ALIGN_LEFT\r\n",
    "table.set_style(BeautifulTable.STYLE_SEPARATED)\r\n",
    "print(table)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Le colonne DROPOUT e LIVELLI non sono considerate fra le feature in quanto colonne target (in particolare, DROPOUT è una regressione di LIVELLI).\r\n",
    "continuous_features = columns_low_ratio_null_values + \\\r\n",
    "                      [\"pu_ma_gr\", \"pu_ma_no\", \"Fattore_correzione_new\", \"Cheating\", \"WLE_MAT\", \"WLE_MAT_200\", \"WLE_MAT_200_CORR\",\r\n",
    "                       \"pu_ma_no_corr\"] + \\\r\n",
    "                      list(ambiti_processi) # Feature sui voti, feature elencate, ambiti e processi\r\n",
    "if cfg.FILL_NAN == \"remove\":\r\n",
    "    continuous_features.remove(\"voto_scritto_ita\")\r\n",
    "    continuous_features.remove(\"voto_orale_ita\")\r\n",
    "ordinal_features = [\"n_stud_prev\", \"n_classi_prev\"]\r\n",
    "int_categorical_features = [\r\n",
    "    \"CODICE_SCUOLA\", \"CODICE_PLESSO\", \"CODICE_CLASSE\", \"campione\", \"prog\",\r\n",
    "]\r\n",
    "str_categorical_features = [\r\n",
    "    \"sesso\", \"mese\", \"anno\", \"luogo\", \"eta\", \"freq_asilo_nido\", \"freq_scuola_materna\",\r\n",
    "    \"luogo_padre\", \"titolo_padre\", \"prof_padre\", \"luogo_madre\", \"titolo_madre\", \"prof_madre\",\r\n",
    "    \"regolarità\", \"cittadinanza\", \"cod_provincia_ISTAT\", \"Nome_reg\",\r\n",
    "    \"Cod_reg\", \"Areageo_3\", \"Areageo_4\", \"Areageo_5\", \"Areageo_5_Istat\"\r\n",
    "]\r\n",
    "bool_features = [\"Pon\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.8 - Gestione dei valori nulli\r\n",
    "Il dataset in considerazione presenta molti valori nulli.\\\r\n",
    "Vi sono diverse tecniche per gestirli, tra cui:\r\n",
    "- sostituzione del valore nullo con un indice di sintesi (media, mediana) della colonna in considerazione: la scelta dell'indice dipende da vari fattori, tra cui la forma della distribuzione del certo attributo;\r\n",
    "- rimozione della riga corrispondente: viene adottata solitamente quando la colonna presenta pochi valori nulli e/o la riga presenta molti valori nulli;\r\n",
    "- rimozione della colonna corrispondente: viene adottata solitamente quando la colonna presenta un alto numero di valori nulli."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_ap[\"sigla_provincia_istat\"].fillna(value=\"ND\", inplace=True)\r\n",
    "\r\n",
    "if cfg.FILL_NAN == \"remove\":\r\n",
    "    # Rimuovere colonne voti ita (molti valori nulli).\r\n",
    "    # Rimuovere record con dati nulli in voti mat (meno valori nulli).\r\n",
    "    dataset_ap.drop([\"voto_scritto_ita\", \"voto_orale_ita\"], axis=1, inplace=True)\r\n",
    "    dataset_ap.dropna(subset=[\"voto_scritto_mat\", \"voto_orale_mat\"], inplace=True)\r\n",
    "else :\r\n",
    "    for col in columns_low_ratio_null_values : \r\n",
    "        if cfg.FILL_NAN == \"median\":\r\n",
    "            replaced_value = dataset_ap[col].median()\r\n",
    "        elif cfg.FILL_NAN == \"mean\":\r\n",
    "            replaced_value = dataset_ap[col].mean()\r\n",
    "\r\n",
    "        dataset_ap[col].fillna(value=replaced_value, inplace=True)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 - Machine Learning\r\n",
    "## 3.1 - Suddivisione dataset in training e test\r\n",
    "Avendo un dataset di una sola coorte di studenti relativi alle prove di un anno, non si dispone di un set per fare testing: Idealmente, l'insieme di testing dovrebbe referirsi ad una coorte diversa da quella su cui è stato effettuato il training. Nel caso di specie, tuttavia, si dispone dei dati relativi ad un'unica coorte, per cui abbiamo pensato di eseguire lo split in questo modo:\r\n",
    "\r\n",
    "1. dataset diviso in training set (default 80%) e test set (default 20%);\r\n",
    "2. training set (ottenuto dalla suddivisione al punto 1) diviso in training set (default 80%) e validation set (default 20%)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_training_set, df_test_set = train_test_split(dataset_ap, test_size=cfg.TEST_SET_PERCENT, random_state=19)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 - Analisi dello sbilanciamento del dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nr_nodrop, nr_drop = np.bincount(dataset_ap['DROPOUT'])\r\n",
    "total_records = nr_drop + nr_nodrop\r\n",
    "nl = '\\n'\r\n",
    "print(\r\n",
    "    f\"Total number of records: {total_records}{nl}\\\r\n",
    "        {nl}\\\r\n",
    "Total num. DROPOUT: {nr_drop}{nl}\\\r\n",
    "Total num. NO DROPOUT: {nr_nodrop}{nl}\\\r\n",
    "    {nl}\\\r\n",
    "Ratio DROPOUT/TOTAL: {round(nr_drop / total_records, 2)}{nl}\\\r\n",
    "Ratio NO DROPOUT/TOTAL: {round(nr_nodrop / total_records, 2)}{nl}\\\r\n",
    "    {nl}\\\r\n",
    "Ratio DROPOUT/NO DROPOUT: {round(nr_drop / nr_nodrop, 2)}\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le due classi (i.e. target del classificatore) appaiono leggermente sbilanciate: in dettaglio, la classe dei soggetti che manifestano dropout ha una cardinalità inferiore della classe in cui non si è avuto dropout."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.1 - Gestione dello sbilanciamento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vi sono diverse tecniche per gestire lo sbilanciamento tra le classi di un attributo, tra cui:\r\n",
    "- **ricampionamento dei dati**: \r\n",
    "    - *random over sampling*: selezionare randomicamente delle istanze della classe sotto rappresentata e duplicarle fino a quando le cardinalità delle classi si equivalgono; aumenta il rischio di overfitting;\r\n",
    "    - *random under sampling*: rimuovere istanze della classe sovrarappresentata fintanto che le cardinalità delle classi si equivalgono; causa una riduzione del training set;\r\n",
    "    - *cluster-based over-sampling*: esecuzione dell'algoritmo *k-means* sulle istanze della classe maggiormente rappresentata e su quelle della classe meno rappresentata in modo indipendente, per poi compiere oversampling sui cluster ottenuti fin tanto che le cardinalità dei cluster di una stessa classe si equivalgono come anche le cardinalità delle classi nel loro complesso;\r\n",
    "- **generazione di dati sintetici**: \r\n",
    "    - *SMOTE* (Synthetic Minority Over-sampling Technique): selezionare due o più istanze simili della classe sotto rappresentata e modificare leggermente il valore di un attributo alla volta di un ammontare inferiore alla differenza tra le istanze simili; evita l'overfitting (a patto che vi siano poche attributi) ma aumenta il rumore;\r\n",
    "- **cambiare la natura del problema**: da classificazione a *anomaly detection* o *change detection*;\r\n",
    "- **penalizzazione delle classificazioni errate sulla classe sottorappresentata**: *Cost-sensitive Training*;\r\n",
    "- **monitorare metriche diverse dall'accuratezza**, in quanto solitamente si ottengono ottimi valori di accuratezza con dati sbilanciati, perché il modello classifica tutti gli input come appartenenti alla classe più numerosa.\r\n",
    "\r\n",
    "Alla luce di queste tecniche e considerate le peculiarità del nostro dataset emerse durante la EDA, abbiamo ritenuto che la tecnica migliore al caso nostro fosse il *random under sampling*: il nostro dataset presenta un alto numero di istanze e le classi dell'attributo target presentano un sbilanciamento poco accentuato per cui conviene ricorrere ad un sottocampionamento randomico, evitando così il rischio di overfitting, piuttosto che al sovracampionamento.\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Eseguire per: campionamento\r\n",
    "\"\"\"\r\n",
    "if cfg.SAMPLING_TO_PERFORM == \"random_undersampling\":\r\n",
    "    # class_nodrop contiene i record della classe sovrarappresentata, ovvero SENZA DROPOUT.\r\n",
    "    class_nodrop = df_training_set[df_training_set['DROPOUT'] == False]\r\n",
    "    # class_drop contiene i record della classe sottorappresentata, ovvero CON DROPOUT.\r\n",
    "    class_drop = df_training_set[df_training_set['DROPOUT'] == True]\r\n",
    "\r\n",
    "    # Sotto campionamento di class_drop in modo che abbia stessa cardinalità di class_nodrop.\r\n",
    "    class_nodrop = class_nodrop.sample(len(class_drop), random_state=19)\r\n",
    "\r\n",
    "    print(f'Class NO DROPOUT: {len(class_nodrop):,}')\r\n",
    "    print(f'Classe DROPOUT: {len(class_drop):,}')\r\n",
    "\r\n",
    "    df_training_set = class_drop.append(class_nodrop)\r\n",
    "    df_training_set = df_training_set.sample(frac=1, random_state=19)\r\n",
    "else:\r\n",
    "    categorical_features_indexes = [i for i in range(len(df_training_set.columns)) if\r\n",
    "                                    df_training_set.columns[i] in str_categorical_features + int_categorical_features]\r\n",
    "\r\n",
    "    df_training_set = df_training_set.apply(lambda col: pd.factorize(col)[0] if col.name in str_categorical_features else col)\r\n",
    "    df_test_set = df_test_set.apply(lambda col: pd.factorize(col)[0] if col.name in str_categorical_features else col)\r\n",
    "\r\n",
    "    sm = SMOTENC(categorical_features=categorical_features_indexes, random_state=19)\r\n",
    "    X_train, y_train = sm.fit_resample(\r\n",
    "        df_training_set[[col for col in df_training_set.columns if col != 'DROPOUT']],\r\n",
    "        df_training_set['DROPOUT']\r\n",
    "    )\r\n",
    "    df_training_set = pd.concat([X_train, y_train], axis=1)\r\n",
    "\r\n",
    "    X_test, y_test = sm.fit_resample(\r\n",
    "        df_test_set[[col for col in df_test_set.columns if col != 'DROPOUT']],\r\n",
    "        df_test_set['DROPOUT']\r\n",
    "    )\r\n",
    "    df_test_set = pd.concat([X_test, y_test], axis=1)\r\n",
    "\r\n",
    "    # Se SMOTENC viene eseguito, ogni feature categorica stringa viene trasformata in feature categorica intera.\r\n",
    "    int_categorical_features = int_categorical_features + str_categorical_features\r\n",
    "    str_categorical_features = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nonostante la scelta di orientarci verso il *random under sampling* abbiamo inserito il codice per poter eseguire l'*over sampling* mediante SMOTENC. Maggiori dettagli su questa tecnica sono presentati nella sezione [Risultati ottenuti](#risultati)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ecco che il training set è stato bilanciato."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if \"Unnamed: 0\" in df_training_set.columns:\r\n",
    "    df_training_set.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 - Preprocessing per creazione del modello di Deep Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.1 - Suddivisione del dataset in training e validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_training_set, df_validation_set = train_test_split(df_training_set, test_size=cfg.VALIDATION_SET_PERCENT, random_state=19)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2 - Conversione dei dati da DataFrame (Pandas) a Dataset (Tensorflow/Keras)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def convert_dropout_column_to_one_hot(dropout_col):\r\n",
    "    dropout_col_one_hot = []\r\n",
    "    for dc in dropout_col:\r\n",
    "        if dc == 1:\r\n",
    "            dropout_col_one_hot.append([1, 0])\r\n",
    "        else:\r\n",
    "            dropout_col_one_hot.append([0, 1])\r\n",
    "    return dropout_col_one_hot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pd_dataframe_to_tf_dataset(dataframe: pd.DataFrame):\r\n",
    "    copied_df = dataframe.copy()\r\n",
    "    if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "        dropout_col = copied_df.pop(\"DROPOUT\")\r\n",
    "        dropout_col = convert_dropout_column_to_one_hot(dropout_col)\r\n",
    "        copied_df.drop(\"LIVELLI\", axis=1, inplace=True)\r\n",
    "    else:\r\n",
    "        # La colonna target LIVELLI viene presa, invertiti i valori (0 -> 5, 1 -> 4, ..., 5 -> 0) e poi li divido per 5.\r\n",
    "        # Questo viene fatto per poter associare a valori sopra 0.6 (corrispondente all'originale 4) il concetto di\r\n",
    "        # \"Dropout Sì\" e a quelli inferiori il concetto di \"Dropout no\".\r\n",
    "        dropout_col = copied_df.pop(\"LIVELLI\")\r\n",
    "        dropout_col = dropout_col.subtract(5)\r\n",
    "        dropout_col = dropout_col.abs()\r\n",
    "        dropout_col = dropout_col.divide(5) # Normalizzazione dei valori della colonna da [0..5] a [0..1].\r\n",
    "        copied_df.drop(\"DROPOUT\", axis=1, inplace=True)\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    Dato che il dataframe ha dati eterogenei lo convertiamo a dizionario (i.e. dict(copied_df)),\r\n",
    "    in cui le chiavi sono i nomi delle colonne e i valori sono i valori della colonna.\r\n",
    "    Infine bisogna indicare la colonna target.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((dict(copied_df), dropout_col))\r\n",
    "    tf_dataset = tf_dataset.shuffle(buffer_size=len(copied_df), seed=19)\r\n",
    "    return tf_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds_training_set = pd_dataframe_to_tf_dataset(df_training_set)\r\n",
    "ds_validation_set = pd_dataframe_to_tf_dataset(df_validation_set)\r\n",
    "ds_test_set = pd_dataframe_to_tf_dataset(df_test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.3 - Suddivisione del dataset in batch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds_training_set = ds_training_set.batch(cfg.BATCH_SIZE, drop_remainder=True)\r\n",
    "ds_validation_set = ds_validation_set.batch(cfg.BATCH_SIZE, drop_remainder=True)\r\n",
    "ds_test_set = ds_test_set.batch(cfg.BATCH_SIZE, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.4 - Creazione dei layer di input per ogni feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_layers = {}\r\n",
    "for name, column in df_training_set.items():\r\n",
    "    if name in [\"DROPOUT\", \"LIVELLI\"]:\r\n",
    "        continue\r\n",
    "\r\n",
    "    if name in continuous_features:\r\n",
    "        dtype = tf.float32\r\n",
    "    elif name in ordinal_features or name in int_categorical_features or name in bool_features:\r\n",
    "        dtype = tf.int64\r\n",
    "    else:  # name in str_categorical_features\r\n",
    "        dtype = tf.string\r\n",
    "\r\n",
    "    input_layers[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.5 - Encoding delle feature in base al loro tipo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preprocessed_features = []\r\n",
    "\r\n",
    "def stack_dict(inputs, fun=tf.stack):\r\n",
    "    values = []\r\n",
    "    for key in sorted(inputs.keys()):\r\n",
    "        values.append(tf.cast(inputs[key], tf.float32))\r\n",
    "\r\n",
    "    return fun(values, axis=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing colonne con dati booleani\r\n",
    "for name in bool_features:\r\n",
    "    inp = input_layers[name]\r\n",
    "    inp = inp[:, tf.newaxis]\r\n",
    "    float_value = tf.cast(inp, tf.float32)\r\n",
    "    preprocessed_features.append(float_value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing colonne con dati interi ordinali\r\n",
    "ordinal_inputs = {}\r\n",
    "for name in ordinal_features:\r\n",
    "    ordinal_inputs[name] = input_layers[name]\r\n",
    "\r\n",
    "normalizer = Normalization(axis=-1)\r\n",
    "normalizer.adapt(stack_dict(dict(df_training_set[ordinal_features])))\r\n",
    "ordinal_inputs = stack_dict(ordinal_inputs)\r\n",
    "ordinal_normalized = normalizer(ordinal_inputs)\r\n",
    "preprocessed_features.append(ordinal_normalized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing colonne con dati continui\r\n",
    "continuous_inputs = {}\r\n",
    "for name in continuous_features:\r\n",
    "    continuous_inputs[name] = input_layers[name]\r\n",
    "\r\n",
    "normalizer = Normalization(axis=-1)\r\n",
    "normalizer.adapt(stack_dict(dict(df_training_set[continuous_features])))\r\n",
    "continuous_inputs = stack_dict(continuous_inputs)\r\n",
    "continuous_normalized = normalizer(continuous_inputs)\r\n",
    "preprocessed_features.append(continuous_normalized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per i dati categorici di tipo stringa o intero abbiamo adottato la *one-hot encoding*; tale codifica consiste nel mappare ogni categoria ad un array contenente $n$-1 elementi settati a 0 e un elemento settato a 1, con $n$ pari al numero delle categorie: l'indice dell'elemento settato ad 1 permette di distinguere le diverse categorie."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing colonne con dati categorici stringa\r\n",
    "for name in str_categorical_features:\r\n",
    "    vocab = sorted(set(df_training_set[name]))\r\n",
    "\r\n",
    "    lookup = StringLookup(vocabulary=vocab, output_mode='one_hot')\r\n",
    "\r\n",
    "    x = input_layers[name][:, tf.newaxis]\r\n",
    "    x = lookup(x)\r\n",
    "\r\n",
    "    preprocessed_features.append(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Preprocessing colonne con dati categorici interi\r\n",
    "for name in int_categorical_features:\r\n",
    "    vocab = sorted(set(df_training_set[name]))\r\n",
    "\r\n",
    "    lookup = IntegerLookup(vocabulary=vocab, output_mode='one_hot')\r\n",
    "\r\n",
    "    x = input_layers[name][:, tf.newaxis]\r\n",
    "    x = lookup(x)\r\n",
    "\r\n",
    "    preprocessed_features.append(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 - Assemblaggio dei vari layer e creazione del modello\r\n",
    "\r\n",
    "### 3.4.1 - Funzioni di attivazione\r\n",
    "Abbiamo studiato e approfondito le principali funzioni di attivazione, passandone in rassegna le peculiarità, i pro e i contro. Alla luce di tali nuove conoscenze, abbiamo optato per le seguenti scelte:\r\n",
    "- *Leaky ReLU* per gli hidden layer: \r\n",
    "    - abbiamo preferito la variante *Leaky ReLU* piuttosto che la classica ReLU, in quanto la prima, prevedendo una leggera pendenza a sinistra dell'origine, evita il *Dying ReLU Problem*, di cui soffre invece la ReLU;\r\n",
    "    - il codominio ha cardinalità infinita (dato che per input > 0, diventa la funzione di identità), per cui non vi è una significativa perdita di informazione nel passaggio da un layer al successivo;\r\n",
    "    - a fronte di somme pesate negative in input, restituisce valori negativi molto bassi che portano a calcoli non pesanti computazionalmente, registrando così tempi minori di addestramento per il modello nel complesso.\r\n",
    "![](./src/img/leaky_relu.png)\r\n",
    "- *Sigmoid* o *Softmax* per l'output layer: sono le funzione che vengono [consigliate](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/) come *best practice* per il livello di output rispettivamente in problemi di regressione vista come classificazione binaria e classificazione (multi-classe)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2 - Inizializzazione dei pesi\r\n",
    "L'inizializzazione dei pesi è un aspetto molto importante nella progettazione di una rete neurale: essi incidono in larga misura sulla somma pesata che si accumula in ogni neurone e che viene passata in input alla sua funzione di attivazione, determinando anche la misura in cui l'informazione passa da un layer al successivo.\r\n",
    "\r\n",
    "I modelli di reti neurali vengono addestrati mediante l'algoritmo di ottimizzazione noto come **discesa del gradiente stocastico**, che modifica iterativamente i pesi della rete con l'obiettivo di minimizzare una funzione di costo: alla fine dell'addestramento, si giunge a quella combinazione di pesi che permettono alla rete di avere una certa performance nelle previsioni.\r\n",
    "L'algoritmo di ottimizzazione richiede un punto di partenza nello spazio dei possibili valori dei pesi e spesso la sua velocità di convergenza o effettiva convergenza è fortemente determinata dall'inizializzazione iniziale dei pesi.\r\n",
    "\r\n",
    "In letteratura, si suggeriscono diverse funzioni di inizializzazione a seconda della funzione di attivazione del layer in oggetto:\r\n",
    "- con ReLU o Leaky ReLU, è consigliato il metodo di inizializzazione **He** che calcola i pesi a partire dalla distribuzione di probabilità gaussiana con media 0 e deviazione standard pari a $\\sqrt{\\frac{2}{n}}$, con $n$ il numero di input che arrivano al neurone;\r\n",
    "    - conseguentemente, l'altezza della gaussiana dipende dal numero di input che il neurone riceve: all'aumentare del numero di questi input, l'altezza aumenta e la gaussiana si restringe, diminuendo la probabilità di avere pesi alti. Così i neuroni tendono meno a saturare, permettendo alla rete di raggiungere il massimo dell'accuratezza in meno tempo. Infatti, tale accorgimento non migliora l'accuratezza, ma solo il tempo che la rete impiega a raggiungere quella massima;\r\n",
    "    \r\n",
    "    [<img src=\"./src/img/He.png\" width=\"50%\"/>](https://alessiomorselli.github.io/DeepLearning/web/pages/documentation/gaussian2.png)\r\n",
    "- con Sigmoid o Tanh, inizializzazione **Glorot** (anche nota *Xavier*, dal nome di *Xavier Glorot*, un ricercatore presso Google DeepMind): questo metodo, per ogni neurone, calcola i diversi pesi da una distribuzione di probabilità uniforme che ha come dominio l'intervallo $[-\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}]$, con $n$ il numero di input che arrivano al neurone considerato;\r\n",
    "    - l'immagine seguente mostra il restringersi del range in cui cadono tali pesi all'aumentare del numero di input entranti nel neurone;\r\n",
    "    \r\n",
    "    [<img src=\"./src/img/Xavier.png\" width=\"50%\"/>](https://machinelearningmastery.com/wp-content/uploads/2021/01/Plot-of-Range-of-Xavier-Weight-Initialization-with-Inputs-from-One-to-One-Hundred-.png)\r\n",
    "    - esiste anche una variante di tale metodo, chiamata **Normalized Xavier Weight Initialization**: essa estrae i valori da una distribuzione di probabilità uniforme, tuttavia il suo dominio è l'intervallo $[-\\frac{\\sqrt{6}}{\\sqrt{n + m}}, \\frac{\\sqrt{6}}{\\sqrt{n + m}}]$, con $n$ il numero di input al nodo in oggetto, $m$ il numero di output del layer di cui fa parte quel neurone (ossia il numero di neuroni di quel layer).\r\n",
    "    \r\n",
    "    [<img src=\"./src/img/Normalized_Xavier.png\" width=\"50%\"/>](https://machinelearningmastery.com/wp-content/uploads/2021/01/Plot-of-Range-of-Normalized-Xavier-Weight-Initialization-with-Inputs-from-One-to-One-Hundred.png)\r\n",
    "\r\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preprocessed = tf.concat(preprocessed_features, axis=-1) # Tensore\r\n",
    "\r\n",
    "preprocessor = tf.keras.Model(input_layers, preprocessed) # Tanti input layer quante le feature\r\n",
    "\r\n",
    "# inizializzatore che verrà usato per i pesi dei layer con ReLU / LeakyReLU\r\n",
    "initializer_hidden_layer = tf.keras.initializers.HeNormal(seed=19)\r\n",
    "# inizializzatore che verrà usato per i pesi dei layer con sigmoid\r\n",
    "initializer_output_layer = tf.keras.initializers.GlorotNormal(seed=19)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.3 - Regolarizzazione per evitare overfitting\r\n",
    "Si ha **overfitting** ogniqualvolta il modello apprende i dati troppo bene, ossia ne incamera il rumore statistico: conseguentemente, il modello presenta un *alto errore di generalizzazione* (scarse performance quando il modello è valutato su dati nuovi, come quelli dell'insieme di test).\r\n",
    "Il rischio di overfitting si fa più concreto quando la cardinalità del training set è bassa e/o la rete neurale è molto grande.\r\n",
    "\r\n",
    "Per ridurre al minimo il rischio di overfitting, vi sono molti metodi di **regolarizzazione**, tra cui il **dropout**: esso approssima l'addestramento in parallelo di un gran numero di reti neurali con architetture differenti; consiste nell'assegnare ad ogni neurone di un certo layer una probabilità $1-p$ che sia ignorato e $p$ che non sia ignorato, per cui si concretizza in un sottocampionamento degli output dei vari layer.\r\n",
    "La probabilità $p$ viene settato a livello del singolo layer; in letteratura, sono suggeriti valori prossimi a 0.5 per i layer nascosti e prossimi a 1 per i layer visibili, mentre di non settare alcun dropout per il layer di output. Noi abbiamo rispettato tali suggerimenti, come è possibile constatare nel codice che segue.\r\n",
    "\r\n",
    "Un effetto collaterale del dropout è la riduzione della capacità rappresentativa della rete, per cui è conveniente solo a fronte di reti neurali grandi: nel caso la rete abbia dimensioni modeste, occorre aumentarne prima l'ampiezza e la profondità e poi settare il dropout ai vari layer. Questo motivo è alla base della nostra scelta di raddoppiare i valori di default degli iperparametri relativi al numero di layer e al numero di neuroni nelle configurazioni in cui è presente il dropout. \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.4 - Batch Normalization\r\n",
    "Tra le innumerevoli architetture esperite, ve ne sono state alcune con un alto numero di hidden layer, nell'ordine delle decine; avendo riscontrato significativi rallentamenti nell'addestramento, abbiamo effettuato diverse ricerche in seguito alle quali abbiamo scoperto la **batch normalization**: trattasi di una tecnica che standardizza gli input al layer cui è applicata per ogni mini-batch di addestramento. In questo modo, i valori in input sono minori e centrati, quindi manipolabili in maniera più agevole: conseguentemente il processo di apprendimento risulta essere più stabile e veloce, impiegando un numero di epoche inferiore.\\\r\n",
    "Gli studi a tal proposito in letteratura suggeriscono di sperimentare l'applicazione di tale processo di standardizzazione in tre punti differenti:\r\n",
    "- tra l'hidden layer e la sua funzione di attivazione;\r\n",
    "- dopo la funzione di attivazione di un hidden layer;\r\n",
    "- subito prima del layer di output. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "body = tf.keras.Sequential()\r\n",
    "\r\n",
    "if cfg.DROPOUT_LAYER:\r\n",
    "    body.add(tf.keras.layers.Dropout(rate=cfg.DROPOUT_INPUT_LAYER_RATE, seed=19))  # aggiunta dropout a layer di input\r\n",
    "\r\n",
    "# segue l'aggiunta degli hidden layers\r\n",
    "for _ in range(cfg.NUMBER_OF_LAYERS):\r\n",
    "    body.add(tf.keras.layers.Dense(cfg.NEURONS, kernel_initializer=initializer_hidden_layer))\r\n",
    "\r\n",
    "    if cfg.BATCH_NORMALIZATION == \"dense_batch_activation\":\r\n",
    "        body.add(tf.keras.layers.BatchNormalization())\r\n",
    "\r\n",
    "    if cfg.ACTIVATION_LAYER == \"leaky_relu\":\r\n",
    "        body.add(tf.keras.layers.LeakyReLU())\r\n",
    "    else:\r\n",
    "        body.add(tf.keras.layers.ReLU())\r\n",
    "\r\n",
    "    if cfg.DROPOUT_LAYER:\r\n",
    "        body.add(tf.keras.layers.Dropout(rate=cfg.DROPOUT_HIDDEN_LAYER_RATE, seed=19))\r\n",
    "\r\n",
    "    if cfg.BATCH_NORMALIZATION == \"dense_activation_batch\":\r\n",
    "        body.add(tf.keras.layers.BatchNormalization())\r\n",
    "\r\n",
    "if cfg.BATCH_NORMALIZATION == \"before_output\":\r\n",
    "    body.add(tf.keras.layers.BatchNormalization())\r\n",
    "\r\n",
    "# segue l'aggiunta dell'output layer\r\n",
    "if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "    body.add(tf.keras.layers.Dense(2, activation=\"softmax\", kernel_initializer=initializer_output_layer))\r\n",
    "else:\r\n",
    "    body.add(tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_initializer=initializer_output_layer))\r\n",
    "\r\n",
    "x = preprocessor(input_layers)\r\n",
    "\r\n",
    "result = body(x)\r\n",
    "\r\n",
    "model = tf.keras.Model(input_layers, result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualizzazione tabellare del modello\r\n",
    "from keras.utils.vis_utils import plot_model\r\n",
    "model.summary()\r\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.5 - Funzione di costo e metriche di performance\r\n",
    "Riguardo alla funzione di costo, abbiamo scelto per il nostro modello la **Cross-entropy**: come riportato dal libro \"Pattern Recognition and Machine Learning\" (2006), utilizzare la cross-entropy piuttosto che la somma dei quadrati come funzione di costo nei problemi di classificazione porta ad un apprendimento più veloce e ad una generalizzazione maggiore.\\\r\n",
    "Trattasi di una misura derivante dal campo della *Teoria dell'informazione*: nella sua accezione più generale, si concretizza nel calcolare la differenza (in termini più precisi, l'entropia totale) tra due distribuzioni di probabilità, dato una variabile aleatoria o un insieme di eventi. Nell'ambito del *Machine Learning*, tali due distribuzioni sono:\r\n",
    "- la distribuzione della variabile target e\r\n",
    "- l'approssimazione della distribuzione della variabile target restituita dal predittore.\r\n",
    "\r\n",
    "Pertanto, la cross-entropy è un valore positivo indicante l'entropia totale necessaria affinchè i valori della variabile target siano rappresentati/trasmessi mediante la distribuzione approssimata delle predizioni.\r\n",
    "\r\n",
    "[<img src=\"./src/img/cross-entropy.png\" width=\"50%\"/>](https://i.stack.imgur.com/gNip2.png)\r\n",
    "\r\n",
    "Più le due distribuzioni divergono, maggiore è la cross-entropy: ciò è dovuto all'informazione ( = $-\\log_2{P(x)}$) maggiore a fronte di predizioni molto distanti da 1.\r\n",
    "\r\n",
    "[<img src=\"./src/img/information.png\" width=\"50%\"/>](https://i.stack.imgur.com/gNip2.png)\r\n",
    "\r\n",
    "A livello implementativo, nel caso il problema in esame sia visto come un problema di:\r\n",
    "    \r\n",
    "- **regressione**, l'output compreso nell'intervallo $[0, 1]$ viene valutato con la funzione di costo `BinaryCrossentropy()` poiché ci interessa sapere solamente quando l'output è inferiore del threshold fissato (0.4) oppure maggiore;\r\n",
    "- **classificazione**, l'output è un vettore con tante componenti quante le classi di output (in questo caso due, \"dropout sì\" e \"dropout no\"), in cui ogni componente è una misura di probabilità di far parte di quella classe, il tutto valutato con la funzione di costo `CategoricalCrossEntropy()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "    accuracy = tf.keras.metrics.Accuracy(name=\"acc\")\r\n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy()\r\n",
    "else:\r\n",
    "    # 0.6 perché dopo il preprocessing, i LIVELLI in [3,4,5] è DROPOUT = True, LIVELLI in [0,1,2] è DROPOUT = False\r\n",
    "    accuracy = tf.metrics.BinaryAccuracy(name=\"bin_acc\", threshold=0.6)  \r\n",
    "    loss_function = tf.keras.losses.BinaryCrossentropy()\r\n",
    "\r\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=cfg.LEARNING_RATE),\r\n",
    "              loss=loss_function,\r\n",
    "              metrics=[\r\n",
    "                accuracy,\r\n",
    "                tf.keras.metrics.FalsePositives(name=\"fp\"),\r\n",
    "                tf.keras.metrics.FalseNegatives(name=\"fn\"),\r\n",
    "                tf.keras.metrics.TruePositives(name=\"tp\"),\r\n",
    "                tf.keras.metrics.TrueNegatives(name=\"tn\"),\r\n",
    "                tf.keras.metrics.Precision(name=\"prec\"),\r\n",
    "                tf.keras.metrics.Recall(name=\"rec\")\r\n",
    "              ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.6 - Early stopping\r\n",
    "Mano a mano che procedavameno nello sperimentare differenti architetture, abbiamo riscontrato qualche dubbio circa il numero giusto di epoche per l'apprendimento\\.\r\n",
    "In effetti, ricerche a tal proposito, hanno evidenziato che trattasi di un iperparametro il cui settaggio è di fondamentale e delicata importanza; in dettaglio, settare:\r\n",
    "- un numero troppo elevato di epoche, può portare al rischio dell'overfitting del modello sul training set mentre,\r\n",
    "- un numero troppo basso può essere la causa di un apprendimento troppo grossolano degli schemi celati dietro ai dati.\r\n",
    "\r\n",
    "Uno dei metodi suggeriti in letteratura è il c.d. **Early stopping**: esso permette di specificare un numero di epoche arbitrariamente alto, in quanto gestisce l'effettiva fine dell'addestramento sulla base delle variazioni registrate da una certa metrica specificata. \r\n",
    "Nel caso del nostro progetto abbiamo specificato come metrica da monitorare il valore restituito dalla funzione di costo sul validation set.\\\r\n",
    "Col parametro `mode` specifichiamo la direzione della variazione della metrica da considerarsi come miglioramento.\r\n",
    "Per evitare che l'addestramento venga stoppato non appena il valore della funzione di costo non migliora (nel caso di specie, migliora sta per diminuisce), abbiamo settato un ulteriore argomento, `patience` che indica il numero di epoche che occorre attendere prima di fermare l'addestramento in caso non ci siano miglioramenti; così facendo, possiamo contemplare la situazione in cui la metrica monitorata sperimenti un plateau o addirittura un peggioramento (nel caso di specie, sta per aumento) per qualche epoca, prima di tornare a migliorare.\\\r\n",
    "Ulteriormente abbiamo settato a `True` il parametro `restore_best_weight`, allo scopo di ripristinare i valori dell'ultima epoca in cui la metrica monitorata è migliorata: questo permette di non evitare che la rete rimanga coi pesi dell'ultima epoca fin cui si è addestrata, grazie al parametro `patience`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "Definizione dello stopper per evitare che la reti continui ad addestrarsi quando non ci sono miglioramenti della loss \r\n",
    "(val_loss = funzione di costo sul validation set) per piu' di 5 epoche\r\n",
    "\"\"\"\r\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\r\n",
    "                              patience=5,\r\n",
    "                              mode=\"min\",\r\n",
    "                              restore_best_weights=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.7 - Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"[Training]\")\r\n",
    "history = model.fit(ds_training_set,\r\n",
    "          epochs=cfg.EPOCH,\r\n",
    "          batch_size=cfg.BATCH_SIZE,\r\n",
    "          validation_data=ds_validation_set,\r\n",
    "          callbacks=([early_stopper] if cfg.EARLY_STOPPING else []) + [PlotLossesKeras()],\r\n",
    "          verbose=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.8 - Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"[Test]\")\r\n",
    "score = model.evaluate(ds_test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Results with test dataset')\r\n",
    "print('Loss:', round(score[0], 4))\r\n",
    "print('Accuracy:', round(score[1], 4))\r\n",
    "print('False positives:', int(score[2]))\r\n",
    "print('False negatives:', int(score[3]))\r\n",
    "print('True positives:', int(score[4]))\r\n",
    "print('True negatives:', int(score[5]))\r\n",
    "print('Precision: ', round(score[6], 4))\r\n",
    "print('Recall: ', round(score[7], 4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.9 - Calcolo manuale delle metriche a partire dalla matrice di confusione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Selezione delle colonne delle feature (utilizzo di dataset_ap, ma uno qualsiasi fra df_training_set,\r\n",
    "# df_test_set e df_validation_set andava bene lo stesso)\r\n",
    "feature_columns = dataset_ap[[col for col in dataset_ap.columns if col not in [\"DROPOUT\", \"LIVELLI\"]]]\r\n",
    "target_col = \"DROPOUT\" if cfg.PROBLEM_TYPE == \"classification\" else \"LIVELLI\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def convert_df_for_prediction(dataframe: pd.DataFrame):\r\n",
    "    copied_df = dataframe.copy()\r\n",
    "    ds = tf.data.Dataset.from_tensor_slices(dict(copied_df))\r\n",
    "\r\n",
    "    return ds.batch(cfg.BATCH_SIZE, drop_remainder=True)\r\n",
    "\r\n",
    "\r\n",
    "def convert_for_confusion_matrix(dataframe: pd.DataFrame):\r\n",
    "    X = convert_df_for_prediction(dataframe[feature_columns])\r\n",
    "    y = dataframe[target_col]\r\n",
    "    len_X = len(X) * cfg.BATCH_SIZE\r\n",
    "    len_y = len(y)\r\n",
    "    \r\n",
    "    if len_X != len_y:\r\n",
    "        y = y.head(len_X - len_y)\r\n",
    "    if cfg.PROBLEM_TYPE == \"classification\":\r\n",
    "        y = convert_dropout_column_to_one_hot(y)\r\n",
    "    else: # cfg.PROBLEM_TYPE == \"regression\"\r\n",
    "        y = y.subtract(5)\r\n",
    "        y = y.abs()\r\n",
    "    \r\n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_x, training_y = convert_for_confusion_matrix(df_training_set)\r\n",
    "validation_x, validation_y = convert_for_confusion_matrix(df_validation_set)\r\n",
    "test_x, test_y = convert_for_confusion_matrix(df_test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predicted_training_y = model.predict(training_x)\r\n",
    "predicted_validation_y = model.predict(validation_x)\r\n",
    "predicted_test_y = model.predict(test_x)\r\n",
    "\r\n",
    "if cfg.PROBLEM_TYPE == \"regression\":\r\n",
    "    predicted_training_y = np.round(predicted_training_y * 5)\r\n",
    "    predicted_validation_y = np.round(predicted_validation_y * 5)\r\n",
    "    predicted_test_y = np.round(predicted_test_y * 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_confusion_matrix = tf.math.confusion_matrix(labels=training_y, predictions=predicted_training_y).numpy()\r\n",
    "validation_confusion_matrix = tf.math.confusion_matrix(labels=validation_y, predictions=predicted_validation_y).numpy()\r\n",
    "test_confusion_matrix = tf.math.confusion_matrix(labels=test_y, predictions=predicted_test_y).numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_metrics(label, confusion_matrix):\r\n",
    "    true_positives = np.diag(confusion_matrix) # vettore in cui ogni cella è il numero di TP per la classe\r\n",
    "    false_positives = confusion_matrix.sum(axis=0) - true_positives\r\n",
    "    false_negatives = confusion_matrix.sum(axis=1) - true_positives\r\n",
    "    true_negatives = confusion_matrix.sum() - (true_positives.sum() + false_positives.sum() + false_negatives.sum())\r\n",
    "\r\n",
    "    accuracy = (true_positives.sum() + true_negatives.sum()) / confusion_matrix.sum()\r\n",
    "    precision = true_positives.sum() / (true_positives.sum() + false_positives.sum())\r\n",
    "    recall = true_positives.sum() / (true_positives.sum() + false_negatives.sum())\r\n",
    "\r\n",
    "    print(f\"Confusion Matrix Name: {label}\")\r\n",
    "    print(confusion_matrix)\r\n",
    "    print(f\"- TP: {int(true_positives.sum())}\")\r\n",
    "    print(f\"- TN: {int(true_negatives.sum())}\")\r\n",
    "    print(f\"- FP: {int(false_positives.sum())}\")\r\n",
    "    print(f\"- FN: {int(false_negatives.sum())}\")\r\n",
    "    print(f\"- Accuracy: {round(accuracy, 4)}\") # Attenzione: si tratta della Binary Accuracy\r\n",
    "    print(f\"- Precision: {round(precision, 4)}\")\r\n",
    "    print(f\"- Recall: {round(recall, 4)}\")\r\n",
    "    print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "compute_metrics(\"Training\", training_confusion_matrix)\r\n",
    "compute_metrics(\"Validation\", validation_confusion_matrix)\r\n",
    "compute_metrics(\"Test\", test_confusion_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"risultati\"></a>\r\n",
    "# 4 - Risultati ottenuti dall'esecuzione del modello"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abbiamo cercato di trovare la configurazione migliore per risolvere questo tipo di problema avviando numerosi job grazie al Cluster HPC del DISI, di cui viene trattato più in dettaglio in [Esecuzione su Cluster HPC](#cluster).\r\n",
    "\r\n",
    "Tutte le configurazioni di iperparametri sono state testate sia vedendo il problema come regressione che con il problema visto come classificazione multi-classe.\r\n",
    "\r\n",
    "Abbiamo fatto più iterazioni:\r\n",
    "- la prima è servita a capire quali iperparametri fornivano i risultati più convincenti da usare nella successiva iterazione\r\n",
    "- la seconda ha permesso di testare se configurazioni più avanzate con gli iperparametri più importanti scoperti nell'iterazione precedente fornissero risultati migliori\r\n",
    "- la terza ci ha permesso di confermare la bontà degli iperparametri trovati alla seconda iterazione."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 - Prima iterazione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La configurazione di default (sia per il problema trattato come classificazione che regressione) è la seguente:\r\n",
    "\r\n",
    "Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| 50 | 128 | 32 | 10, Leaky ReLU | 0.001 | No | mediana | No |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nelle tabella che seguono vengono indicati i valori di accuratezza riscontrati e delle note (quando necessario).\r\n",
    "\r\n",
    "**N.B.** Solo le celle che differiscono dalla configurazione di default vengono compilate."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.1 - Classificazione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "\r\n",
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| base_model | - | - | - | - | - | - | - | - | 0.3451 | 0.2581 | 0.2809 | - |\r\n",
    "| batch128 | - | - | 128 | - | - | - | - | - | 0.2990 | 0.1936 | 0.2076 | - |\r\n",
    "| neurons256 | - | 256 | - | - | - | - | - | - | 0.6569 | 0.3675 | 0.3995 | - |\r\n",
    "| batch128neurons256 | - | 256 | 128 | - | - | - | - | - | 0.4621 | 0.3638 | 0.3744 | - |\r\n",
    "| dense5 | - | - | - | 5, Leaky ReLU | - | - | - | - | 0.3202 | 0.1477 | 0.1645 | - |\r\n",
    "| dense15 | - | - | - | 15, Leaky ReLU | - | - | - | - | 0.5303 | 0.3234 | 0.3038 | - |\r\n",
    "| dense15 | - | - | - | 15, Leaky ReLU | - | - | - | - | 0.5303 | 0.3234 | 0.3038 | - |\r\n",
    "| dropout | - | 256 | - | - | - | Sì | - | - | 0.0024 | 0.0000 | 0.0000 | - |\r\n",
    "| epoch100 | 100 | - | - | - | - | - | - | - | 0.5000 | 0.2945 | 0.3135 | - |\r\n",
    "| fillmean | - | - | - | - | - | - | media | - | 0.3538 | 0.2022 | 0.2070 | - |\r\n",
    "| fillremove | - | - | - | - | - | - | rimozione | - | 0.3538 | 0.2022 | 0.2070 | Le colonne con alto tasso di valori nulli vengono rimosse, mentre quelle con un basso tasso vengono rimossi i record con quella feature a null. |\r\n",
    "| lr001 | - | - | - | - | - | - | - | - | 0.7400 | 0.7131 | 0.6720 | - |\r\n",
    "| relu | - | - | - | 10, ReLU | - | - | - | - | 0.4603 | 0.1242 | 0.1639 | - |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.2 - Regressione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| base_model | - | - | - | - | - | - | - | - | 0.1975 | 0.1897 | 0.2460 | - |\r\n",
    "| batch128 | - | - | 128 | - | - | - | - | - | 0.1976 | 0.1908 | 0.2494 | - |\r\n",
    "| neurons256 | - | 256 | - | - | - | - | - | - | 0.1975 | 0.1865 | 0.2418 | - |\r\n",
    "| batch128neurons256 | - | 256 | 128 | - | - | - | - | - | 0.1976 | 0.1868 | 0.2417 | - |\r\n",
    "| dense5 | - | - | - | 5, Leaky ReLU | - | - | - | - | 0.1975 | 0.1869 | 0.2426 | - |\r\n",
    "| dense15 | - | - | - | 15, Leaky ReLU | - | - | - | - | 0.1975 | 0.1822 | 0.2361 | - |\r\n",
    "| dropout | - | 256 | - | - | - | Sì | - | - | 0.1903 | 0.1964 | 0.2563 | - |\r\n",
    "| epoch100 | 100 | - | - | - | - | - | - | - | 0.1975 | 0.1840 | 0.2380 | - |\r\n",
    "| fillmean | - | - | - | - | - | - | media | - | 0.1975 | 0.1874 | 0.2422 | - |\r\n",
    "| fillremove | - | - | - | - | - | - | rimozione | - | 0.1869 | 0.1771 | 0.2174 | Le colonne con alto tasso di valori nulli vengono rimosse, mentre quelle con un basso tasso vengono rimossi i record con quella feature a null. |\r\n",
    "| lr001 | - | - | - | - | - | - | - | - | 0.1493 | 0.1162 | 0.1502 | - |\r\n",
    "| relu | - | - | - | 10, ReLU | - | - | - | - | 0.1975 | 0.1924 | 0.2497 | - |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 - Seconda iterazione\r\n",
    "\r\n",
    "Analizzando le tabelle della prima iterazione osserviamo che gli iperparametri più importanti sono il numero dei neuroni, il numero di hidden layer e il learning rate.\r\n",
    "\r\n",
    "Per questa ragione, abbiamo ideato delle configurazioni con decisamente più neuroni e learning rate più elevati, tenendo però più basso il numero di hidden layer, ottenendo i risultati che seguono. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2.1 - Classificazione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| 512N_001LR | - | 512 | - | - | 0.01 | - | - | - | 0.7292 | 0.6546 | 0.7322 | - |\r\n",
    "| 512N_7L_01LR_DrTrue | - | 512 | - | 7, Leaky ReLU | 0.1 | Sì | - | - | 0.5803 | 0.5965 | 0.4735 | - |\r\n",
    "| 512N_7L_001LR | - | 512 | - | - | 7, Leaky ReLU | 0.01 | - | - | 0.7561 | 0.6419 | 0.5438 | - |\r\n",
    "| 512N_7L_001LR_DrTrue | - | - | - | 7, Leaky ReLU | 0.01 | Sì | - | - | 0.5782 | 0.5473 | 0.4065 | - |\r\n",
    "| before_output | - | - | - | - | - | - | - | Prima del layer di output | $1.8430 \\cdot 10^-4$ | $6.7003 \\cdot 10^-5$ | $7.3082 \\cdot 10^-5$ | - |\r\n",
    "| dense_activation_batch | - | - | - | - | - | - | - | ... | 0.1437 | 0.0710 | 0.0659 | - |\r\n",
    "| dense_batch_activation | - | - | - | - | - | - | - | ... | 0.1307 | 0.0809 | 0.0900 | - |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2.2 - Regressione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| 512N_001LR | - | 512 | - | - | 0.01 | - | - | - | 0.1509 | 0.1962 | 0.2560 | - |\r\n",
    "| 512N_7L_01LR_DrTrue | - | 512 | - | 7, Leaky ReLU | 0.1 | Sì | - | - | 0.1149 | 0.0033 | 0.0019 | - |\r\n",
    "| 512N_7L_001LR | - | 512 | - | - | 7, Leaky ReLU | 0.01 | - | - | 0.1570 | 0.1396 | 0.1781 | - |\r\n",
    "| 512N_7L_001LR_DrTrue | - | - | - | 7, Leaky ReLU | 0.01 | Sì | - | - | 0.1147 | 0.1951 | 0.2549 | - |\r\n",
    "| before_output | - | - | - | - | - | - | - | Prima del layer di output | 0.1975 | 0.1745 | 0.2252 | - |\r\n",
    "| dense_activation_batch | - | - | - | - | - | - | - | ... | 0.1974 | 0.1897 | 0.2464 | - |\r\n",
    "| dense_batch_activation | - | - | - | - | - | - | - | ... | 0.1975 | 0.1882 | 0.2444 | - |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 - Ultimi tentativi"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Considerati i buoni risultati ottenuti in particolar modo dalla configurazione chiamata `512N_001LR` abbiamo provato a tenere fisso il numero di neuroni e di aumentare il numero di hidden layer. Al contempo abbiamo provato anche a tenere il learning rate a 0.01 ma il framework Tensorflow non è stato in grado di proseguire per problemi a runtime. Abbiamo quindi provato a ridurre questo tasso fino a 0.005, ovvero la metà di quello desiderato."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.1 - Classificazione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| great | - | 512 | - | 15, Leaky ReLU | 0.005 | - | - | - | 0.7230 | 0.7161 | 0.7138 | - |\r\n",
    "| smotenc | - | - | - | - | - | - | - | - | ND | ND | ND | Dataset diviso in training 50%, validation 25%, test 25%, sampling eseguito con SMOTENC. |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3.2 - Regressione"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Configurazione | Epoche | Neuroni | Batch | Layer lineari (numero, attivazione) | Tasso apprendimento | Dropout | Tecnica riempimento valori nulli | Batch normalization | Accuratezza in training | Accuratezza in validation | Accuratezza in test | Note aggiuntive |\r\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\r\n",
    "| great | - | 512 | - | 15, Leaky ReLU | 0.005 | - | - | - | 0.1535 | 0.1391 | 0.1783 | - |\r\n",
    "| smotenc | - | - | - | - | - | - | - | - | ND | ND | ND | Dataset diviso in training 50%, validation 25%, test 25%, sampling eseguito con SMOTENC. |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 - Conclusioni\r\n",
    "\r\n",
    "Per quanto riguarda il problema di classificazione, nonostante l'aumento di hidden layer notiamo come la configurazione `great` non fornisca risultati particolarmente migliori rispetto a quelli forniti da quella su cui ci eravamo basati, `512N_001LR`, aumentando al contempo il costo computazionale. Possiamo quindi ritenere la configurazione `512N_001LR` come soddisfacente per la risoluzione di questo problema.\r\n",
    "\r\n",
    "Per quanto riguarda invece il problema di regressione, .... COMPLETARE."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"cluster\"></a>\r\n",
    "# 5 - Esecuzione su Cluster HPC\r\n",
    "Per via della numerosità e complessità computazionale dei test nella fase di progettazione dell'architettura della rete, abbiamo pensato di usufruire del servizio di High Performance Computing su cluster dipartimentale con GPU offerto dal dipartimento DISI.\r\n",
    "\r\n",
    "Dopo aver fatto richiesta, i nostri account istituzionali sono stati abilitati all'accesso ai sistemi dipartimentali e al cluster stesso: la macchina di nostro interesse è `slurm.cs.unibo.it` su cui si trova lo schedulatore del cluster. \r\n",
    "\r\n",
    "In dettaglio, il cluster utilizza uno schedulatore [SLURM](https://slurm.schedmd.com/overview.html) per la distribuzione \r\n",
    "dei job. Pertanto, per effettuare il submit di un job, abbiamo predisposto nella nostra area di lavoro un file di script \r\n",
    "SLURM contenente le direttive per la configurazione del job di interesse.\\\r\n",
    "Sulla scia delle [raccomandazioni](https://disi.unibo.it/it/dipartimento/servizi-tecnici-e-amministrativi/servizi-informatici/utilizzo-cluster-hpc/unibo.tiles.multi.links_attachments/e7809f6f52644346a912b99dd2280788/@@objects-download/11906a7e3d2345278c0fc5ee68ce7975/file/IstruzioniUsoClusterGPU.pdf) contenute nelle istruzioni consegnateci dai tecnici del DISI, abbiamo proceduto col creare sulla macchina slurm un virtual environment \r\n",
    "Python, in cui, mediante il comando `pip3 install` abbiamo installato le dipendenze necessarie (specificate nel file di testo `requirements.txt`).\r\n",
    "\r\n",
    "Abbiamo creato differenti file di script SLURM per le diverse architetture neurali progettate. A titolo di esempio, riportiamo il contenuto dello script di default.\r\n",
    "\r\n",
    "```bash\r\n",
    "#!/bin/bash\r\n",
    "#SBATCH --job-name=base_model\r\n",
    "#SBATCH --mail-type=ALL\r\n",
    "#SBATCH --mail-user=tommaso.azzalin@studio.unibo.it\r\n",
    "#SBATCH --time=10:00:00\r\n",
    "#SBATCH --nodes=1\r\n",
    "#SBATCH --ntasks-per-node=1\r\n",
    "#SBATCH --output=base_model\r\n",
    "#SBATCH --gres=gpu:1\r\n",
    "\r\n",
    "cd  ../\r\n",
    "\r\n",
    ". venv/bin/activate # per attivare il virtual environment python\r\n",
    "\r\n",
    "pip3 install -r requirements.txt\r\n",
    "\r\n",
    "python3 src/base.py\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 - Lavori futuri\r\n",
    "Abbiamo intenzione di continuare a lavorare a questo progetto, perché riteniamo sia un ottimo banco di prova per maturare nuove conoscenze e competenze relative al Machine Learning. In dettaglio, prevediamo di concentrare gli sforzi futuro lungo le seguenti direzioni:\r\n",
    "- gestione dello sbilanciamento del training set mediante:\r\n",
    "    - *SMOTE*, in quanto permette di evitare la riduzione di cardinalità del training set che inevitabilmente segue al *random under-sampling*, senza rischiare l'overfitting proprio del *random over-sampling*;\r\n",
    "    - *Random Weighted Sampling*."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "dea9b1862afc9acd5187c15b3c157615bc525d15a5086a5de3d47d6af39a43d8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}